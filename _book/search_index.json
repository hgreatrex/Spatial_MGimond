[["index.html", "Intro to GIS and Spatial Analysis Preface", " Intro to GIS and Spatial Analysis Manuel Gimond Last edited on 2025-09-11 Preface These pages compile lecture notes for my Introduction to GIS and Spatial Analysis course (ES214). They are organized to follow the course outline, though most chapters can be read in any preferred order. The course–and this book–is divided into two main parts: data manipulation &amp; visualization and exploratory spatial data analysis. The first part is typically conducted using ArcGIS Pro whereas the latter part of the book is carried out in R. ArcGIS was selected as the GIS data manipulation environment due to its strong presence in job applications for undergraduates in the Unites States. However, other GIS software platforms, such as the open source software QGIS, can easily be adopted in lieu of ArcGIS. In fact, R itself is capable of performing many spatial data manipulations including clipping, buffering and projecting. Although some chapters reference ArcGIS-specific techniques, most can be studied without access to the software. A separate website provides ArcGIS Pro tutorials used in this course. The second part of this book–and the course–relies heavily on R because of a) its widespread use in data analysis, b) its rich (arguably unmatched) collection of spatial analysis and spatial statistics packages, c) its scripting environment, which facilitates reproducibility and, d) its cost-effectiveness (it’s completely free and open source!). R is also suited for many traditional GIS tasks involving data manipulation operations. The main advantages of using a full-featured GIS environment like ArcGIS or QGIS lie in creating/editing spatial data, rendering complex maps, and managing spatial data. The Appendix explores various aspects of spatial data manipulation and analysis using R. While the course focuses specifically on point pattern analysis and spatial autocorrelation in R, additional resources are included for students interested in expanding their GIS skills using R. The resources used as references for this course are listed in the Reference. 2025 UPDATE: This version includes a complete rewrite of “Part 2: Spatial Analysis”. The chapters are now more closely aligned with the sequence of topics covered in the course, with an overarching focus on distinguishing between first-order and second-order spatial processes. A new chapter on modeling spatial trends has been added to Part 2. 2023 UPDATE: Removed dependence on rgdal and maptools in Appendices Added Statistical Maps chapter (wrapped confidence maps into this chapter) 2021 UPDATE: This book has been updated for the 2021-2022 academic year. Most changes are in the Appendix and pertain to the sf ecosystem. This includes changes in the mapping appendix, and coordinate systems appendix. This also includes a new appendix that describes the simple feature anatomy and step-by-step instructions on creating new geometries from scratch. This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],["introGIS.html", "Chapter 1 Introduction to GIS 1.1 What is a GIS? 1.2 What is Spatial Analysis? 1.3 What’s in an Acronym? 1.4 Course Roadmap", " Chapter 1 Introduction to GIS 1.1 What is a GIS? A Geographic Information System is a multi-component environment used to create, manage, visualize and analyze data and its spatial counterpart. It’s important to note that most datasets you will encounter in your lifetime can all be assigned a spatial location whether on the earth’s surface or within some arbitrary coordinate system (such as a soccer field or a gridded petri dish). So in essence, any dataset can be represented in a GIS: the question then becomes “does it need to be analyzed in a GIS environment?” The answer to this question depends on the purpose of the analysis. If, for example, we are interested in identifying the ten African countries with the highest conflict index scores for the 1966-78 period, a simple table listing those scores by country is all that is needed. table { width: 600px !important; } td{ font-size: 10px; padding-top: 0px !important; padding-bottom: 0px !important; padding-right: 45px !important; padding-left: 5px !important; } Table 1.1: Index of total African conflict for the 1966-78 period (Anselin and O’Loughlin 1992). Country Conflicts Country Conflicts EGYPT 5246 LIBERIA 980 SUDAN 4751 SENEGAL 933 UGANDA 3134 CHAD 895 ZAIRE 3087 TOGO 848 TANZANIA 2881 GABON 824 LIBYA 2355 MAURITANIA 811 KENYA 2273 ZIMBABWE 795 SOMALIA 2122 MOZAMBIQUE 792 ETHIOPIA 1878 IVORY COAST 758 SOUTH AFRICA 1875 MALAWI 629 MOROCCO 1861 CENTRAL AFRICAN REPUBLIC 618 ZAMBIA 1554 CAMEROON 604 ANGOLA 1528 BURUNDI 604 ALGERIA 1421 RWANDA 487 TUNISIA 1363 SIERRA LEONE 423 BOTSWANA 1266 LESOTHO 363 CONGO 1142 NIGER 358 NIGERIA 1130 BURKINA FASO 347 GHANA 1090 MALI 299 GUINEA 1015 THE GAMBIA 241 BENIN 998 SWAZILAND 147 Data source: Anselin, L. and John O’Loughlin. 1992. Geography of international conflict and cooperation: spatial dependence and regional context in Africa. In The New Geopolitics, ed. M. Ward, pp. 39-75. A simple sort on the Conflict column reveals that EGYPT, SUDAN, UGANDA, ZAIRE, TANZANIA, LIBYA, KENYA, SOMALIA, ETHIOPIA, SOUTH AFRICA are the top ten countries. What if we are interested in knowing whether countries with a high conflict index score are geographically clustered, does the above table provide us with enough information to help answer this question? The answer, of course, is no. We need additional data pertaining to the geographic location and shape of each country. A map of the countries would be helpful. Figure 1.1: Choropleth representation of African conflict index scores. Countries for which a score was not available are not mapped. This example demonstrates how spatial data can uncover spatial patterns that are invisible in tabular formats. It highlights the importance of integrating location into data analysis. Maps often prioritize spatial relationships, while tables emphasize numerical comparisons. Understanding this hierarchy helps choose the right tool for the question. Maps are ubiquitous: available online and in various print medium. But we seldom ask how the boundaries of the map features are encoded in a computing environment? After all, if we expect software to assist us in the analysis, the spatial elements of our data should be readily accessible in a digital form. Spending a few minutes thinking through this question will make you realize that simple tables or spreadsheets are not up to this task. A more complex data storage mechanism is required. This is the core of a GIS environment: a spatial database that facilitates the storage and retrieval of data that define the spatial boundaries, lines or points of the entities we are studying. This may seem trivial, but without a spatial database, most spatial data exploration and analysis would not be possible! 1.1.1 GIS software Many GIS software applications are available–both commercial and open source. Two popular applications are ArcGIS Pro and QGIS. 1.1.1.1 ArcGIS A popular commercial desktop GIS software is ArcGIS Pro developed by Esri (pronounced ez-ree). Esri was once a small land-use consulting firm which did not start developing GIS software until the mid 1970s. ArcGIS Pro comes in different licensing levels and can be purchased with additional add-on packages. As such, a single license can range from a few thousand dollars to well over ten thousand dollars. In addition to software licensing costs, ArcGIS is only available for Windows operating systems–so, if your workplace is a Mac only environment, the purchase of a Windows PC would add to the expense. 1.1.2 QGIS A very capable open source (free) GIS software is QGIS. It encompasses most of the functionality included in ArcGIS Pro. If you are looking for a GIS application for your Mac or Linux environment, QGIS is a wonderful choice given its multi-platform support. Built into the current versions of QGIS are functions from another open source software: GRASS. GRASS has been around since the 1980’s and has many advanced GIS data manipulation functions however, its use is not as intuitive as that of QGIS or ArcGIS (hence the preferred QGIS alternative). 1.2 What is Spatial Analysis? A distinction is made in this course between GIS and spatial analysis. In mainstream GIS software, the term analysis typically refers to operations such as data manipulation and querying. In contrast, spatial analysis focuses on the statistical examination of spatial patterns and the processes that may have generated them. More broadly, spatial analysis seeks to answer questions like: “What could have caused the observed spatial pattern?” It is an exploratory process in which we quantify spatial patterns and investigate the underlying mechanisms that may explain their distribution. For example, imagine you record the location of each tree within a well-defined study area. Mapping these locations is a typical GIS task. Once the trees are mapped, you may begin to draw inferences about the spatial pattern: Are the trees clustered or dispersed? Is tree density consistent across the study area? Could environmental factors such as soil type or slope have influenced the observed distribution? These are the kinds of questions addressed through spatial analysis, using quantitative and statistical techniques to explore and explain spatial patterns. Figure 1.2: Distribution of Maple trees in a 1,000 x 1,000 ft study area. In this course, you’ll learn that while popular GIS software like ArcGIS Pro excels at creating and manipulating spatial data, it is limited when it comes to analyzing the patterns and processes that may have produced those data. To move beyond basic data handling and explore deeper spatial relationships, we turn to more robust quantitative tools. One such tool is R–a free, open-source data analysis environment. R offers one of the richest collections of spatial data analysis and statistical packages available today. Learning to work in the R programming environment will be highly beneficial, as many of the skills you acquire are transferable to a wide range of quantitative analysis tasks, both spatial and non-spatial. R can be installed on both Windows and Mac operating systems. Another related piece of software that you might find useful is RStudio which offers a nice interface to R. To learn more about data analysis in R, visit the ES218 course website. 1.3 What’s in an Acronym? GIS is a ubiquitous technology. Many of you are taking this course in part because you have seen GIS listed as a “desirable”” or “required” skill in job postings. Often, GIS is thought of primarily as a “map-making” tool, a perception shared by many casual users in the workforce. While visualizing data is indeed a key feature of GIS, it is equally important to consider what data is being visualized and why. O’Sullivan and Unwin (O’Sullivan and Unwin 2010) use the term accidental geographer to describe individuals “whose understanding of geographic science is based on the operations made possible by GIS software”. Building on this idea, we introduce the term accidental data analyst–someone whose grasp of data and its analysis is limited to the point-and-click interfaces of popular software such as spreadsheets, statistical packages, and GIS platforms. The aggressive marketing of GIS technology has at times, placed technology ahead of purpose and theory. This concern is not unique to GIS; similar issues arose decades ago when personal computers made it easier to graph non-spatial data and perform statistical procedures. The different purposes of mapping spatial data closely parallel the goals of graphing non-spatial data. John Tukey (Tukey 1972) identified three broad categories of graphical displays: “Graphs from which numbers are to be read off–substitutes for tables. Graphs intended to show the reader what has already been learned (by some other technique)–these we shall sometimes impolitely call propaganda graphs. Graphs intended to let us see what may be happening over and above what we have already described- these are the analytical graphs that are our main topic.” A GIS-based analogy to Tukey’s categories might be: Reference maps (USGS maps, hiking maps, road maps): used to navigate landscapes or identify locations of interest. Presentation maps: designed to convey a specific narrative. While we avoid Tukey’s term “propaganda,” it’s worth noting that maps can be used to persuade. Statistical maps: created to manipulate raw data in ways that reveal patterns not immediately visible. These often require multiple data transformations and may benefit from being explored both within and outside a spatial context. This course emphasizes the last two categories of spatial data visualization, with a particular focus on statistical maps. 1.4 Course Roadmap This course is divided into two main parts, each focusing on distinct aspects of spatial data science. 1.4.1 Part 1: Working with Spatial Data This section introduces foundational GIS concepts and tools for data manipulation and visualization. Introduction to GIS &amp; Spatial Analysis What is GIS? What is spatial analysis? GIS software overview Feature Representation Vector vs. Raster Object vs. Field views Scale and attribute tables GIS Data Management File formats and project organization Managing data in ArcGIS Symbolizing Features Color theory and classification Choropleth mapping techniques Statistical Maps Mapping distributions and uncertainty Classification intervals and outlier detection Pitfalls to Avoid MAUP, ecological fallacy, unstable rates Good Map Making Tips Map elements, layout, and typography Spatial Operations and Vector Overlays Selection, overlays, and spatial queries Coordinate Systems Geographic vs. projected systems Spatial properties and geodesic geometries Map Algebra Local, focal, zonal, and global raster operations 1.4.2 Part 2: Exploratory Spatial Data Analysis This section focuses on statistical analysis of spatial patterns. Spatial Trends First order analysis of field variables Fitting polynomial models Spatial Autocorrelation Second order property of field variables Global and local Moran’s I Point Pattern Analysis: First order analysis Density and distance-based methods Testing for CSR processes Point Pattern Analysis: Second order analysis ANN analysis K and L functions Paired correlation function Spatial Interpolation Deterministic (IDW, Thiessen) and statistical (Kriging) methods References Anselin, Luc, and John O’Loughlin. 1992. “Geography of international conflict and cooperation: spatial dependence and regional context in Africa.” The New Geopolitics, 39–75. O’Sullivan, David, and David Unwin. 2010. Geographic Information Analysis. New Jersey, USA: Wiley. Tukey, John W. 1972. “Some Graphic and Semigraphic Displays.” In Statistical Papers in Honor of George w. Snedecor, edited by T. A. Bancroft, 293–316. August 1969. "],["chp02_0.html", "Chapter 2 Feature Representation 2.1 Introduction 2.2 Vector vs. Raster 2.3 Object vs. Field 2.4 Scale 2.5 Attribute Tables 2.6 Summary", " Chapter 2 Feature Representation 2.1 Introduction Before we can analyze or visualize spatial data in a GIS, we must first decide how to represent real-world phenomena in a digital environment. This chapter introduces the foundational data models used in GIS–vector and raster–and explores how different types of geographic features are encoded using these models. Understanding how features are represented is essential for making informed decisions about data collection, storage, analysis, and visualization. We’ll also examine conceptual frameworks such as the object vs. field view of the world, the role of scale in determining how features are modeled, and how attribute data are structured and classified. These concepts form the backbone of spatial data modeling and are critical for interpreting and working with spatial data effectively throughout the rest of the course. 2.2 Vector vs. Raster To work in a GIS environment, real world observations (objects or events that can be recorded in 2D or 3D space) need to be reduced to spatial entities. These spatial entities can be represented in a GIS as a vector data model or a raster data model. Figure 2.1: Vector and raster representations of a river feature. 2.2.1 Vector Vector features can be decomposed into three different geometric primitives: points, polylines and polygons. Vector data models are used to represent discrete features like roads, buildings and boundaries. They also benefit from supporting rich attribute data and precise geometry. Let’s explore how different types of features are represented using the vector model. 2.2.1.1 Point Figure 2.2: Three point objects defined by their X and Y coordinate values. A point is composed of one coordinate pair representing a specific location in a coordinate system. Points are the most basic geometric primitives having no length or area. By definition a point can’t be “seen” since it has no area; but this is not practical if such primitives are to be mapped. So points on a map are represented using symbols that have both area and shape (e.g. circle, square, plus signs). We seem capable of interpreting such symbols as points, but there may be instances when such interpretation may be ambiguous (e.g. is a round symbol delineating the area of a round feature on the ground such as a large oil storage tank or is it representing the point location of that tank?). 2.2.1.2 Polyline Figure 2.3: A simple polyline object defined by connected vertices. A polyline is composed of a sequence of two or more coordinate pairs called vertices. A vertex is defined by coordinate pairs, just like a point, but what differentiates a vertex from a point is its explicitly defined relationship with neighboring vertices. A vertex is connected to at least one other vertex. Like a point, a true line can’t be seen since it has no area. And like a point, a line is symbolized using shapes that have a color, width and style (e.g. solid, dashed, dotted, etc…). Roads and rivers are commonly stored as polylines in a GIS. 2.2.1.3 Polygon Figure 2.4: A simple polygon object defined by an area enclosed by connected vertices. A polygon is composed of three or more line segments whose starting and ending coordinate pairs are the same. Sometimes you will see the words lattice or area used in lieu of ‘polygon’. Polygons represent both length (i.e. the perimeter of the area) and area. They also embody the idea of an inside and an outside; in fact, the area that a polygon encloses is explicitly defined in a GIS environment. If it isn’t, then you are working with a polyline feature. If this does not seem intuitive, think of three connected lines defining a triangle: they can represent three connected road segments (thus polyline features), or they can represent the grassy strip enclosed by the connected roads (in which case an ‘inside’ is implied thus defining a polygon). 2.2.2 Raster Figure 2.5: A simple raster object defined by a 10x10 array of cells or pixels. A raster data model uses an array of cells, or pixels, to represent real-world objects. Raster datasets are commonly used for representing and managing imagery, surface temperatures, digital elevation models, and any other continuous data. A raster can be thought of as a special case of an area object where the area is divided into a regular grid of cells. While rasters are often described as grids of cells, they can also be thought of as arrays of spatially referenced values, each tied to a specific location in space. In a raster data model, each cell (or pixel) is explicitly associated with a value, representing a measurement or classification at that location. This contrasts with the vector model, where attribute values are linked to geometric features (points, lines, or polygons) and may not be defined for every location in space. Raster datasets are organized as regular grids—typically square or rectangular in shape. If the spatial features of interest do not occupy the entire extent of the grid, the remaining cells are assigned special values such as NULL or NoData to indicate the absence of valid data. 2.3 Object vs. Field While the vector/raster distinction focuses on data structure, the object vs. field perspective focuses on how we conceptualize spatial phenomena. This distinction is especially useful when deciding how to model and analyze different types of spatial features. 2.3.1 Object View The object view treats spatial features as discrete entities that exist independently and occupy specific locations. These features do not occur everywhere in space. Examples include point locations of cities, road networks, or polygonal representations of land parcels or urban areas. Objects are typically represented using the vector data model. 2.3.2 Field View The field view treats spatial phenomena as continuous surfaces or distributions that vary across space. A field is a property that can be measured at any location within a study area. Common examples include surface elevation, temperature, or precipitation. These are typically represented using the raster data model. Some features can be represented as either objects or fields, depending on the analytical context. For instance, the presence or absence of buildings can be modeled as: Objects, if we are interested in the location and shape of individual buildings. Fields, if we are interested in identifying areas where buildings do or do not exist (e.g., using a binary raster where 1 = presence, 0 = absence). Figure 2.6: Field and Object representation of buildings. Ultimately, the choice between object and field representations depends on the nature of the phenomenon and the goals of the analysis. 2.4 Scale In GIS, scale refers to the ratio between a distance on the map and the corresponding distance in the real world. For example, a scale of 1:10,000 means that one unit on the map represents 10,000 units on the ground. It’s important to distinguish between two uses of the term: Cartographic scale refers to the map’s level of detail. Analytical scale refers to the spatial extent or resolution of a study. In cartographic terms, a large scale map (e.g., 1:10,000) shows a small area in great detail, while a small scale map (e.g., 1:10,000,000) shows a large area with less detail. This is opposite to how “large scale” is often used in everyday language, where it implies a broad or extensive scope. The following two maps of the Boston region illustrate this difference: At a small scale (1:10,000,000), Boston and other cities may be represented as simple points. At a large scale (1:34,000), Boston may be represented as a detailed polygon, and roads may appear as polygons rather than simple lines. Figure 2.7: Map of the Boston area at a 1:10,000,000 scale. Note that in geography, this is considered small scale whereas in layperson terms, this extent is often referred to as a large scale (i.e. covering a large area). Figure 2.8: Map of the Boston area at a 1:34,000 scale. Note that in geography, this is considered large scale whereas in layperson terms, this extent is often referred to as a small scale (i.e. covering a small area). Understanding scale is essential for choosing appropriate data and representation methods in GIS analysis. 2.5 Attribute Tables In GIS, attributes are non-spatial data that describe the characteristics of spatial features–such as a city’s population, a road’s name, or a parcel’s land use type. A feature on a GIS map is linked to its record in the attribute table by a unique numerical identifier (ID). Each feature in a GIS layer is associated with one or more records in the attribute table, enabling a one-to-one or many-to-one relationship between geometry and descriptive data. Most GIS software allows users to click on a map feature and view its associated attributes directly. Raster datasets can also store attribute information, but only when pixel values represent discrete categories (e.g., land cover classes). In such cases, each unique integer value may correspond to a class label or description. However, most raster datasets used in this course will represent continuous data and will not include attribute tables. 2.5.1 Measurement Levels Attribute data can be broken down into four measurement levels: Nominal data which have no implied order, size or quantitative information (e.g. paved and unpaved roads) Ordinal data have an implied order (e.g. ranked scores), however, we cannot quantify the difference since a linear scale is not implied. Interval data are numeric and have a linear scale, however they do not have a true zero and can therefore not be used to measure relative magnitudes. For example, one cannot say that 60°F is twice as warm as 30°F since when presented in degrees °C the temperature values are 15.5°C and -1.1°C respectively (and 15.5 is clearly not twice as big as -1.1). Ratio scale data are interval data with a true zero such as monetary value (e.g. $1, $20, $100). 2.5.2 Data type In GIS, choosing the correct data type for an attribute is essential for accurate data storage, efficient processing, and meaningful analysis. ArcGIS supports several common data types, including integer, float, double, and text. The choice of data type should align with the attribute’s measurement level and intended use. The following table summarizes popular data types available in most GIS applications: Type Stored values Note Short integer -32,768 to 32,767 Whole numbers Long integer -2,147,483,648 to 2,147,483,647 Whole numbers Float -3.4 * E-38 to 1.2 E38 Real numbers Double -2.2 * E-308 to 1.8 * E308 Real numbers Text Up to 64,000 characters Letters and words While whole numbers can be stored as floats or doubles (e.g., storing 2 as 2.0), doing so increases storage requirements. This may not be a concern for small datasets, but for large datasets with tens of thousands of records, it can impact file size and processing speed. Conversely, storing decimal values as integers can lead to significant data loss. For example, values such as 0.2, 0.01, 0.34, 0.1, and 0.876 would all round to either 0 or 1, obscuring meaningful differences. This can distort map outputs, especially in choropleth visualizations, where subtle variations are important as shown in the following two figures. Figure 2.9: Map of data represented as decimal (float) values. Figure 2.10: Map of same data represented as integers instead of float. 2.6 Summary This chapter introduces the foundational concepts of how geographic features are represented in a GIS environment. It begins by distinguishing between the two primary data models–vector and raster–and explains how real-world entities are encoded as points, lines, polygons, or grids of cells. The discussion then shifts to conceptual frameworks, including the object vs. field view of spatial phenomena, which helps clarify how the same feature can be represented differently depending on the analytical context. The importance of scale is also addressed, with a focus on how map detail and spatial extent influence representation choices. Finally, the chapter explores attribute tables, which store non-spatial information linked to spatial features, and introduces key concepts such as measurement levels (nominal, ordinal, interval, ratio) and data types (integer, float, double, text). These distinctions are critical for ensuring accurate data storage, querying, and analysis in GIS workflows. "],["gis-data-management.html", "Chapter 3 GIS Data Management 3.1 Introduction 3.2 GIS File Data Formats 3.3 Managing GIS Files in ArcGIS 3.4 Managing GIS Projects and File References 3.5 Summary", " Chapter 3 GIS Data Management 3.1 Introduction Effective data management is essential to any GIS workflow. Whether working with spatial data for analysis, visualization, or sharing, understanding how GIS files are structured, stored, and organized ensures efficiency, and reproducibility. This chapter introduces the most common GIS file formats used for both vector and raster data, highlighting their strengths, limitations, and compatibility across platforms. Beyond file formats, the chapter explores best practices for managing GIS projects within software environments like ArcGIS. Topics such project folder organization and file dependencies are covered to help avoid common pitfalls when working with complex spatial datasets. 3.2 GIS File Data Formats In the GIS world, you will encounter many different GIS file formats. Some file formats are unique to specific GIS applications, others are universal. For this course, we will focus on a subset of spatial data file formats: shapefiles for vector data, Imagine and GeoTiff files for rasters and file geodatabases and geopackages for both vector and raster data. 3.2.1 Vector Data File Formats 3.2.1.1 Shapefile A shapefile is a file-based data format native to ArcView 3.x software (a much older version of ArcGIS Pro). Conceptually, a shapefile is a feature class–it stores a collection of features that have the same geometry type (point, line, or polygon), attributes, and spatial extent. Despite its name, a shapefile is not a single file but a set of files that work together. At minimum, three files are required, but up to eight may be present. Each file shares the same base name but has a different extension, as shown below: File extension Content .dbf Attribute information .shp Feature geometry .shx Feature geometry index .aih Attribute index .ain Attribute index .prj Coordinate system information .sbn Spatial index file (optional) .sbx Spatial index file (optional) While shapefiles are widely supported and simple to use, they have notable limitations, such as lack of support for topology, limited attribute field name lengths, and constraints on file size and data types. 3.2.1.2 File Geodatabase A file geodatabase is a relational database storage format developed by Esri. It is a more complex and versatile structure than the shapefile, consisting of a .gdb folder that houses dozens of files. These files collectively store spatial features, attribute tables, indexes, metadata, and rules that define relationships between features. One of the key advantages of a file geodatabase is its ability to store multiple feature classes (e.g. vector layers) and raster datasets within a single container. It also supports topological rules, which allow users to define spatial relationships between features—such as shared boundaries or connectivity between lines. An example of a geodatabase’s contents is shown in the following figure. Figure 3.1: Sample content of an ArcGIS file geodatabase. File geodatabases are efficient for managing large datasets and support advanced GIS operations. However, they are proprietary to ESRI and may have limited compatibility with non-ESRI software. 3.2.1.3 GeoPackage A GeoPackage is a modern, open-standard data format built on top of SQLite–a lightweight, self-contained relational database. It is non-proprietary and designed for efficient storage and transfer of spatial data. One of its key advantages is compactness: coordinate values, metadata, attribute tables, projection information, and even multiple layers (vector and raster) can be stored in a single .gpkg file. This structure makes GeoPackage highly portable and ideal for sharing complex GIS projects. Unlike shapefiles, which require multiple files, or file geodatabases, which are proprietary to Esri, GeoPackage offers a unified and open solution compatible with a wide range of applications including QGIS (2.12 and up), R, and ArcGIS Pro. An example of a geopackage’s contents is shown in the following figure. Figure 3.2: Sample content of a geopackage. While GeoPackage supports many advanced features, performance may vary depending on the software and dataset size. Some specialized capabilities, such as topology rules, may be more limited compared to Esri’s file geodatabase. 3.2.2 Raster Data File Formats Raster data represents spatial phenomena as a grid of cells (pixels), each storing a numeric value. These values can represent elevation, temperature, and land cover for example. Rasters are particularly well-suited for modeling continuous surfaces and are commonly used in remote sensing and environmental analysis. One important characteristic of raster data is pixel depth, which defines the range of distinct values a raster can store. For example, a 1-bit raster can only store two values (0 and 1), while a 16-bit raster can store up to 65,536 distinct values. The following figure illustrates how pixel depth affects the range of values. Figure 3.3: Examples of different pixel depths. Some of the most popular raster file formats are listed next 3.2.2.1 Imagine The Imagine file format was originally created by an image processing software company called ERDAS. This file format consists of a single .img file. This is a simpler file format than the vector shapefile. It is sometimes accompanied by an .xml file which usually stores metadata information about the raster layer. 3.2.2.2 GeoTiff A popular public domain raster data format is the GeoTIFF format. It extends the standard TIFF (Tagged Image File Format) by embedding georeferencing information directly within the file. This includes coordinate system, projection, datum, and spatial resolution, allowing the raster to be accurately placed in geographic space without the need for auxiliary files. GeoTIFF is widely supported across GIS and remote sensing software, including QGIS, ArcGIS and R. Its portability, platform independence, and open specification make it a popular format for sharing raster data. GeoTIFF also supports internal compression (e.g., LZW, DEFLATE, JPEG), which can reduce file size. 3.2.2.3 File Geodatabase A raster file can also be stored in a file geodatabase alongside vector files. Geodatabases have the benefit of defining image mosaic structures thus allowing the user to create “stitched” images from multiple image files stored in the geodatabase. Also, processing very large raster files can be computationally more efficient when stored in a file geodatabase as opposed to an Imagine or GeoTiff file format. 3.2.2.4 GeoPackage In addition to vector data, GeoPackage also supports raster data storage. Raster layers are stored in a tiled format within the same .gpkg file making it a compact and portable solution for mixed data types. GeoPackage is a good choice for sharing raster data across platforms due to its open standard and single-file structure. 3.2.3 Comparison table The following table summarizes key characteristics of the most commonly used GIS file formats discussed above. This comparison includes aspects such as data type, structure, portability, and support for topology and metadata. /* Your CSS rules for tables go here */ table { border-collapse: collapse; /* Removes spacing between cell borders */ width: 100%; /* Optional: ensures table takes full available width */ } th, td { padding: 1px 6px !important; /* Sets small padding (top/bottom: 4px, left/right: 8px) */ text-align: left; /* Optional: aligns text to the left within cells */ border: 1px solid #ddd !important; /* Optional: adds a light border to cells */ } th { background-color: #f2f2f2 !important; /* Optional: adds a background to header cells */ } } Format Type Structure Portability Topology Support Metadata Support Notes Shapefile Vector Multi-file High No Limited Legacy format, widely supported File Geodatabase Vector/Raster Folder-based Low (ESRI only) Yes Strong Efficient for large datasets GeoPackage Vector/Raster Single file High Partial Strong Open format, cross-platform Imagine (.img) Raster Single file (+xml) Medium N/A Moderate Common in remote sensing GeoTIFF Raster Single file High N/A Strong Public domain, widely supported 3.3 Managing GIS Files in ArcGIS Due to the complex and often multi-file structure of many GIS formats–such as shapefiles or file geodatabases–it is strongly recommended to manage GIS files (e.g., copy, move, delete, rename) from within the GIS software environment rather than through external tools like Windows File Explorer. GIS applications are designed to handle the internal dependencies and metadata links that exist between files. Managing files outside of GIS software can lead to broken links, missing components, or corrupted datasets, especially when dealing with formats that span multiple files or folders. For example, a shapefile may appear as a single layer in ArcGIS but is actually composed of several interdependent files. Renaming or moving just one of these files manually can render the entire dataset unusable. The figure below illustrates how ArcGIS Catalog simplifies file management by presenting complex datasets as unified entries, reducing the risk of accidental mismanagement. Figure 3.4: Windows File Explorer view vs. ArcGIS Catalog view. Note, for example, how the many files that make up the Cities shapefile (as viewed in a Windows file manager environment) appears as a single entry in the Catalog view. This makes it easier to rename the shapefile since it needs to be done only for a single entry in the GIS software (as opposed to renaming the Cities files seven times in the Windows file manager environment). 3.4 Managing GIS Projects and File References Unlike many other software environments–such as word processors or spreadsheets–a GIS project is not self-contained in a single file. Instead, it consists of a project file (e.g., .aprx in ArcGIS Pro or .qgz in QGIS) and a collection of external spatial data files (e.g., shapefiles, rasters, geodatabases, or geopackages). The project file stores information about how layers are symbolized and where the associated data files are located, but it does not embed the data itself. Because of this structure, GIS software must be able to locate the external data files referenced in the project. If any of these files are renamed, moved, or deleted outside of the GIS environment (e.g., using Windows’ File Explorer), the project file may no longer be able to find them. This results in broken links, which prevent the layers from loading properly. In ArcGIS Pro, broken links are indicated by exclamation marks next to the affected layers in the Contents pane. This typically occurs when the file path stored in the project no longer matches the actual location of the data on disk. Figure 3.5: Figure shows an ArcGIS Pro project with properly referenced GIS layers (left) and the same project reopened after the associated data files were moved or deleted outside of ArcGIS (right). The broken links are indicated by exclamation marks next to the affected layers, signaling that the project can no longer locate the original data sources. To avoid these issues, it is best practice to: Keep all project-related files in a single, well-organized folder. Use relative pathnames, which store file locations relative to the project file’s location rather than as full absolute paths. Avoid renaming, moving, or deleting GIS data files outside of the GIS software. 3.5 Summary This chapter introduced foundational concepts in GIS data management, emphasizing the importance of understanding file formats, project organization, and software-based file handling. It began by exploring common GIS file formats for both vector and raster data, including shapefiles, file geodatabases, GeoPackages, Imagine files, and GeoTIFFs. Each format was discussed in terms of structure, portability, metadata support, and compatibility across platforms. The chapter then addressed best practices for managing GIS files within software environments like ArcGIS Pro and QGIS. It highlighted the risks of manipulating GIS files outside of these environments. "],["symbolizing-features.html", "Chapter 4 Symbolizing features 4.1 Introduction 4.2 Color 4.3 Color Space 4.4 Classification 4.5 Choosing an Appropriate Color Scheme 4.6 Classification Intervals 4.7 Summary", " Chapter 4 Symbolizing features 4.1 Introduction Effective map design is not just about placing data on a map–it’s about communicating information clearly and accurately. This chapter explores the principles and practices behind symbolizing spatial features, focusing on how color and classification schemes can be used to enhance the readability and interpretability of maps. The chapter begins by breaking down the three perceptual dimensions of color–hue, lightness, and saturation–and explains how each can be used to represent different types of data. It then introduces the concept of color spaces, including the commonly used HSV (Hue-Saturation-Value) model and perceptually accurate models like CIELAB and Munsell, highlighting how human perception of color can differ from software-generated color models. The chapter also covers classification schemes used in choropleth mapping: Qualitative schemes for categorical data, Sequential schemes for ordered data, Divergent schemes for data with a meaningful midpoint. It emphasizes the importance of choosing appropriate classification intervals, such as equal interval, quantile, and Jenks natural breaks, and demonstrates how different choices can significantly affect the appearance and interpretation of a map. Finally, the chapter introduces tools like ColorBrewer, which help cartographers select color palettes that are visually effective, colorblind-safe, and suitable for grayscale printing. 4.2 Color Color plays a central role in map design, influencing how effectively spatial patterns and relationships are communicated. In cartography, color is not just aesthetic–it encodes meaning and guides interpretation. To use color effectively, it’s important to understand its three perceptual dimensions: hue, lightness and saturation. These dimensions interact to shape how we perceive and differentiate features in a map features. In the following sections, we’ll explore each dimension and its implications for symbolizing different types of data. 4.2.1 Hue Hue is the perceptual dimension most commonly associated with color names–such as red, green, or blue. In cartography, hue is typically used to represent categorical data, where each category is assigned a distinct color. This allows map readers to easily differentiate between classes without implying any order or magnitude. However, the choice of hues should be made carefully, as some colors may carry cultural connotations or be difficult to distinguish for individuals with color vision deficiencies. Figure 4.1: An example of eight different hues. Hues are associated with color names such as green, red or blue. Note that magentas and purples are not part of the natural visible light spectrum; instead they are a mix of reds and blues (or violets) from the spectrum’s tail ends. 4.2.2 Lightness Lightness–sometimes referred to as value–describes how much light is reflected or emitted from a surface, influencing how bright or dark a color appears. In cartographic design, lightness is particularly useful for symbolizing ordinal, interval, or ratio data, where a progression from light to dark can intuitively represent increasing values. Unlike hue, which separates categories, lightness enables us to show variation within a single category or color. However, care should be taken to maintain sufficient contrast between classes, especially in grayscale or colorblind-safe designs Figure 4.2: Eight different hues (across columns) with decreasing lightness values (across rows). 4.2.3 Saturation Saturation–also referred to as chroma–describes the intensity or vividness of a color. Highly saturated colors appear bold and vibrant, while low-saturation colors appear muted or grayish. In cartographic design, saturation can be used to emphasize specific features or categories, but it should be applied with care. Overuse can lead to visual clutter or misinterpretation, especially when combined with other color dimensions. Saturation is best used to subtly enhance contrast or draw attention to key map elements. Figure 4.3: Eight different hues (across columns) with decreasing saturation values (across rows). 4.3 Color Space The three perceptual dimensions of color–hue, lightness, and saturation–can be visualized as forming a three-dimensional color space. While one might expect this space to be a cube, it is more accurately represented as a cone: hue wraps around the circumference, saturation extends outward from the center, and lightness runs vertically. Figure 4.4: This is how the software defines the color space. But does this match our perception of color space? This model helps explain how colors blend, fade, or become indistinguishable as saturation or lightness changes. However, most software-defined color spaces (like HSV) assume symmetry, which does not reflect how humans actually perceive color. For example, we can distinguish more shades of blue than yellow at the same saturation and lightness. Figure 4.5: A cross section of the color space with constant hues and lightness values and decreasing saturation values where the two hues merge. How many distinct yellows can you perceive? How many distinct blues? Do the counts seem to match? Unless you have exceptionally acute color vision, you’ll likely notice a discrepancy–even though the software has generated exactly 30 distinct shades of each. To confirm this, we can add borders around each color swatch to visually verify that the number of distinct colors is indeed the same. Figure 4.6: A cross section of the color space with each color distinctly outlined. y now, it should be evident that symmetrical color spaces—like those commonly used in software–do not accurately reflect how humans perceive color. Our visual system is more sensitive to certain hues and less so to others, resulting in perceptual asymmetries that these models fail to capture. More rigorously designed color spaces, such as CIELAB and Munsell, address this limitation by modeling color as a non-symmetrical space that better aligns with human perception. For instance, in the Munsell system, a vertical slice of the color cone along the blue/yellow axis reveals noticeable differences in how many distinct shades of each hue we can perceive. Figure 4.7: A slice of the Munsell color space. As illustrated by the Munsell color space, our ability to distinguish colors is not uniform across hues. For example, we can perceive fewer distinct shades of yellow than blue at comparable lightness levels. In one comparison, 29 unique shades of yellow were discernible (excluding grayscale values where saturation = 0), while 36 shades of blue were distinguishable. This perceptual asymmetry has important implications for map design. So how can we apply our understanding of color spaces to make better choices when symbolizing features? The next section introduces three foundational color schemes–qualitative, sequential and divergent–each tailored to different types of data and mapping goals. 4.4 Classification Once you’ve selected an appropriate color space, the next step is to determine how to apply color to your data. This involves choosing a classification scheme–a method for grouping data values into discrete classes, each of which is represented by a distinct color swatch. The choice of classification scheme depends on the nature of your data and the message you want your map to convey. In the following sections, we’ll explore three common approaches to classification: qualitative, sequential, and divergent. 4.4.1 Qualitative color scheme Qualitative color schemes are used to symbolize data that have no inherent order–such as categories or types. These schemes assign distinct hues to each class. To maintain perceptual balance, the hues are typically chosen to have equal lightness and saturation, ensuring that no category appears more prominent than another. Figure 4.8: Example of four different qualitative color schemes. Color hex numbers are superimposed on each palette. These schemes are ideal for mapping variables like land cover types, political affiliations, or survey responses. However, care should be taken when selecting hues, especially in contexts where cultural associations may influence interpretation. For example, it may not make sense to assign “blue” to Republican regions or “red”” to Democratic ones. Figure 4.9: Map of 2012 election results shown in a qualitative color scheme. Note the use of three hues (red, blue and gray) of equal lightness and saturation. 4.4.2 Sequential color scheme Sequential color schemes are used to represent data that follow a meaningful order, such as income, temperature, elevation, or infection rates. These schemes typically progress from light to dark, with lighter shades representing lower values and darker shades representing higher ones. This visual gradient helps convey magnitude and direction in the data. Most sequential schemes use a single hue, but some may incorporate two hues to emphasize a broader range of values. Figure 4.10: Example of four different sequential color schemes. Color hex numbers are superimposed on each palette. The following example uses a sequential color scheme to map household income in Maine using a green-based scheme. Figure 4.11: Map of household income shown in a sequential color scheme. Note the use of a single hue (green) and 7 different lightness levels. 4.4.3 Divergent color scheme Divergent color schemes are used to represent ordered data that revolve around a meaningful central value–such as a median, mean, or zero. These schemes are particularly effective when the goal is to highlight differences on either side of a reference point, making them well-suited for visualizing change, deviation, or polarity. A typical divergent scheme uses two contrasting hues—one for values below the center and one for values above. Lightness and saturation are adjusted symmetrically to reflect the magnitude of deviation from the central value. This approach helps viewers quickly identify extremes and interpret the direction and intensity of variation. The following examples showcase several divergent palettes: Figure 4.12: Example of four different divergent color schemes. Color hex numbers are superimposed onto each palette. Building on the previous example in section 4.4.2, we can also represent income data using a divergent color scheme–particularly useful when emphasizing variation around a central value. In this case, we center the scheme on the median household income of $36,641. Values below the median are shown using a brown hue, while values above the median are represented with a green-blue hue. Each hue is further divided into progressively lighter or darker shades to reflect the degree of deviation from the median, allowing for a more nuanced interpretation of income disparities across regions. Figure 4.13: This map of household income uses a divergent color scheme where two different hues (brown and blue-green) are used for two sets of values separated by the median income of $36,641. Each hue is further divided into progressively lighter or darker shades. 4.5 Choosing an Appropriate Color Scheme Selecting the right color scheme for your map depends on both the nature of your data and the number of classes you wish to represent. Fortunately, tools like ColorBrewer–developed by Cynthia Brewer and colleagues at Pennsylvania State University–offer curated palettes tailored to different data types: qualitative, sequential, and divergent. The site also provides guidance on choosing colorblind-safe schemes and palettes that translate well to grayscale, which is especially useful for print publications. One important design consideration is the number of color swatches used. ColorBrewer limits most schemes to 12 or fewer swatches, reflecting the reality that human perception struggles to reliably distinguish more than a handful of colors in a legend. For example, try matching nine shades of green in a map to their corresponding legend entries–it quickly becomes a challenge. 4.6 Classification Intervals The way data values are grouped into intervals can significantly affect how a map looks and how its patterns are interpreted. In the previous examples, two different classification schemes were used: an equal interval scheme for the sequential map and a quantile interval scheme for the divergent map. Classification intervals define how the full range of data is divided into discrete classes, each represented by a color swatch. Different schemes produce different visual outcomes, even when applied to the same dataset. For instance, equal interval schemes divide the data range into evenly spaced segments, which is useful for uniformly distributed data. In contrast, quantile schemes ensure that each class contains an equal number of features, which can help balance visual representation across a map. Other options include the Jenks natural breaks method which uses an algorithm to identify clusters in the data, optimizing class boundaries to reflect inherent groupings. The following figure compares these three schemes–quantile, equal interval, and Jenks–applied to the same income dataset. Notice how each scheme emphasizes different aspects of the distribution. Figure 4.14: Three different representations of the same spatial data using different classification intervals. The quantile interval scheme ensures that each color swatch is represented an equal number of times. If we have 20 polygons and 5 classes, the interval breaks will be such that each color is assigned to 4 different polygons. The equal interval scheme divides the range of values into equal interval widths. If the polygon values range from 10,000 to 25,000 and we have 5 classes, the intervals will be [10,000 ; 13,000], [13,000 ; 16,000], …, [22,000 ; 25,000]. The Jenks interval scheme (aka natural breaks) uses an algorithm that identifies clusters in the dataset. The number of clusters is defined by the desired number of intervals. It may help to view the breaks when superimposed on top of a distribution of the attribute data. In the following figure. the three classification intervals are superimposed on a histogram of the per-household income data. The histogram shows the distribution of values as “bins” where each bin represents a range of income values. The y-axis shows the frequency (or number of occurrences) for values in each bin. Figure 4.15: Three different classification intervals used in the three maps. Note how each interval scheme encompasses different ranges of values. 4.6.1 An Interactive Example The following interactive frame demonstrates the different “looks” a map can take given different combinations of classification schemes and class numbers. 4.7 Summary This chapter introduces key principles for symbolizing spatial data using color and classification schemes to improve map readability and interpretation. Color Dimensions: Explains how hue, lightness, and saturation represent different data types—categorical, ordered, and intensity-based—and how they interact perceptually. Color Spaces: Highlights the limitations of symmetrical models like HSV and introduces perceptually accurate alternatives such as CIELAB and Munsell. Classification Schemes: Describes three main approaches: Qualitative for unordered categories, Sequential for ordered data, Divergent for data centered around a meaningful midpoint. Color Selection Tools: Introduces ColorBrewer for choosing effective, accessible palettes. Classification Intervals: Compares equal interval, quantile, and Jenks natural breaks, showing how each affects data representation. "],["statistical-maps.html", "Chapter 5 Statistical maps 5.1 Introduction 5.2 Statistical distribution maps 5.3 Mapping uncertainty 5.4 Summary", " Chapter 5 Statistical maps 5.1 Introduction In the previous chapter, we explored how visual variables–particularly color–can be used to symbolize spatial features. We examined the perceptual dimensions of hue, lightness, and saturation, and how different color schemes (qualitative, sequential, and divergent) can be matched to the nature of the data being mapped. We also saw how classification intervals influence the appearance and interpretability of choropleth maps. This chapter builds on those foundations by shifting focus from the aesthetics and perception of map design to the statistical logic behind classification schemes. Rather than choosing breaks arbitrarily or purely for visual balance, we explore how statistical principles can guide the discretization of continuous spatial data. This includes methods such as equal intervals, quantiles, boxplots, and standard deviation units, each offering a different lens through which to interpret spatial distributions. We also extend the discussion to mapping uncertainty, a critical but often overlooked aspect of spatial analysis. Many datasets–especially those derived from surveys–carry margins of error or standard errors that affect how confidently we can interpret mapped patterns. This chapter introduces techniques for visualizing uncertainty and simulating its impact on spatial rankings and statistical relationships. 5.2 Statistical distribution maps Many spatial datasets contain continuous variables, meaning that each geographic unit–such as a polygon in a data layer–can have a unique value. When these values are mapped using a one-to-one color assignment, the result is a type of thematic map known as a choropleth map, where each polygon is shaded according to its attribute value. For example, a map of Massachusetts showing median household income with a unique color for each tract would produce a visually complex and potentially overwhelming display. Figure 5.1: Example of a continuous color scheme applied to a choropleth map. While technically accurate, such maps often obscure broader patterns and make interpretation difficult. To address this, we turn to statistical classification methods that group continuous values into meaningful categories, allowing for clearer visual communication and more effective spatial analysis. Figure 5.2: An equal interval choropleth map using 10 bins. The histogram accompanying the map is rotated vertically to align each bin with its corresponding color swatch. The length of each gray bar in the histogram reflects the number of polygons assigned to each color category, offering a quick sense of how values are distributed across the map. An equal interval classification scheme divides the full range of data values into intervals of equal width. This approach ensures that each color swatch represents the same span of values, making it easier to compare differences between categories. Because this method does not assume a central reference point, a sequential color scheme—typically progressing from light to dark—is used to convey increasing magnitude. 5.2.1 Quantile map While equal interval classification offers intuitive comparisons by assigning each color swatch an equal range of values, it can be misleading when the data are unevenly distributed. In such cases, many polygons may cluster within a few intervals, leaving others sparsely populated. This imbalance can obscure meaningful spatial patterns. Quantile classification addresses this issue by dividing the data into intervals that each contain an equal number of observations. For example, a map using six quantiles ensures that each color swatch is applied to approximately the same number of polygons. This approach enhances the map’s exploratory power and can help reveal spatial clusters that might be hidden in an equal interval map. Figure 5.3: Example of a quantile map. You’ll note the differing color swatch lengths in the color bar reflecting the different ranges of values covered by each color swatch. For example, the darkest color swatch covers the largest range of values, [131944, 250001], yet it is applied to the same number of polygons as most other color swatches in this classification scheme. 5.2.2 Boxplot map Another approach to classifying continuous spatial data is to use summary statistics that describe the distribution’s central tendency and spread. The boxplot, a common statistical visualization, provides five summary statistics including the median, the upper and lower quartiles (within which 50% of the data lie–also known as the interquartile range,IQR), and upper and lower “whiskers” that encompass 1.5 times the interquartile range. The boxplot may also display “outliers”–data points that may be deemed unusual or not characteristic of the bulk of the data. In the context of mapping, these summary statistics can be used to define classification breaks. A boxplot map applies color swatches to polygons based on where their values fall within the boxplot-defined intervals. This method is particularly useful when the goal is to highlight the shape of the distribution–whether it is symmetrical, skewed, or contains outliers. Because the boxplot includes a measure of centrality (the median), a divergent color scheme is often appropriate. This allows the map to visually emphasize deviations from the center, helping to identify regions that are unusually high or low relative to the bulk of the data. Figure 5.4: Example of a boxplot map. Boxplot maps strike a balance between statistical rigor and visual interpretability. They are especially effective when the data distribution is not normal and when understanding the spread and extremes is as important as identifying central values. 5.2.3 IQR map The interquartile range (IQR) map is a simplified version of the boxplot map that focuses on the middle 50% of the data. Instead of dividing the distribution into multiple segments, the IQR map reduces the classification to just three categories: values within the IQR, values below the IQR, and values above the IQR. This approach is particularly useful when the goal is to highlight the “core” of the distribution–those observations that are neither exceptionally high nor low. By emphasizing the middle range, the IQR map can reveal spatial patterns that are less influenced by outliers or extreme values. For example, while previous maps may have consistently emphasized an east-west gradient in income across Massachusetts, the IQR map may show that middle-income households are more evenly distributed across the state. Figure 5.5: Example of an IQR map. Visually, the IQR category benefits from being assigned a darker hue to distinguish it from the lighter tones used for the upper and lower extremes. This design choice helps draw attention to the central portion of the data while still acknowledging regions with higher and lower values. 5.2.4 Standard deviation map When a dataset approximates a normal (bell-shaped) distribution, classification based on standard deviation units can be a powerful way to highlight how values deviate from the mean. In a standard deviation map, class breaks are defined at regular intervals above and below the mean–typically at ±1, ±2, and ±3 standard deviations. This creates a symmetrical classification scheme centered on the average value. Each class represents a specific range of deviation from the mean, making it easy to identify which regions fall within the expected range and which stand out as unusually high or low. For example, areas within one standard deviation of the mean might be considered typical, while those beyond two or three standard deviations may be flagged as exceptional. This method is particularly useful when the data are approximately normally distributed and when the goal is to emphasize variation relative to the average. A divergent color scheme is typically used, with a neutral color at the mean and increasingly intense hues in opposite directions to represent values above and below the mean. This method is particularly useful when the data are approximately normally distributed and when the goal is to emphasize variation relative to the average. A divergent color scheme is typically used, with a neutral color at the mean and increasingly intense hues in opposite directions to represent values above and below the mean. Figure 5.6: Example of a standard deviation map. However, caution is warranted when applying this method to skewed data (as seems to the be the case in the working example). If the distribution is not symmetrical, the resulting map may misrepresent the data by assigning more polygons to one side of the mean. In such cases, the visual balance of the map may not reflect the actual distribution of values. 5.2.5 Outlier maps While previous classification schemes aim to represent the full range or central tendencies of a dataset, outlier maps focus specifically on identifying and emphasizing extreme values–those that fall significantly above or below the bulk of the distribution. These maps are particularly useful when the goal is to highlight regions that deviate sharply from expected norms, such as areas with unusually high income, low population density, or elevated disease rates. Outliers can be defined in several ways, depending on the statistical framework used. Examples of boxplot outlier map, standard deviation outlier map and quantile otulier map follow. 5.2.5.1 Boxplot outlier map A boxplot outlier map identifies values that fall outside the whiskers of a boxplot–typically 1.5 times the interquartile range (IQR) above the lower and upper quartiles. These regions are often assigned darker hues, while all other values are grouped into a single lighter category to draw attention to the extremes. Figure 5.7: Example of a boxplot outlier choropleth map. You’ll note the asymmetrical distribution of outliers with a little over a dozen regions with unusually high income values and just one region with unusually low income values. 5.2.5.2 standard deviation outlier map A standard deviation outlier map flags values beyond ±2 standard deviations from the mean. If the data follow a normal distribution, this corresponds to roughly the top and bottom 2.5 percent of observations. These outliers are visually emphasized using a divergent color scheme, often with neutral tones for typical values and saturated colors for extremes. Figure 5.8: Example of a standard deviation outlier choropleth map. 5.2.5.3 Quantile otulier map A quantile outlier map defines outliers based on percentile thresholds. For instance, the top and bottom 2.5% of values can be isolated by dividing the data into 40 quantiles and mapping only the outermost ones. This method is especially useful when the data distribution is skewed or non-normal. Figure 5.9: Example of a quantile outlier choropleth map where the top and bottom 2.5% regions are characterized as outliers. 5.3 Mapping uncertainty Many spatial datasets–particularly those derived from surveys like the U.S. Census Bureau’s American Community Survey (ACS)–are not direct measurements but estimates accompanied by a measure of uncertainty. This uncertainty is often expressed as a margin of error (MoE) or a standard error (SE), which reflects the confidence we have in the reported values. For example, the ACS uses a 90% confidence interval, meaning there is a 90% chance that the true value lies within the reported range. Mapping such data presents a challenge: how do we visualize both the estimate and its uncertainty in a way that supports meaningful spatial interpretation? One common approach is to display side-by-side maps—one showing the estimated values and another showing the associated SE or MoE. Figure 5.10: Maps of income estimates (left) and associated standard errors (right). An alternative is to overlay uncertainty directly onto the estimate map using textures or hatch marks. For example, a map of income estimates might use shades of green to represent income levels, with different hatch patterns indicating the degree of uncertainty. This approach allows viewers to assess both value and reliability simultaneously. Figure 5.11: Map of estimated income (in shades of green) superimposed with different hash marks representing the ranges of income SE. Another technique involves mapping the upper and lower bounds of the confidence interval as separate maps. This can help visualize the full range of possible values but may still suffer from the same interpretive challenges as side-by-side maps. Figure 5.12: Maps of top end of 90 percent income estimate (left) and bottom end of 90 percent income estimate (right). 5.3.1 Assessing confidence in spatial patterns While the maps presented earlier in this chapter offer ways to visualize uncertainty—such as margins of error or standard errors—they do not fully address a key reason we map data in the first place: to compare values across space. In spatial analysis, we are often interested in identifying patterns of high or low values and ranking regions accordingly. However, these comparisons assume that the observed estimates are stable and that their relative order will persist across samples. This assumption is problematic. To illustrate this, we begin by examining confidence interval plots for each polygon. These plots reveal that many regions have overlapping intervals, meaning that their true values could plausibly fall above or below those of neighboring regions. For example, a county that appears to have a lower income than its neighbor may, in fact, have a higher income if a different sample were taken. This overlap undermines the reliability of spatial rankings and calls into question the robustness of apparent patterns. Figure 5.13: Income estimates by county with 90 percent confidence interval. Note that many counties have overlapping estimate ranges. Consider, for instance, Piscataquis County, whose income estimate (represented by the gray point in the plot) appears lower than that of neighboring Oxford County. At first glance, this suggests a clear ranking between the two. However, when we examine their confidence intervals, we see substantial overlap—indicating that this apparent difference may not be statistically reliable. If a new sample were drawn from each county, the resulting estimates could easily shift, potentially placing Piscataquis above Oxford in income rankings. The following example illustrates how such reversals can occur when uncertainty is taken into account. Figure 5.14: Example of income estimates one could expect to sample based on the 90 percent confidence interval shown in the previous plot. In one such simulated sample, Oxford County’s income estimate drops below that of both Piscataquis and Franklin counties—reversing the original ranking. A similar shift is observed for Sagadahoc County, which falls behind two other counties, Hancock and Lincoln. These changes underscore how uncertainty can affect not just individual estimates, but the broader spatial hierarchy we infer from mapped data. What appears to be a clear pattern in the original map may, in fact, be a fragile construct shaped by sampling variability. How do the spatial patterns in the original estimated income map hold up when uncertainty is introduced? By comparing it with a simulated income map–generated from values sampled within each county’s confidence interval–we can begin to assess the stability of observed rankings and the reliability of apparent spatial patterns. Figure 5.15: Original income estimate (left) and realization of a simulated sample (right). A few more simulated samples (using the 90% confidence interval) are shown below: Figure 5.16: Original income estimate (left) and realizations from simulated samples (R2 through R5). 5.3.2 Class comparison maps Effectively conveying both estimates and their associated uncertainty in a single map remains a challenge. As Sun and Wong (Sun and Wong 2010) note, the appropriate strategy often depends on the context and purpose of the analysis. One useful approach is the class comparison method, which evaluates whether a polygon’s margin of error (MoE) extends beyond the boundaries of its assigned classification. In this method, the map displays not only the estimated value but also whether the confidence interval surrounding that estimate crosses into adjacent classes. For example, if we adopt the classification breaks [0 , 20600 , 22800 , 25000 , 27000 , 34000 ], we find that many polygons have MoEs that span multiple class boundaries. Figure 5.17: Income estimates by county with 90 percent confidence interval. Note that many of the counties’ MoE have ranges that cross into an adjacent class. Take Piscataquis county, for example. Its estimate is assigned the second classification break (20600 to 22800 ), yet its lower confidence interval stretches into the first classification break indicating that we cannot be 90% confident that the estimate is assigned the proper class (i.e. its true value could fall into the first class). Other counties such as Cumberland and Penobscot don’t have that problem since their 90% confidence intervals fall inside the classification breaks. This information can be mapped as a hatch mark overlay. For example, income could be plotted using varying shades of green with hatch symbols indicating if the lower interval crosses into a lower class (135° hatch), if the upper interval crosses into an upper class (45° hatch), if both interval ends cross into a different class (90°-vertical-hatch) or if both interval ends remain inside the estimate’s class (no hatch). Figure 5.18: Plot of income with class comparison hatches. 5.4 Summary This chapter explores statistical approaches to mapping continuous spatial data, focusing on how classification schemes influence the interpretation of choropleth maps. Building on the visual principles introduced in the previous chapter, it introduces methods such as equal interval, quantile, boxplot, interquartile range (IQR), and standard deviation classifications. Each technique offers a different lens for revealing spatial patterns and understanding data distributions. The chapter also introduces outlier maps, which emphasize extreme values using statistical definitions derived from boxplots, standard deviations, or quantiles. These maps are particularly useful for identifying regions that deviate sharply from the norm. A major emphasis is placed on mapping uncertainty–especially in datasets derived from surveys. Techniques such as confidence interval plots, simulated sample maps, and class comparison overlays are used to assess the reliability of spatial rankings and classifications. These methods highlight the fragility of apparent patterns and promote more cautious, statistically informed interpretations of mapped data. References Sun, M., and W. S. Wong. 2010. “Incorporating Data Quality Information in Mapping American Community Survey Data.” Cartography and Geography Information Science 37 (4): 285–300. "],["pitfalls-to-avoid.html", "Chapter 6 Pitfalls to avoid 6.1 Introduction 6.2 Representing Count 6.3 MAUP 6.4 Ecological Fallacy 6.5 Mapping rates 6.6 Empirical Bayes Smoothing 6.7 Summary", " Chapter 6 Pitfalls to avoid 6.1 Introduction Spatial data analysis offers powerful tools for uncovering patterns, relationships, and insights across geographic space. However, with this power comes a set of challenges that can easily mislead—even seasoned analysts–if not properly understood and addressed. This chapter explores several common pitfalls in spatial analysis, focusing on how data representation, aggregation, and interpretation can distort our understanding of spatial phenomena. We begin by examining how the choice of spatial units and aggregation schemes can dramatically alter the appearance of spatial patterns, a problem known as the Modifiable Areal Unit Problem (MAUP). We then explore the ecological fallacy, where inferences about individuals are mistakenly drawn from aggregate-level data. These issues are compounded when mapping raw counts without accounting for differences in area or population, leading to misleading choropleth maps and unstable rate estimates. To address these challenges, we introduce techniques for normalizing data, smoothing unstable rates, and framing interpretations appropriately. 6.2 Representing Count Consider a 5km × 5km study area populated by individuals arranged in a perfect grid. If we overlay two different zoning schemes—one using a uniform grid and another using irregular polygons—and count the number of individuals within each zone, we produce two choropleth maps that suggest very different spatial distributions. Yet, the underlying point pattern is identical. Figure 6.1: Layout of individual units with two different zonal unit overlays. Figure 6.2: Count of individuals in each zonal unit. Note how an underlying point distribution can generate vastly different looking choropleth maps given different aggregation schemes. This discrepancy illustrates a fundamental pitfall in spatial analysis: the appearance of spatial patterns can be heavily influenced by the choice of aggregation units (O’Sullivan and Unwin 2010). Non-uniform aerial units can create misleading impressions of clustering or dispersion, even when none exists. This phenomenon is closely related to the Modifiable Areal Unit Problem (MAUP), which is discussed in the next section. To mitigate this issue, it is often better to represent counts as ratios—for example, population per square kilometer or deaths per capita. These normalized measures reduce the influence of arbitrary zone shapes and sizes, offering a more reliable representation of spatial intensity. In Figure 6.3, switching from raw counts to density maps reveals a more consistent and interpretable spatial pattern. Figure 6.3: Point density choropleth maps. The sample study extent is 20x20 units which generates a uniform point density of 1. The slight discrepancy in values for the map on the right is to be expected given that the zonal boundaries do not split the distance between points exactly. 6.3 MAUP The Modifiable Areal Unit Problem (MAUP) (Openshaw 1983) is a well-documented issue in spatial analysis that arises when data are aggregated into spatial units such as census tracts, counties, or states. These units are often arbitrary with respect to the phenomena being studied. O’Sullivan and Unwin (O’Sullivan and Unwin 2010) further emphasize that MAUP is not just a technical nuisance–it challenges the validity of spatial inference when based on aggregated data. Continuing with the uniform point distribution from the last section, consider a uniform distribution of individuals across a study area, each with two recorded variables: v1 and v2. The variables are symbolized as varying shades of green and reds in the two left-hand maps of Figure 6.4). Figure 6.4: Plots of variables v1 and v2 for each individual in the survey. The color scheme is sequential with darker colors depicting higher values and lighter colors depicting lower values. At the individual level, a scatterplot reveals no meaningful correlation between these variables–both the slope and the coefficient of determination (\\(R^2\\)) are close to zero. However, when the same data are aggregated using a uniform zoning scheme, a weak correlation emerges. Figure 6.5: Data summarized using a uniform aggregation scheme. The resulting regression analysis is shown in the right-hand plot. Note the slight increase in slope and \\(R^2\\) values. If we then apply a non-uniform aggregation scheme, the correlation appears even stronger, with a much higher \\(R^2\\) value. Figure 6.6: Data summarized using a non-uniform aggregation scheme.The resulting regression analysis is shown in the right-hand plot. This discrepancy is not due to any real relationship between v1 and v2, but rather to the aggregation scheme itself. In fact, it is possible to design an aggregation scheme that produces a near-perfect correlation between two variables that are entirely uncorrelated at the individual level. This is the nature of MAUP: different aggregation schemes can yield dramatically different analytical outcomes! 6.4 Ecological Fallacy As is often the case, analyses can be constrained by the data at hand. It’s important to recognize the limits of what such data can tell us. A common mistake is to assume that relationships observed at the group level also apply to individuals within those groups. This misstep is known as the ecological fallacy (Freedman 1999). For example, if the analysis was conducted with the data summarized using the non-uniform aggregation scheme shown in Figure 6.6, it might be tempting to conclude that individuals with high values of v1 also tend to have high values of v2. However, this inference is not supported by the data. The observed correlation exists only at the level of the aggregated units–it may not hold, and often does not hold, at the individual level. To avoid the ecological fallacy, analysts must be cautious in how they interpret and report results from aggregated data. It is appropriate to say, “At this level of aggregation, we observe a strong relationship between v1 and v2,” but it is incorrect to claim that this relationship necessarily exists at finer scales. 6.5 Mapping rates You learned earlier in this chapter of the pitfalls in mapping raw counts without accounting for differences in the size or shape of the spatial units. To address this, counts should be normalized—either by area (e.g., people per square kilometer) or by population (e.g., deaths per 100,000 people). This transforms raw counts into rates, which are more meaningful for comparison across regions. To illustrate this, let’s examine a series of maps showing kidney cancer death rates by county for the period 1980-1984. Figure 6.7: Kidney cancer death rates for the period spanning 1980-1984. We begin by looking at the counties with the highest death rates: Figure 6.8: Top 10% of counties with the highest kidney cancer death rates. Now compare this to the counties with the lowest death rates: Figure 6.9: Bottom 10% of counties with the lowest kidney cancer death rates. At first glance, these maps suggest that both high and low rates are clustered in similar regions. In fact, many of the counties with the lowest rates are adjacent to those with the highest rates. This raises questions: If environmental factors are driving kidney cancer mortality, why would they affect one county but not its neighbor? Could differences in local regulations or healthcare access explain the pattern? Before jumping to conclusions, we need to examine the population counts behind these rates: Figure 6.10: Population count for each county. Note that a quantile classification scheme is adopted forcing a large range of values to be assigned a single color swatch. The central part of the states where we are observing both very high and very low cancer death rates have low population counts. Could population count have something to do with this odd congruence of high and low cancer rates? To explore this further, let’s examine the relationship between death rates and population counts: Figure 6.11: Plot of rates vs population counts. Note the skewed nature of both data batches. The scatterplot reveals a wide spread of rates among counties with small populations. When we transform both axes to a logarithmic scale, the pattern becomes clearer: Figure 6.12: Plot of rates vs population counts on log scales. Here, we see that as population increases, the variability in death rates decreases. This is expected: larger populations provide more stable estimates of underlying rates. In contrast, small populations produce unstable rates that can distort spatial patterns. This issue is well-documented in spatial analysis literature. O’Sullivan and Unwin (O’Sullivan and Unwin 2010) emphasize that mapping raw rates without considering population size can lead to misleading interpretations, especially in public health contexts. To illustrate the problem, consider a county with a population of 1,000. If the true death rate is 5 per 100,000, we would expect 0.05 deaths–that’s less than one person. But if one person dies, the observed rate in that county becomes 1 per 1,000, or 100 per 100,000–twenty times higher than the expected rate! This demonstrates how small denominators can inflate rates and create artificial hotspots and coldspots. Rates that are computed using relatively small “at risk” population counts are deemed unstable. The next section explores one technique–Empirical Bayes smoothing–which can help reduce the influence of unstable rates and reveals more reliable spatial patterns. 6.6 Empirical Bayes Smoothing When mapping disease rates, especially for rare conditions, small population counts can lead to unstable rate estimates. These unstable rates often manifest as extreme values—either very high or very low—in sparsely populated counties. To address this, we turn to a statistical technique known as Empirical Bayes (EB) smoothing. Let’s begin by revisiting the kidney cancer example where we observed that counties with small populations tend to produce highly variable death rates. This variability is not necessarily indicative of true differences in cancer risk–it’s often a mathematical artifact. To mitigate this, EB smoothing adjusts each county’s rate toward the overall mean, with the degree of adjustment depending on the population size. An EB smoothed representation of kidney cancer deaths gives us the following rate vs population plot: Figure 6.13: Plot of EB smoothed rates vs population counts on log scales. The variability in rates for smaller counties has decreased. The range of rate values has dropped from 0.00045 to 0.00023. Variability is still greater for smaller counties than larger ones, but not as pronounced as it was with the raw rates Let’s now examine how EB smoothing affects the spatial distribution of extreme rates. First, we look at the top 10% of counties with the highest EB-smoothed death rates: Figure 6.14: Top 10% of counties with the highest kidney cancer death rates using EB smoothing techniques. Next, we examine the bottom 10%: Figure 6.15: Bottom 10% of counties with the lowest kidney cancer death rates using EB smoothing technique. Notice how the spatial pattern has changed. High-rate counties now appear in Florida, which aligns with expectations given the region’s older population. However, it’s important to remember that EB smoothing does not uncover the true underlying rates–it simply reduces the influence of unreliable estimates. Beyond EB smoothing, other strategies for dealing with unstable rates include: Grouping small counties into larger ones–thus increasing population sample size. Increasing the study’s time interval. In this working example, data were aggregated over a five year period (1980-1984) but they could be increased by adding five more years worth of data. Grouping small counties and increasing the study’s time interval. These solutions do have their downside in that they decrease the spatial and/or temporal resolutions. A thoughtful analysis will weigh these options based on the goals of the study and the nature of the data. 6.7 Summary This chapter has explored several key pitfalls in spatial data analysis, emphasizing how choices in data representation, aggregation, and interpretation can distort our understanding of geographic patterns. We examined how raw counts can mislead when mapped without normalization, and how aggregation schemes can artificially inflate correlations–a phenomenon known as the Modifiable Areal Unit Problem (MAUP). We also discussed the ecological fallacy, where group-level patterns are incorrectly assumed to apply to individuals. Through the example of kidney cancer mortality rates, we saw how small population counts can produce unstable rates that exaggerate spatial variation. To address this, we introduced Empirical Bayes smoothing, a technique that stabilizes rate estimates by borrowing strength from the overall distribution. While EB smoothing improves interpretability, it does not reveal true underlying rates and must be used with care. References Freedman, David A. 1999. “Ecological Inference and the Ecological Fallacy.” Technical Report 549. University of California, Berkeley. https://statistics.berkeley.edu/sites/default/files/tech-reports/549.pdf. O’Sullivan, David, and David Unwin. 2010. Geographic Information Analysis. New Jersey, USA: Wiley. Openshaw, Stan. 1983. “The Modifiable Areal Unit Problem.” Concepts and Techniques in Modern Geography 38. "],["good-map-making-tips.html", "Chapter 7 Good Map Making Tips 7.1 Introduction 7.2 Elements of a Map 7.3 How to Create a Good Map 7.4 Typefaces and Fonts 7.5 Summary", " Chapter 7 Good Map Making Tips 7.1 Introduction While this course does not focus on cartography, it’s important to recognize that how we present spatial data can significantly influence how it’s interpreted. Even the most rigorous analysis can be undermined by poor map design. This chapter offers a brief overview of practical map-making principles–not as a deep dive into cartographic theory, but as a guide to help you avoid common pitfalls and make thoughtful design choices. We’ll walk through examples of both ineffective and improved maps, highlighting how layout, color, typography, and supporting elements like legends and scale bars contribute to a map’s clarity and impact. The goal here isn’t to master cartographic technique, but to develop an eye for what makes a map readable, purposeful, and appropriate for its audience. 7.2 Elements of a Map Maps often include a variety of elements: the main map body, legend, title, scale bar, orientation indicator (e.g., north arrow), inset map, and source or ancillary information. Not all elements are necessary in every map. In fact, some may be inappropriate depending on the context. For example, a scale bar may be misleading if the coordinate system used does not preserve distance uniformly across the map’s extent. The purpose and audience of the map should guide its layout. If the map is intended for inclusion in a paper or report, simplicity and clarity should take precedence. If it’s a standalone map, additional elements may be warranted. Similarly, a general audience may benefit from a cleaner, less technical presentation, while a more specialized audience might appreciate added detail. Figure 7.1: Map elements. Note that not all elements are needed, nor are they appropriate in some cases. Can you identify at least one element that does not belong in the map (hint, note the orientation of the longitudinal lines; are they parallel to one another? What implication does this have on the North direction and the placement of the North arrow? src. Esri) 7.3 How to Create a Good Map Let’s begin with a cautionary example–a map layout that demonstrates several poor design choices. Figure 7.2: Example of a bad map. Can you identify the problematic elements in this map? A good map establishes a clear visual hierarchy, ensuring that the most important elements–typically the main map body, title (if standalone), and legend–are visually dominant. Supporting elements like scale bars and north arrows should be placed lower in the hierarchy unless they serve a critical navigational purpose. When designing choropleth maps, limit the number of color swatches to fewer than twelve. Too many colors can overwhelm the viewer and make it difficult to match map regions to legend categories. Classification breaks should be chosen deliberately–quantile schemes can help distribute colors evenly across the map, while theory-driven breaks may better reflect meaningful thresholds. Scale bars and north arrows should be used judiciously. They are essential in reference maps (e.g., USGS topographic maps) but often unnecessary in thematic maps, where the focus is on comparing values across regions rather than measuring distances. If included, these elements should be visually subtle. Figure 7.3: Scale bar designs from simplest (top) to more complex (bottom). Use the simpler design if it’s to be placed low in the visual hierarchy. Title and other text elements should be concise. If the map is embedded in a report or article, these elements can be omitted in favor of figure captions and accompanying text. Let’s now look at an improved map layout that applies these principles. A divergent color scheme is used to highlight variation around a median income value. The coordinate system preserves distance and orientation, justifying the inclusion of a scale bar and north arrow. The inset map is placed low in the hierarchy and could be omitted for audiences familiar with the region. The legend is ordered to reflect the spatial gradient in the data. Figure 7.4: Example of an improved map. 7.4 Typefaces and Fonts Typography plays a subtle but important role in map design. A general rule is to limit the number of fonts to two: a serif and a sans serif font. Figure 7.5: Serif fonts are characterized by brush strokes at the letter tips (circled in red in the figure). Sans Serif fonts are devoid of brush strokes. Serif fonts are typically used for natural features such as rivers, lakes, and mountain ranges. Sans serif fonts are better suited for human-made features like roads, cities, and administrative boundaries. Avoid excessive variation in font size unless you are intentionally creating a hierarchy of labels. Likewise, stick to a single font color unless you need to distinguish between categories. The following example shows how thoughtful use of font style and color can improve legibility and feature separation. Figure 7.6: The lack of typeset differences makes the map on the left difficult to differentiate county names from lake/river names. The judicious use of font colors and style on the right facilitate the separation of features. 7.5 Summary In this chapter, we explored the essential principles of effective map design, emphasizing clarity, purpose, and audience awareness. A good map is not just visually appealing–it communicates spatial information with precision and intent. By establishing a clear visual hierarchy, limiting unnecessary elements, and choosing appropriate color schemes and fonts, a map can guide viewers toward meaningful interpretation. We also examined how scale bars, north arrows, and text elements should be used thoughtfully, and how typographic choices can enhance legibility. Through examples of poor and improved map layouts, we highlighted the importance of deliberate design decisions. Ultimately, good map making is about balancing aesthetics with function, ensuring that every element serves the story the map is meant to tell. "],["spatial-operations-and-vector-overlays.html", "Chapter 8 Spatial Operations and Vector Overlays 8.1 Introduction 8.2 Selection by Attribute 8.3 Selection by location 8.4 Vector Overlay 8.5 Summary", " Chapter 8 Spatial Operations and Vector Overlays 8.1 Introduction Spatial operations are at the heart of GIS analysis. Whether you’re querying features based on their attributes or analyzing how layers interact spatially, these tools allow us to extract meaningful insights from geographic data. This chapter introduces three core operations: selection by attribute, selection by location, and vector overlay. Each is illustrated with examples and figures to help clarify how they work in practice. 8.2 Selection by Attribute GIS layers often contain rich attribute data that can be queried to isolate features of interest. For example, if a layer represents land parcels, we might want to select all parcels with an area greater than 2.0 acres. This is where set algebra and Boolean logic come into play. 8.2.1 Set Algebra Set algebra includes basic comparison operators: less than (&lt;), greater than (&gt;), equal to (=) not equal to (&lt;&gt;). In some programming environments (such as R and Python), the equality condition is expressed using two equal signs, ==, and not one. In such an environment x = 3 is interpreted as “pass the value 3 to x” and x == 3 is interpreted as “is x equal to 3?”. Suppose we want to select all cities with a population greater than 50,000. Assuming the population field is named POP, the expression would be: &quot;POP&quot; &gt; 50000 In ArcGIS, one would use the Select Layer by Attributes tool. Figure 8.1: An example of the Select Layer by Attributes tool in ArcGIS Pro where the pull-down menu is used to define the selection. Figure 8.2: An example of the Select Layer by Attributes tool in ArcGIS Pro where the SQL syntax is used to define the selection. Figure 8.3: Selected cities meeting the criterion are shown in cyan color in ArcGIS Pro. Note that most GIS platforms do not create a new layer when performing selections—they simply highlight the matching features. 8.2.2 Boolean Algebra Boolean logic allows us to combine multiple conditions: or (either conditions can be met), and (both conditions must be met), not (condition must not be met). Let’s refine our previous example. Suppose we want to select cities with a population over 50,000 that are located in the United States. If the country field is labeled FIPS_CNTRY, the expression would be: `&quot;POP&quot; &gt; 50000 AND &quot;FIPS_CNTRY&quot; = US` Once the query is executed, the selected cities are highlighted in cyan: Figure 8.4: Selected cities meeting where POP &gt; 50000 AND FIPS_CNTRY == US criteria are shown in cyan color. 8.3 Selection by location In addition to attribute queries, GIS allows us to select features based on their spatial relationship to other layers. These relationships include: Adjacency features that share a boundary Containment features that are entirely within another Intersection features that overlap Distance features within a specified proximity For example, we might want to select cities that are within 100 miles of recorded earthquake events. The earthquake data comes from a separate layer named Earthquakes. Figure 8.5: An example of a Select Layer by Location tool in ArcGIS Pro. The spatial association chosen is distance. This generates the following selection: Figure 8.6: Selected cities meeting the criterion are shown in cyan color. 8.4 Vector Overlay Vector overlay is a foundational concept in GIS, dating back to pre-digital methods like sieve mapping, where planners used transparent layers to identify suitable areas for development. Today, vector overlay involves combining two or more layers to produce a new output layer. The three most common overlay operations are: clipping, intersecting and unioning. 8.4.1 Clipping Clipping uses one layer (the clip feature) to trim another layer (the input feature). The result is a subset of the input layer, restricted to the area defined by the clip feature. Only the attributes of the input layer are retained. Figure 8.7: The Maine counties polygon layer is clipped to the circle polygon. Note that the ouput layer is limited to the county polygon geometry and its attributes (and does not include the clipping circle polygon). 8.4.2 Intersecting Intersecting combines two layers and outputs only the features that share spatial overlap. The resulting layer inherits attributes from both input layers. Figure 8.8: The Maine counties polygon layer is intersected with the circle polygon. The ouput layer combines both intersecting geometries and attributes. 8.4.3 Unioning Unioning overlays both input layers and retains all features from both. Where features overlap, new polygons are created. The output layer typically contains more polygons than either input layer alone, and includes attributes from both sources. Areas without overlap may be assigned NULL or zero values. Figure 8.9: The Maine counties polygon layer is unioned with the circle polygon. The ouput layer combines both (complete) geometries and attributes. Where spatial overlaps do not occur, most software will either assign a NULL value or a 0. 8.5 Summary This chapter introduced key spatial operations used in GIS, focusing on how we select and manipulate geographic features based on their attributes and spatial relationships. We explored selection by attribute using set and Boolean algebra, and demonstrated how spatial selection can be based on proximity, containment, or intersection with other layers. We then examined vector overlay techniques–clip, intersect, and union–which allow us to combine multiple layers and extract meaningful spatial intersections. "],["chp09_0.html", "Chapter 9 Coordinate Systems 9.1 Introduction 9.2 Geographic Coordinate Systems 9.3 Projected Coordinate Systems 9.4 Spatial Properties 9.5 Geodesic geometries 9.6 Summary", " Chapter 9 Coordinate Systems 9.1 Introduction Every GIS dataset is tied to a spatial reference system, which defines how locations are represented and interpreted. This system may be arbitrary—such as a 10 m × 10 m sampling grid in a forest plot or the layout of a soccer field—or it may be geographic, linking spatial features to positions on the Earth’s surface. This chapter focuses on Earth-based reference systems, which fall into two main categories: Geographic Coordinate Systems (GCS) and Projected Coordinate Systems (PCS). 9.2 Geographic Coordinate Systems A geographic coordinate system (GCS) is a reference framework used to identify locations on the curved surface of the Earth. Locations are measured in angular units from the Earth’s center, relative to two intersecting planes: the equatorial plane and the prime meridian plane (which passes through Greenwich, England). A location is therefore defined by two values: a latitude and a longitude. Figure 9.1: Examples of latitudinal lines are shown on the left and examples of longitudinal lines are shown on the right. The 0° degree reference lines for each are shown in red (equator for latitudinal measurements and prime meridian for longitudinal measurements). Latitude measures the angle between the equatorial plane and a location on the Earth’s surface. Longitude measures the angle between the prime meridian plane and the north-south plane that intersects the location of interest. For example, Colby College is located at approximately 45.56° North and 69.66° West. In a GIS environment, directions are encoded using signs: North and East are assigned a positive (+) sign, while South and West are assigned a negative (-) sign. Colby College’s location is therefore encoded as +45.56° and -69.66°. Figure 9.2: A slice of Earth showing the latitude and longitude measurements. A GCS is defined by three components: ellipsoid, geoid and datum. These are introduced next. 9.2.1 Sphere and Ellipsoid Assuming the Earth is a perfect sphere simplifies mathematical calculations and is often sufficient for small-scale maps (i.e., maps that cover large areas). However, for large-scale maps where accuracy is critical, an ellipsoidal model is preferred. An ellipsoid is defined by two radii: the semi-major axis (equatorial radius) and the semi-minor axis (polar radius). Figure 9.3: Ellipsoid axes. The Earth’s slightly ellipsoidal shape results from its rotation, which induces a centripetal force along the equator. This causes the equatorial radius to be approximately 21 km longer than the polar radius. Figure 9.4: The Earth can be mathematically modeled as a simple sphere (left) or an ellipsoid (right). Thanks to satellite and computational technologies, we now have precise estimates of these radii: the semi-major axis is 6,378,137 meters, and the semi-minor axis is 6,356,752 meters. Although the differences between distances measured on a sphere versus an ellipsoid are small, they are measurable–up to 20 km in some cases–as illustrated in the following lattice plots. Figure 9.5: Differences in distance measurements between the surface of a sphere and an ellipsoid. Each graphic plots the differences in distance measurements made from a single point location along the 0° meridian identified by the green colored box (latitude value) to various latitudinal locations along a longitude (whose value is listed in the bisque colored box). For example, the second plot from the top-left corner plot shows the differences in distance measurements made from a location at 90° north (along the prime meridian) to a range of latitudinal locations along the 45° meridian. 9.2.2 Geoid The geoid represents the Earth’s true shape as defined by its gravitational potential. Unlike the smooth surface of an ellipsoid, the geoid has subtle undulations caused by variations in gravitational pull across the planet. These undulations are not visible to the naked eye but are measurable and can influence positional accuracy. In this context, we are not concerned with topographic features like mountains or ocean trenches. Instead, we focus on the gravitational surface—imagine the Earth completely submerged in water and measure the distance from the center of the Earth to the water surface at every location. Figure 9.6: Earth’s EGM 2008 geoid. The ondulations depicted in the graphics are exaggerated x4000. The Earth’s gravitational field is dynamic, influenced by the movement of its molten core. As a result, the geoid is constantly changing, albeit over long time scales. The science of measuring and modeling the Earth’s shape is known as geodesy, a branch of applied mathematics. 9.2.3 Datum To reconcile the simplicity of an ellipsoidal model with the complexity of the geoid, we align the two. This alignment defines a datum. A datum allows us to map the Earth’s surface features onto an ellipsoid (or sphere) in a way that approximates the geoid. The alignment can be: Local, where the ellipsoid is closely fitted to the geoid at a specific location (e.g., Kansas), or Geocentric, where the ellipsoid is aligned with the Earth’s center of mass. Figure 9.7: Alignment of a geoid with a spheroid or ellipsoid help define a datum. 9.2.3.1 Local Datum Figure 9.8: A local datum couples a geoid with the ellipsoid at a location on each element’s surface. There are many local datums, both historical and modern. The choice of datum is typically driven by geographic context. For example: NAD27 (North American Datum of 1927) is widely used in the U.S., especially in older maps. ED50 (European Datum of 1950) is common in Western Europe. WGS72 (World Geodetic System 1972) was developed for global use by the U.S. Department of Defense. 9.2.3.2 Geocentric Datum Figure 9.9: A geocentric datum couples a geoid with the ellipsoid at each element’s center of mass. Modern datums typically use a geocentric alignment. Examples include: NAD83 (North American Datum of 1983) ETRS89 (European Terrestrial Reference System 1989) WGS84 (World Geodetic System 1984) Most modern datums use either the WGS84 or GRS80 ellipsoid, which have nearly identical dimensions: a semi-major axis of 6,378,137 meters and a semi-minor axis of 6,356,752 meters. 9.2.4 Building the Geographic Coordinate System A Geographic Coordinate System is largely defined by two components: The ellipsoid model, and The way this ellipsoid is aligned with the geoid (i.e., the datum). It is important to know which GCS is associated with a GIS dataset or map document. This is especially critical when combining layers that use different datums. A single location on Earth can have different coordinate values depending on the GCS used. For example, a point recorded as 44.56698° N, 69.65939° W in NAD27 may appear as 44.56704° N, 69.65888° W in NAD83, and as 44.37465° N, –69.65888° W in a spherical WGS84 system. Without proper metadata, these differences can lead to misaligned features on a map. This is analogous to recording temperature in different units–Celsius, Fahrenheit, or Kelvin. Each unit yields a different numeric value, but all refer to the same physical quantity. Figure 9.10: Map of the Colby flagpole in two different geographic coordinate systems (GCS NAD 1983 on the left and GCS NAD 1927 on the right). Note the offset in the 44.5639° line of latitude relative to the flagpole. Also note the 0.0005° longitudinal offset between both reference systems. 9.3 Projected Coordinate Systems The surface of the Earth is curved, but maps are flat. A projected coordinate system (PCS) provides a framework for identifying locations and measuring features on a flat (map) surface. It consists of a grid formed by lines that intersect at right angles. Projected coordinate systems—based on Cartesian coordinates—have an origin, an x-axis, a y-axis, and a linear unit of measure. Transforming data from a geographic coordinate system (GCS) to a PCS requires mathematical projection. The many types of projections can be grouped into three broad categories: planar, cylindrical and conical. 9.3.1 Planar Projections A planar projection (also known as an azimuthal projection) maps Earth’s surface features onto a flat plane that touches the globe at a single point (tangent case), Alternatively, the plane may intersect the globe along a line (a secant case). Planar projections are often used for mapping polar regions but can be applied to any location on Earth. When the point of contact is not at the poles, the projection is referred to as an oblique planar projection. Figure 9.11: Examples of three planar projections: orthographic (left), gnomonic (center) and equidistant (right). Each covers a different spatial range (with the latter covering both northern and southern hemispheres) and each preserves a unique set of spatial properties. 9.3.2 Cylindrical Projection A cylindrical projection maps Earth’s surface onto a cylinder, which is then unrolled into a flat map. The cylinder may touch the globe along a single line of tangency (a tangent case), or it may intersect the globe along two lines (a secant case). The cylinder is typically tangent to the equator, but it can also be oblique. A special case is the transverse aspect, where the cylinder is tangent to a meridian (line of longitude). This is the basis for the Universal Transverse Mercator (UTM) and State Plane coordinate systems. The UTM PCS divides the globe into zones, each 6° wide. This zonal structure limits the spatial extent of each projection. For example, the state of Maine (USA) uses UTM Zone 19 North for most of its statewide GIS maps. Most USGS quadrangle maps are also based on the UTM system. Common datums used with UTM in the U.S. include NAD27 and NAD83, though a WGS84-based UTM system also exists. Distortion is minimized along the tangent or secant lines and increases with distance from them. Figure 9.12: Examples of two cylindrical projections: Mercator (preserves shape but distortes area and distance) and equa-area (preserves area but distorts shape). 9.3.3 Conical Projection A conical projection maps Earth’s surface onto a cone. Like cylindrical projections, the cone may touch the globe along a single line of tangency (a tangent case), or it may intersect the globe along two lines (a secant case). Distortion is minimized along the tangent or secant lines and increases with distance from them. For mapping the contiguous 48 U.S. states, conical projections are often preferred. Use an Equidistant Conic projection when preserving distance is important, or an Albers Equal Area Conic projection when preserving area is the priority. Conical projections are also widely used in European maps, including the Europe Albers Equal Area Conic and Europe Lambert Conformal Conic projections. Figure 9.13: Examples of three conical projections: Albers equal area (preserves area), equidistant (preserves distance) and conformal (preserves shape). 9.4 Spatial Properties All map projections introduce distortion to real-world geographic features. The four spatial properties most affected by projection are: shape, area, distance and direction. A map that preserves shape is called conformal; one that preserves area is called equal-area; one that preserves distance is called equidistant; and one that preserves direction is called azimuthal. For most GIS applications (e.g. ArcGIS and QGIS), many of the built-in projections are named after the spatial properties they preserve. Each projection type is typically optimized to preserve one or two of these properties. When working with small-scale maps (i.e., maps covering large areas), and when multiple spatial properties are important, it is best to split the analysis across different projections to minimize distortion-related errors. To assess how a projection distorts spatial properties across a study region, one can generate Tissot indicatrix (TI) ellipses. The idea is to project a small circle–small enough that distortion remains relatively uniform across its extent–and examine its transformed shape on the map. For example, to evaluate distortion in a Mollweide projection across the continental U.S., a grid of circles can be generated at regular latitudinal and longitudinal intervals. Note the varying levels of distortion type and magnitude across the region. Let’s zoom in on a Tissot circle centered at 44.5°N and 69.5°W (near Waterville Maine): The plot shows a perfect circle (filled in bisque) that would be expected if no distortion were present. The blue ellipse (the indicatrix) represents the transformed circle for this particular projection and location. The green and red lines indicate the magnitude and orientation of the ellipse’s major and minor axes, respectively. These lines can also be used to assess scale distortion, which may vary depending on direction (bearing). The green line shows the direction of maximum scale distortion. The red line shows the direction of minimum scale distortion. These directions are sometimes referred to as the principal directions. In this example, the principal scale values are 1.1293 and 0.8856. A scale value of 1 indicates no distortion; values less than 1 indicate a smaller-than-true scale, and values greater than 1 indicate a larger-than-true scale. It’s important to note that scale distortion does not necessarily imply area distortion. In fact, for this projection, area is relatively well preserved despite directional scale distortion. Area distortion can be computed by multiplying the two principal scale values. In this example, the area distortion is 1.0001–effectively negligible. The dashed north-south line in the graphic shows the orientation of the meridian, while the dotted east-west line shows the orientation of the parallel. It’s important to remember that these distortions occur at the center of the TI and may not reflect distortion across the entire region covered by the TI circle. 9.5 Geodesic geometries Projected coordinate systems introduce geometric measurement errors due to the nature of the projection. Specifically, the distance between two points on a sphere or ellipsoid is difficult to replicate accurately on a projected (flat) surface unless the points are relatively close to one another. In many cases, such errors are tolerable if the expected level of precision is met. Other sources of error in spatial representation often outweigh those introduced by projection. However, when the scale of analysis is large (e.g., covering a continent like North America), the measurement errors associated with a projected coordinate system may become unacceptable. One way to circumvent these limitations is to adopt a geodesic solution. A geodesic distance is the shortest distance between two points on an ellipsoid (or spheroid). Likewise, a geodesic area is one measured directly on an ellipsoid. These measurements are independent of the underlying projected coordinate system. In fact, the Tissot circles presented in the previous section were all generated using geodesic geometry. To illustrate the benefits of geodesic geometry, consider the following example. The map below compares the shortest distance between two points located on opposite sides of the Atlantic. The blue solid line represents the shortest distance on a planar coordinate system, while the red dashed line represents the shortest distance on a spheroid. At first glance, the geodesic line may appear nonsensical due to its curvature on the projected map. However, this curvature is a byproduct of increasing distortion in the reference system as one moves poleward. To better visualize the geodesic path, we can display both the geodesic and planar distance layers on a 3D globe–or a projection that mimics the Earth as viewed from space, centered on the midpoint of the geodesic segment. So, if geodesic measurements are more precise than planar ones, why not use geodesic geometry for all spatial operations? In many cases, geodesic methods are perfectly acceptable–and even encouraged. However, they come with a computational cost. It is far more efficient to compute area and distance on a plane than on a spheroid. Geodesic calculations lack simple algebraic solutions and often require iterative approximations, which can be computationally taxing when processing millions of line segments. It’s also important to note that not all geodesic implementations are created equal. Some algorithms prioritize speed over precision, which may lead to reduced accuracy. In R, several packages support geodesic measurements: geosphere (based on the authoritative GeographicLib libraries), lwgeom (an R binding to the liblwgeom libraries), s2 is an implementation of Google’s spherical measurement library. 9.6 Summary This chapter introduces the foundational concepts of spatial referencing in GIS, focusing on how locations on Earth are represented using coordinate systems. It begins with Geographic Coordinate Systems (GCS), which use angular measurements (latitude and longitude) based on an ellipsoid model of the Earth. The chapter explains how GCS is built from three components–ellipsoid, geoid, and datum–and highlights the importance of datum alignment in determining accurate coordinates. Examples of local and geocentric datums (e.g., NAD27, NAD83, WGS84) illustrate how coordinate values can vary depending on the chosen reference system. The chapter then transitions to Projected Coordinate Systems (PCS), which transform the curved surface of the Earth into a flat map using mathematical projections. It categorizes projections into planar, cylindrical, and conical types, each with specific use cases and distortion characteristics. The concept of spatial distortion is explored through Tissot’s indicatrix, which visualizes how projections affect shape, area, distance, and direction. Finally, the chapter introduces geodesic geometries as a more accurate alternative for measuring distances and areas on an ellipsoid. While geodesic calculations offer precision, they are computationally intensive. "],["chp10_0.html", "Chapter 10 Map Algebra 10.1 Introduction 10.2 Local operations and functions 10.3 Focal operations and functions 10.4 Zonal operations and functions 10.5 Global operations and functions 10.6 Operators and functions 10.7 Summary", " Chapter 10 Map Algebra 10.1 Introduction Cartographic modeling is a structured approach to geographic analysis that uses a language of operations to interpret spatial data. These operations are organized into functional categories that describe how data values are transformed and combined across geographic space. The modeling system, as developed by Dana Tomlin (Tomlin 1990) is built around three core spatial contexts: Local operations: Compute new values for each location based on data explicitly associated with that location, often across one or more layers. Focal operations: Compute values based on the surrounding neighborhood of each location. These neighborhoods can be defined by distance, direction, travel cost, or visibility. Zonal operations: Aggregate data across defined zones, which are mutually exclusive and collectively exhaustive areas of the study region. In addition to these, O’Sullivan and Unwin (O’Sullivan and Unwin 2010) propose a fourth category: global operations. These refer to analytical functions that consider all locations in a dataset simultaneously, such as calculating overall statistics, evaluating spatial autocorrelation, or computing Euclidean distances to nearest features. 10.2 Local operations and functions Local operations are the most fundamental type of cartographic modeling function. They compute a new value for each location based solely on the data explicitly associated with that same location—either from a single layer or from multiple layers. For instance, starting with an original raster layer, we might apply a sequence of operations—first multiplying each cell value by 2, then adding 1. The result is a new raster in which each cell reflects the cumulative effect of those operations applied to the corresponding cell in the original layer. This is an example of a unary operation, where only a single raster is involved in the computation. Figure 10.1: Example of a local operation where output=(2 * raster + 1). Local operations can also involve multiple raster layers. For example, two rasters can be added together by summing the values of overlapping cells, resulting in a new raster where each cell reflects the combined value from the corresponding cells in the input layers. Figure 10.2: Example of a local operation where output=(input1+input2). Note how each cell output only involves input raster cells that share the same exact location. Local operations also include reclassification, where a range of input raster values is assigned a new, common value. This technique is useful for grouping data into meaningful categories. For example, we might reclassify the input raster values as follows: Original values Reclassified values 0-25 25 26-50 50 51-75 75 76-100 100 Figure 10.3: Example of a local operation where the output results from the reclassification of input values. 10.3 Focal operations and functions Focal operations differ from local operations in that they consider the values of neighboring cells when computing a new value for each location. Instead of looking only at a single cell, focal functions examine a defined neighborhood–such as the eight adjacent cells surrounding a central location. For example, a focal operation might calculate the average of all neighboring cell values and assign that average to the central cell. This approach is useful for smoothing data, detecting spatial patterns, and identifying transitions across a surface. Figure 10.4: Example of a focal operation where the output cell values take on the average value of neighboring cells from the input raster. Focal cells surrounded by non-existent cells are assigned an NA in this example. Notice in the example above that the edge cells in the output raster have been assigned a value of NA (No Data). This occurs because the neighborhood around those edge cells extends beyond the raster’s boundary, where no values exist. Some GIS applications handle this differently—they may ignore the missing surrounding cells and compute the average using only the available neighboring values, as demonstrated in the next example. Figure 10.5: Example of a focal operation where the output cell values take on the average value of neighboring cells from the input raster. Surrounding non-existent cells are ignored. Focal (or neighborhood) operations require the definition of a window region, commonly referred to as a kernel. In the examples above, a simple 3×3 kernel was used, where each cell’s value was influenced by its immediate neighbors. The kernel itself can vary in both size and shape—for instance, a 3×3 square where the central cell is excluded (resulting in eight neighbors), or a circular neighborhood defined by a specified radius. Figure 10.6: Example of a focal operation where the kernel is defined by a 3 by 3 cell without the center cell, and whose output cell takes on the average value of those neighboring cells. n addition to defining the shape and size of the neighborhood, a kernel also specifies the weight each neighboring cell contributes to the summary statistic. For example, in a 3×3 neighborhood, each cell might contribute an equal weight of 1/9th to the final value. However, weights can also be defined using more complex functions–these are known as kernel functions. One commonly used kernel function is the Gaussian, which assigns greater weight to cells closer to the center and less to those farther away. Figure 10.7: Example of a focal operation where the kernel is defined by a Gaussian function whereby the closest cells are assigned a greater weight. 10.4 Zonal operations and functions Zonal operations differ from local and focal operations in that they summarize values across defined zones rather than individual cells or neighborhoods. A zone is a group of cells that share a common value on a separate raster layer—often representing administrative boundaries, land parcels, or other categorical regions. In the following example, the cell values from the raster layer are aggregated into three zones whose boundaries are delineated by red polygons. Each output zone shows the average value of the cells within that zone. Figure 10.8: Example of a zonal operation where the cell values are averaged for each of the three zones delineated in red. While rasters can be used to define zones, it’s more common for a GIS to have a polygon layer define the zones. 10.5 Global operations and functions Global operations and functions may use some or all input cells when computing the value of each output cell. A common example is the Euclidean Distance tool, which calculates the shortest straight-line distance between each cell and a specified source or destination. In the example below, a new raster assigns to each cell the distance to the nearest cell with a value of 1–note that only two such cells exist in the input raster. Figure 10.9: Example of a global function: the Euclidean distance. Each pixel is assigned its closest distance to one of the two source locations (defined in the input layer). Global operations and functions can also generate single value outputs such as the overall pixel mean or standard deviation. Another popular use of global functions is in the mapping of least-cost paths where a cost surface raster is used to identify the shortest path between two locations which minimizes cost (in time or money). 10.6 Operators and functions Operations and functions applied to gridded data can be broken down into three groups: mathematical, logical comparison and Boolean. 10.6.1 Mathematical operators and functions Two mathematical operators have already been demonstrated in earlier sections: the multiplier and the addition operators. Other operators include division and the modulo (also known as the modulus) which is the remainder of a division. Mathematical functions can also be applied to gridded data manipulation. Examples are square root and sine functions. The following table showcases a few examples with ArcGIS and R syntax. Operation ArcGIS Syntax R Syntax Example Addition + + input1 + input2 Subtraction - - input1 - input2 Division / / input1 / input2 Modulo Mod() %% Mod(input1, 100), input1 %% 10 Square root SquareRoot() sqrt() SquareRoot(input1), sqrt(input1) 10.6.2 Logical comparison The logical comparison operators evaluate a condition then output a value of 1 if the condition is true and 0 if the condition is false. Logical comparison operators consist of greater than, less than, equal and not equal. Logical comparison Syntax Greater than &gt; Less than &lt; Equal == Not equal != For example, the following figure shows the output of the comparison between two rasters where we are assessing if cells in input1 are greater than those in input2 (on a cell-by-cell basis). Figure 10.10: Output of the operation input1 &gt; input2. A value of 1 in the output raster indicates that the condition is true and a value of 0 indicates that the condition is false. When assessing whether two cells are equal, some programming environments such as R and ArcGIS’s Raster Calculator require the use of the double equality syntax, ==, as in input1 == input2. In these programming environments, the single equality syntax is usually interpreted as an assignment operator so input1 = input2 would instruct the computer to assign the cell values in input2 to input1 (which is not what we want to do here). Some applications make use of special functions to test a condition. For example, ArcGIS has a function called Con(condition, out1, out2) which assigns the value out1 if the condition is met and a value of out2 if it’s not. For example, ArcGIS’s raster calculator expression Con( input1 &gt; input2, 1, 0) outputs a value of 1 if input1 is greater than input2 and 0 if not. It generates the same output as the one shown in the above figure. Note that in most programming environments (including ArcGIS), the expression input1 &gt; input2 produces the same output because the value 1 is the numeric representation of TRUE and 0 that of FALSE. 10.6.3 Boolean (or Logical) operators In map algebra, Boolean operators are used to compare conditional states of a cell (i.e. TRUE or FALSE). The three Boolean operators are AND, OR and NOT. Boolean ArcGIS R Example AND &amp; &amp; input1 &amp; input2 OR | | input1 | input2 NOT ~ ! ~input2, ! input2 A “TRUE” state is usually encoded as a 1 or any non-zero integer while a “FALSE” state is usually encoded as a 0. For example, if cell1=0 and cell2=1, the Boolean operation cell1 AND cell2 results in a FALSE (or 0) output cell value. This Boolean operation can be translated into plain English as “are the cells 1 and 2 both TRUE?” to which we answer “No they are not” (cell1 is FALSE). The OR operator can be interpreted as “is x or y TRUE?” so that cell1 OR cell2 would return TRUE. The NOT interpreter can be interpreted as “is x not TRUE?” so that NOT cell1 would return TRUE. Figure 10.11: Output of the operation input1 AND input2. A value of 1 in the output raster indicates that the condition is true and a value of 0 indicates that the condition is false. Note that many programming environments treat any none 0 values as TRUE so that -3 AND -4 will return TRUE. Figure 10.12: Output of the operation NOT input2. A value of 1 in the output raster indicates that the input cell is NOT TRUE (i.e. has a value of 0). 10.6.4 Combining operations Both comparison and Boolean operations can be combined into a single expression. For example, we may wish to find locations (cells) that satisfy requirements from two different raster layers: e.g. 0&lt;input1&lt;4 AND input2&gt;0. To satisfy the first requirement, we can write out the expression as (input1&gt;0) &amp; (input1&lt;4). Both comparisons (delimited by parentheses) return a 0 (FALSE) or a 1 (TRUE). The ampersand, &amp;, is a Boolean operator that checks that both conditions are met and returns a 1 if yes or a 0 if not. This expression is then combined with another comparison using another ampersand operator that assesses the criterion input2&gt;0. The amalgamated expression is thus ((input1&gt;0) &amp; (input1&lt;4)) &amp; (input2&gt;0). Figure 10.13: Output of the operation ((input1&gt;0) &amp; (input1&lt;4)) &amp; (input2&gt;0). A value of 1 in the output raster indicates that the condition is true and a value of 0 indicates that the condition is false. Note that most software environments assign the ampersand character, &amp;, to the AND Boolean operator. 10.7 Summary In this chapter, we explored the foundational concepts of Map Algebra, a framework for performing spatial analysis using raster data. We examined how local, focal, zonal, and global operations each contribute to different types of spatial reasoning. Local operations focus on individual cells, focal operations incorporate neighborhood context, zonal operations summarize values across defined regions, and global operations consider the entire raster dataset. References O’Sullivan, David, and David Unwin. 2010. Geographic Information Analysis. New Jersey, USA: Wiley. Tomlin, Dana C. 1990. GIS and Cartographic Modeling. New Jersey: Prentice Hall. "],["chp11_0.html", "Chapter 11 Analyzing Spatial Patterns 11.1 Introduction 11.2 From Maps to Models: Defining Spatial Patterns Statistically 11.3 Why Distinguishing These Effects Matters 11.4 Spatial Data Types and Applicability of First- and Second-Order Effects 11.5 Quantifying Patterns vs. Hypothesis Testing 11.6 A Note on Terminology: Why First-Order and Second-Order? 11.7 Summary 11.8 Suggested resources", " Chapter 11 Analyzing Spatial Patterns 11.1 Introduction Spatial data are pervasive in daily life and scientific research, from disease outbreaks to precipitation distribution. We often look at these maps and intuitively recognize patterns: clusters of events, gradients across space, or areas of unusual activity. But how do we move from visual impressions to statistical analysis? This chapter lays the conceptual foundation for spatial statistical analysis by introducing spatial processes, which generate the observed patterns we analyze. We begin by defining what it means to quantify a spatial pattern, and then explore how spatial processes can be characterized through two key lenses: first-order effects, which describe broad spatial trends, and second-order effects, which capture local interactions or dependencies. Understanding this distinction is essential for interpreting spatial patterns. It helps us answer questions like: Is this cluster of events meaningful, or could it have occurred by chance? Are nearby observations influencing each other, or is there an underlying gradient driving the pattern? 11.2 From Maps to Models: Defining Spatial Patterns Statistically Spatial processes may include both deterministic and stochastic components. The deterministic part reflects systematic variation—such as a spatial trend or the influence of known covariates—while the stochastic part captures random variation or uncertainty, including spatial dependence. Most statistical techniques in spatial analysis (including those covered in subsequent chapters) combine these elements allowing us to describe both the predictable and unpredictable aspects of spatial patterns. To analyze these patterns, we need a statistical baseline–a null model that represents the absence of spatial structure. The most common is Complete Spatial Randomness (CSR), which assumes: No First-Order Effects: Every location has an equal probability of hosting an event, meaning the event intensity or density is constant across the study area. No Second-Order Effects: The location of one event is independent of the location of another, implying no spatial interaction or clustering/dispersion among events. For point patterns this is characterized as a homogeneous Poisson process where events occur independently and are uniformly distributed across the study region. For attribute data (such as values measured at fixed locations or aggregated to areas) the analogous null hypothesis often assumes that observed spatial variation is purely random. CSR provide a reference against which observed patterns can be compared. Deviations from these benchmarks of no spatial structure suggest the presence of non-random spatial structure, which can manifest as either large-scale variation (first-order process) or local spatial dependence (second-order process). It is important to note that distinguishing between these first- and second-order effects from a single observed pattern can be challenging. 11.2.1 First-Order Effects: Spatial Trend and Intensity First-order effects describe the large-scale variation or spatial trend of a phenomenon across space. For point patterns, this involves how the event intensity (the average rate or expected number of events) varies. For attribute data (such as continuous fields or values aggregated to areas), it refers to how the mean or expected value of the attribute changes systematically across the study region. These effects reflect broad, underlying factors that influence the spatial distribution of the observed phenomena Examples: Tree density increasing with elevation or soil type. Crime rates higher in urban centers due to population density. Rainfall decreasing as one moves westward. If not accounted for, strong first-order trends can create the illusion of clustering or dispersion. This highlights a crucial challenge in spatial analysis: it can be difficult to distinguish between first-order variation and true inter-point interaction (second-order effects) from a single observed pattern. For instance, a gradient in tree density might appear as a cluster when viewed on a map, even if no local interaction exists. 11.2.2 Second-Order Effects: Interaction and Autocorrelation Second-order effects describe local dependencies–how the presence or value of one observation influences others nearby. Examples: Trees clustering due to seed dispersal. High crime rates spreading to adjacent neighborhoods due to local dynamics. For point data, these are often called interactions between events, while for continuous field data, they are referred to as spatial autocorrelation. Analyzing second-order properties typically involves assessing patterns of interaction beyond any underlying spatial trend or varying intensity. Figure 11.1: Tree distribution can be influenced by 1st order effects such as elevation gradient or spatial distribution of soil characteristics; this, in turn, changes the tree density distribution across the study area. Tree distribution can also be influenced by 2nd order effects such as seed dispersal processes where the process is independent of location and, instead, dependent on the presence of other trees. 11.3 Why Distinguishing These Effects Matters First-order and second-order effects offer complementary insights into spatial patterns. An observed cluster might be due to a spatial trend (first-order), local interaction (second-order), or both. However, it is often difficult to distinguish these effects in practice from a single observed pattern. Failing to distinguish between these can lead to incorrect conclusions about the underlying process, a common pitfall in spatial analysis. From a single realization, it is mathematically impossible to distinguish between a process of independent events generated under a heterogeneous intensity and one of dependent events generated under homogeneous intensity. In practice, realistic null hypotheses often account for first-order effects when testing for second-order ones. For example, we might use a heterogeneous Poisson process to model varying intensity across space. This model assumes that event intensity or the likelihood of observing a point changes spatially due to external factors, but critically, it still assumes that events occur independently of each other. Thus, it serves as a baseline to test for clustering or interaction beyond any underlying spatial trend 11.4 Spatial Data Types and Applicability of First- and Second-Order Effects We learned in chapter 2 that spatial data can be broadly categorized into two types: object-based, which treat the world as discrete entities located in space (e.g., points for crime locations or tree positions, or polygons for administrative boundaries), and field-based, which represent continuous phenomena measured across space (such as temperature, elevation, or precipitation). These distinct representations influence the methods used to analyze spatial patterns. Regardless of the data type, the concepts of first-order and second-order effects apply. For point data, first-order effects describe variations in event intensity across space, while second-order effects capture interactions between events. For continuous field data, first-order effects describe spatial trends in attribute values, and second-order effects reflect spatial autocorrelation in those values. Understanding the nature of the data helps determine the appropriate analytical techniques, but the overarching framework of first-order and second-order effects remains relevant across data types. 11.5 Quantifying Patterns vs. Hypothesis Testing Spatial statistical analysis involves both descriptive and inferential approaches. Descriptive methods quantify spatial patterns using metrics such as density/intensity, clustering indices, or autocorrelation coefficients. These tools help summarize and visualize spatial structure. Inferential methods, on the other hand, involve hypothesis testing to assess whether observed patterns deviate significantly from a null model such as CSR. For example, we might test whether clustering in a point pattern exceeds what would be expected under a random process, or whether spatial autocorrelation in a field is statistically significant. Distinguishing between quantifying and testing is important: the former describes what is observed, while the latter evaluates whether those observations are likely to have occurred by chance. 11.6 A Note on Terminology: Why First-Order and Second-Order? The terms first-order and second-order effects in spatial statistics are inspired by the first and second moments of statistical distributions. The first moment refers to the mean or expected value, which aligns with first-order effects describing average intensity or spatial trend. The second moment relates to variance and covariance, which corresponds to second-order effects capturing spatial dependence or interaction. This terminology provides a conceptual bridge between spatial statistics and classical statistical theory, reinforcing the idea that spatial patterns can be rigorously analyzed using familiar statistical principles. 11.7 Summary This chapter lays the conceptual foundation for spatial statistical analysis. By distinguishing between first-order and second-order effects, we gain a clearer understanding of spatial processes and avoid common pitfalls in interpretation. The chapters that follow build on the concepts of first-order and second-order effects while introducing a logical progression of analytical tools: Chapter 12: Modeling Spatial Trend Focuses on quantifying first-order effects (broad spatial trends) using polynomial functions to describe how a variable changes across space, often as a preliminary step to remove global variation Chapter 13: Spatial Autocorrelation Introduces spatial autocorrelation as a second-order property, exploring how values at one location relate to nearby locations using measures like Global and Local Moran’s I in continuous or areal data. Chapter 14: First-Order Point Pattern Analysis: Modeling Spatial Intensity Examines first-order effects in point patterns by detecting and modeling variations in event intensity across space, using descriptive density measures and Poisson point process models (PPM) to test for covariate influences. Chapter 15: Second-Order Point Pattern Analysis: Modeling Distance Analyzes second-order effects (spatial interaction, clustering, or dispersion) in point patterns through distance-based methods like Average Nearest Neighbor (ANN), Ripley’s K and L functions, and the Pair Correlation Function (PCF), often using Monte Carlo tests Chapter 16: Interpolation Explores techniques such as inverse distance weighting (IDW), trend surfaces, and kriging to estimate unknown values at unsampled locations using spatial data. 11.8 Suggested resources The following list of resources is designed for readers wanting to learn more about spatial statistics, ordered by increasing complexity: Geographic Information Analysis (O’Sullivan and Unwin 2010) Interactive Spatial Data Analysis (Bailey and Gatrell 1995) Spatial Data Science (with applications in R) (Pebesma and Bivand 2021) Spatial Point Patterns, Methodology and Applications with R (Baddeley, Rubak, and Turner 2016) Applied Spatial Statistics for Public Health Data (Waller and Gotway 2004) Spatial-Temporal Statistics with R (C. K. Wikle 2019) Spatial Data Analysis: Theory and Practice (Haining 2004) References Baddeley, Adrian, Ege Rubak, and Rolf Turner. 2016. Spatial Point Patterns, Methodology and Applications with r. Florida: CRC Press. Bailey, Trevor C., and Anthony C. Gatrell. 1995. Interactive Spatial Data Analysis. England: Prentice Hall. C. K. Wikle, N. Cressie, A. Zammit-Mangion. 2019. Spatio-Temporal Statistics with r. Chapman &amp; Hall/CRC. https://spacetimewithr.org/. Haining, Robert. 2004. Spatial Data Analysis: Theory and Practice. Cambridge. O’Sullivan, David, and David Unwin. 2010. Geographic Information Analysis. New Jersey, USA: Wiley. Pebesma, Edzer, and Roger Bivand. 2021. Spatial Data Science (with Applications in r). https://r-spatial.org/book/. Waller, Lance A., and Carol A. Gotway. 2004. Applied Spatial Statistics for Public Health Data. Wiley. "],["chp12_0.html", "Chapter 12 Modeling Spatial Trends 12.1 Introduction 12.2 Polynomial functions 12.3 Polynomial functions in one dimension 12.4 Transitioning from a one-dimensional to a two-dimensional space 12.5 Evaluating a model’s goodness of fit 12.6 A note about adopting traditional statistical measures of significance to spatial functions 12.7 Summary", " Chapter 12 Modeling Spatial Trends 12.1 Introduction Polynomial functions are a foundational tool for modeling spatial trends–particularly first-order effects–in spatial data. These models help describe how a variable changes across space due to absolute location, rather than proximity to neighboring values. In spatial statistics, modeling trend is often a critical first step before exploring second-order properties such as spatial autocorrelation. Polynomial functions are parametric, meaning their form is predefined and their coefficients are estimated by fitting the model to data. In practice, they are used to: Describe broad spatial variation in a variable. Remove global trend to expose residuals for further analysis. Support interpolation across unsampled locations. In spatial analysis, polynomial functions typically consist of non-negative integer powers of the x and y coordinates. These models are most appropriate for spatially continuous fields–phenomena that can be measured at any location within the study area using ratio or interval scales (e.g., precipitation, elevation, population density). They are not suitable for discrete features such as crime events or tree locations. While this chapter focuses on raster data, the principles of polynomial trend modeling apply across spatial data models, including point and polygon formats. The goal is to capture the overall spatial pattern without overfitting local variation. To that end, it is best to use the lowest polynomial order necessary to represent the trend. In the sections that follow, we will: Explore polynomial functions in one and two dimensions. Learn how to fit and evaluate models using regression techniques. Use residuals to assess model fit. 12.2 Polynomial functions Polynomial functions are mathematical models that seek to describe global trends in a dataset using the general form: \\[ \\begin{equation} \\tag{1} z_i = \\beta \\boldsymbol{s_i} + \\varepsilon_i \\end{equation} \\] where \\(z_i\\) is a scalar of interest (e.g., precipitation, income, elevation, etc.) and \\(\\boldsymbol{s_i}\\) denotes location in one, two or n-dimensional space. The term \\(\\varepsilon\\) refers to the residuals–i.e., parts of the observed data not explained by the model (sometimes described as the error term in the field of statistics). The subscript \\(i\\) reminds us that the value \\(z\\) and its residual are a function of location \\(i\\). In spatial analysis, we usually work in two-dimensional space, hence location \\(s\\) is defined by the coordinate pair (x, y). Thus, equation (1) can be expressed as a multivariate regression with the x and y coordinate values being two independent variables. \\[ \\begin{equation} \\tag{2} z_i = \\beta_0 + \\beta_1 x_i + \\beta_2 y_i + \\beta_3 x_i^2 + \\beta_4 y_i^2 + ... + \\varepsilon_i \\end{equation} \\] Here, the ellipsis “…” is a placeholder for higher order polynomial values and interaction terms. Fitting a polynomial function requires two steps: (1) defining the model (in this case the polynomial order)–this can be linear, quadratic, cubic, quartic, quintic, etc., (2) then fitting the model to the data using, for example, ordinary least squares regression methods. Because distances in the x and y directions are assumed to be in the same units and scale (i.e., a unit change in x is equal to a unit change in y), it is best to work off a projected (aka Cartesian) coordinate system. Hence, the projected coordinates system should do a decent job in preserving distance within the study extent. 12.3 Polynomial functions in one dimension It may be best to first explore a polynomial function in one dimension. In what follows, we will work with the distribution of 2020 total precipitation for the state of Kansas (USA). The following figure shows a raster representation of the 2020 total precipitation (in mm) for Kansas. Figure 12.1: Map of the total 2020 precipitation distribution for Kansas (USA). However, we will first focus on the bottom row of pixels which will provide us with a one-dimensional (east-west) dataset. Figure 12.2: Row of pixels used to explore the trend along a one-dimensional space. A scatter plot of the precipitation values along the east-west gradient is shown next. Each point represents the precipitation value at a specific pixel location. Figure 12.3: Scatter plot of precipitation along the east-west gradient defined in figure 12.2. The simplest model that can be fitted to the data is the mean precipitation value–a constant. It can be visualized as a horizontal line. This is also known as a 0th order polynomial model where: \\[ \\begin{equation} \\tag{3} Precipitation_i = \\beta_0 x_i^0 + \\varepsilon = \\beta_0 + \\varepsilon \\end{equation} \\] Here, \\(\\beta_0\\) is nothing more than the mean precipitation value–756.8 mm in our working example. Recall that we are only focusing on the east-west precipitation values, hence the \\(y\\) coordinate value is ignored in equation (3). Figure 12.4: The mean precipitation fitted to the data. This is the simplest form of a polynomial function where the power is set to 0. While such a simple model may not prove useful in modeling trends, it serves as a reference for how well other polynomial models perform. Model evaluation will be addressed later in this chapter. It is clear from the plot that the precipitation is far from constant across the east-west slice of the data. It shows a distinct upward trend that may be better represented using a sloped line. We can modify the above model by augmenting the polynomial order: \\[ \\begin{equation} \\tag{4} Precipitation_i = \\beta_0 + \\beta_1 x_i^1 + \\varepsilon_i \\end{equation} \\] Equation (4) is mathematically defined as a first-order polynomial model, but it is more often described as a linear model. With the model defined a priori, the model is then fitted to the points. Figure 12.5: A first-order polynomial function being fitted to the precipitation data. This is often referred to as a linear trend model. There are many approaches to finding the “best fit” with the most popular being the ordinary least squares method. This approach seeks to minimize the distance between the points and the fitted model when measured parallel to the y-axis. These are shown as red colored line segments in the following plot. Figure 12.6: The first-order polynomial function (blue line) is shown with the residual lines (in red). The function is fitted to the data in such a way to reduce the sum of the squared residual values. The distance between the modeled line and each precipitation value is the residual (\\(\\varepsilon_i\\)). This is part of the data not explained by the model. Fitting the first-order polynomial to the data gives us the coefficients of 305.8 and 1.6 for \\(\\beta_0\\) and \\(\\beta_1\\) respectively. The \\(\\beta_1\\) coefficient tells us that for every km increase in the (positive) x direction, precipitation increases by 1.6 mm. Note that the interpretation of the coefficient is dependent on the mapping units used. Typically, projection units are in meters or feet. But, when working at a scale like the State of Kansas, it is best to report changes in value as a function of larger mapping units such kilometers or miles. Statisticians rarely stop at this stage when fitting a model to the data. It is good practice to plot the residuals against the x value. This diagnostic plot is useful in assessing if the overall trend in the data was properly characterized by our polynomial model. To help guide our eyes, we add a non-parametric curve to the data which does not make any assumptions about the pattern in the data. Here, we make use of a loess fit but note that many other non-parametric models can be used for this purpose. Figure 12.7: The residuals are checked for any lingering pattern that may have been missed by the first-order polynomial function. A non-parametric line such as a loess is used to identify any pattern in the data. The diagnostic plot suggests that the first-order polynomial model we adopted does not fully capture the overall trend in the data. There appears to be some curvature in the residuals. This may suggest that we increase the polynomial order by an extra degree. A second-order polynomial model in a one-dimensional space takes on the form: \\[ \\begin{equation} \\tag{5} Precipitation_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i \\end{equation} \\] This model generates a curved fit often called a quadratic model. Figure 12.8: A second-order polynomial function is fitted to the data. This is also referred to as a quadratic function. The residual diagnostic plot for this model shows a significant improvement in that the non-parametric fit (dashed blue line) does not show an overall trend in the data suggesting that a second-order polynomial model is a good fit for our data. Figure 12.9: The lack of an overall pattern in the residuals suggests that the second-order polynomial function does a decent job in capturing the trend in precipitation. A unique characteristic of spatial data is the presence of autocorrelation which is very much apparent in both the original scatter plot and the diagnostic plot. One might be tempted to try and capture this non-random pattern in a polynomial model. For example, fitting a 20th order polynomial model would do a decent job in capturing both the overall trend and localized variation as shown in the following plot. Figure 12.10: An example of a higher polynomial order function fitted to the data. This is an example of an overfitted model. But, doing so is ill-advised–the model has 20 coefficients! Polynomial models are intended to capture general trends in the data. There are more sophisticated and specialized statistical methods used to capture localized spatial autocorrelation. A general rule of thumb is to avoid going above a 3rd order polynomial model. Any higher order model is likely to capture some aspects of spatial autocorrelation. 12.4 Transitioning from a one-dimensional to a two-dimensional space The polynomial functions described in the earlier section can easily be extended to a two-dimensional space. Next, we fit polynomial functions to the full precipitation extent for the state of Kansas. This adds the y-coordinate variable to the model. We learned that the 0th order polynomial is a horizontal line in one dimensional space. For a two-dimensional space, this is a horizontal plane that can be defined as: \\[ \\begin{equation} \\tag{6} Precipitation_i = \\beta_0 + \\beta_1 x_i^0 + \\beta_2 y_i^0 + \\varepsilon_i = \\beta^{&#39;} + \\varepsilon_i \\end{equation} \\] where \\(\\beta^{&#39;} = (\\beta_0 + \\beta_1 + \\beta_2)\\) is the mean precipitation for the entire study extent–660 mm in our working example. The following figure shows this plane in a three-dimensional space. Figure 12.11: A 0th order polynomial function fitted to the full two-dimensional precipitation data. The z-axis is assigned the precipitation variable. An east-west trend is apparent in the three-dimensional scatter plot suggesting that we should seek a first-order polynomial, at the very least. The model takes on the form: \\[ \\begin{equation} \\tag{7} Precipitation_i = \\beta_0 + \\beta_1 x_i + \\beta_2 y_i + \\varepsilon_i \\end{equation} \\] The model defines a plane that can be tilted in any direction. As was the case with the one-dimensional dataset, we can leverage the least squares method to fit the model to the data. This results in the following coefficients: \\[ \\begin{equation} \\tag{8} Precipitation_i = 402 + 1.15 x_i - 0.45 y_i + \\varepsilon_i \\end{equation} \\] The coefficients tell us by how much the precipitation changes along the x and y coordinate directions. In our working example, precipitation increases by 1.15 mm in the x direction (west to east) and decreases by 0.45 mm in the y direction (south to north). The precipitation gradient is greater in the x-direction than in the y direction. The following figure shows a three-dimensional rendering of the plane. Figure 12.12: A first-order polynomial function fitted to the two-dimensional precipitation data. The same model can be viewed as a two-dimensional raster where each pixel represents the modeled precipitation. Figure 12.13: A raster view of the first-order polynomial function shown in Figure 11.12. How well does the first-order polynomial fit our data? We may first want to look at the residuals. Figure 12.14: The residuals from the fitting of the first-order polynomial function. Note the cluster of low precipitation values revealed by removing the trend. The overall trend observed in the original raster is no longer obvious. However, subtle trends are more difficult to make out when viewing a two-dimensional dataset than a one-dimensional dataset. A more effective way to assess model fit in two-dimensional space is to split the residuals into their x and y coordinate components by plotting the residuals as a function of x and y respectively. Figure 12.15: The residuals from the fitting of the first-order polynomial function as viewed from the x-coordinate axis and the y-coordinate axis. The plots suggest a very mild curvature in the residuals along the x coordinate. A similar, but more complex curvature can be observed along the y-coordinate. It may be worthwhile to augment the function by another polynomial order giving us a second-order polynomial function to see if it removes some of these trends. Such a function is also called a quadratic polynomial function. It takes on the form: \\[ \\begin{equation} \\tag{9} Precipitation_i = \\beta_0 + \\beta_1 x_i + \\beta_2 y_i + \\beta_3 x_i^2 + \\beta_4 y_i^2 + \\beta_5 x_iy_i + \\varepsilon_i \\end{equation} \\] Note that in addition to adding the squared coordinate values, the function also adds the interactive terms \\(xy\\). The interaction term captures situations where the coefficients for \\(x\\) or \\(y\\) are no longer a constant. For example, if the effect of the x coordinate value on precipitation changes as a function of changing y coordinate values, then x’s coefficient can no longer be deemed a constant. This is captured by the \\(x_iy_i\\) term in equation 9. The least squares method can be used to fit the function to the data. In our example, this gives us: \\[ \\begin{equation} \\tag{10} Precipitation_i = 378 + 1.43 x_i -0.66 y_i -0.0003 x_i^2 +0.0013 y_i^2 -0.0005 x_iy_i + \\varepsilon_i \\end{equation} \\] The coefficients suggest that the \\(x_i\\) and \\(y_i\\) coordinate values have a greater influence on the estimate of precipitation than their \\(x_i^2\\) and \\(y_i^2\\) counterparts despite the \\(x\\) and \\(y\\) terms being raised to the power of 2. Note too that the exceedingly small \\(x_iy_i\\) coefficient suggests little to no interaction between the \\(x_i\\) and \\(y_i\\) coordinate values. The next figure shows this curved surface in a three-dimensional space. Figure 12.16: A second-order polynomial function fitted to the precipitation data. Note the slight curvature in the trend. The next figure shows the quadratic surface as a two-dimensional raster. Figure 12.17: A raster view of the second-order polynomial function shown in Figure 16. Note that the resulting residuals look barely different from those of the 1st order polynomial model. Figure 12.18: The residuals from the fitting of the second-order polynomial function. It is therefore best to assess the goodness of fit by plotting the residuals against the x and y coordinate values. Figure 12.19: The residuals from the fitting of the first-order polynomial function as viewed from the x-coordinate axis and the y-coordinate axis. The second-order polynomial is an improvement over the first-order polynomial, especially along the x-coordinate axis where the slight curvature observed in the first-order polynomial model’s residuals is no longer apparent. Note that the range of residual values are also slightly less with the 2nd order function versus the first-order function. Mathematically, there is no limit to the polynomial order you choose. For example, you could explore a third order polynomial function (a cubic polynomial) or even a fourth order polynomial function (a quartic polynomial). But, as was forewarned with the one-dimensional analysis presented earlier in this chapter, doing so increases the chance of overfitting the data. Recall that the purpose of modeling the trend is to capture the data’s overall pattern. Polynomial functions are not well suited to capturing local variability in the data. One option that may help eliminate the need for higher polynomial orders is to transform (aka re-express) the variable \\(z_i\\). Transforming the data can render a curvilinear function straight (or flat in a two-dimensional space). Transforming the variable may also be scientifically justified. There are many datasets that benefit from a non-linear scale. For example, earthquake magnitudes are measured on a log scale and some count datasets can benefit from a square root transformation. 12.5 Evaluating a model’s goodness of fit The diagnostic plots have, so far, been visual. We can adopt a more rigorous and systematic approach to evaluating how well the functions fit the data. A popular statistical measure of “fit” is the coefficient of determination, \\(R^2\\). It is computed by subtracting one from the ratio of the sum of residuals squared in the fitted model to the sum of residuals squared one would get if we had fitted the mean value. \\[ \\begin{equation} \\tag{11} R^2 = 1 - \\frac{\\sum(residuals_{model})^2}{\\sum(residuals_{mean})^2} \\end{equation} \\] If the residuals from the model are smaller than those we would have gotten if we adopted the simpler mean model, \\(R^2\\) ends up being large. \\(R^2\\) ranges from 0 (no improvement whatsoever over the mean) to 1 (a perfect fit where the residuals from the fitted model are zero). In our working example, the first-order polynomial model resulted in an \\(R^2\\) of 0.87. The second-order polynomial resulted in an \\(R^2\\) of 0.90–a slight improvement over the first-order polynomial. Note that the \\(R^2\\) value is sometimes reported as a percentage rather than a fraction. 12.6 A note about adopting traditional statistical measures of significance to spatial functions Because underlying residuals are likely to exhibit spatial autocorrelation, traditional methods used to assess “statistical significance” of a model and its coefficient should be avoided. For example, the cluster of low residual values in the eastern part of Kansas is a good indication that spatial autocorrelation is present in our data. 12.7 Summary This chapter introduces polynomial functions as a parametric tool for modeling first-order spatial effects–the broad, location-driven variation in spatial data. Polynomial models help describe global trends in continuous fields such as precipitation or elevation, and are commonly used to: Quantify spatial trend across one- or two-dimensional space. Remove global variation to expose residuals for further analysis. Support interpolation at unsampled locations. Modeling spatial trend can be a critical first step in spatial statistics. Once global variation is accounted for using polynomial functions, the residuals can be analyzed to uncover second-order properties such as spatial autocorrelation. "],["chp13_0.html", "Chapter 13 Spatial Autocorrelation 13.1 Introduction 13.2 Global Moran’s I 13.3 Moran’s I at different distance bands 13.4 Local Moran’s I 13.5 Moran’s I equation explained 13.6 Summary", " Chapter 13 Spatial Autocorrelation 13.1 Introduction In the previous chapter, we explored how polynomial functions can be used to model spatial trend—capturing broad, location-driven variation in continuous spatial fields. By fitting and removing global trends, we revealed residuals that may still exhibit spatial structure. This chapter builds on that foundation by introducing spatial autocorrelation, a second-order property that describes how values at one location relate to values at nearby locations. This concept is rooted in Tobler’s First Law of Geography which states: “The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler 1970) This principle underpins much of spatial analysis. It suggests that spatial data often exhibit non-random patterns, where nearby features tend to have similar attribute values. Spatial autocorrelation provides a formal way to quantify this tendency. To make the mathematical foundations of autocorrelation more accessible, we shift from raster-based data to a simpler dataset composed of polygonal areal units (e.g., counties). This allows us to clearly illustrate how spatial relationships are defined, how spatial weights are constructed, and how measures like Moran’s I are computed. This chapter focuses on: - Defining spatial autocorrelation and its role in spatial statistics. - Constructing spatial weights to represent neighborhood relationships. - Computing and interpreting global and local Moran’s I statistics. - Using permutation-based hypothesis testing to assess significance. 13.2 Global Moran’s I While maps can sometimes reveal clusters of similar values, our visual interpretation is often limited—especially when patterns are subtle or complex. To move beyond subjective impressions, we need a quantitative and objective measure of spatial patterning. Specifically, we want to quantify the degree to which similar attribute values are clustered or dispersed across space. One widely used statistic for this purpose is **Moran’s *I , a measure of global spatial autocorrelation**. It quantifies the overall tendency for features with similar values to be located near one another, based on a defined spatial relationship (e.g., contiguity or distance). In essence, Moran’s I is a correlation coefficient between a variable and its spatially lagged counterpart. 13.2.1 Computing the Moran’s I Let’s start with a working example: 2020 median per capita income for the state of Maine. Figure 13.1: Map of 2020 median per capita income for Maine counties (USA). It may seem apparent that, when aggregated at the county level, the income distribution appears clustered with high counties surrounded by high counties and low counties surrounded by low counties. But a qualitative description may not be sufficient; we might want to quantify the degree to which similar (or dissimilar) counties are clustered. One measure of this type of relationship is the Moran’s I statistic. The Moran’s I statistic is a measure of spatial autocorrelation that quantifies the degree to which similar values (like income) cluster together in space. It is computed as the correlation between a variable and its spatially lagged counterpart, based on a defined spatial weights matrix. But before we go about computing this correlation, we need to come up with a way to define a neighbor. One approach is to define a neighbor as being any contiguous polygon. For example, the northern most county (Aroostook), has four contiguous neighbors while the southern most county (York) has just two contiguous counties. Other neighborhood definitions can include distance bands (e.g. counties within 100 km) and k nearest neighbors (e.g. the 2 closest neighbors). Note that distance bands and k nearest neighbors are usually measured using the polygon’s centroids and not their boundaries. Figure 13.2: Maps show the links between each polygon and their respective neighbor(s) based on the neighborhood definition. A contiguous neighbor is defined as one that shares a boundary or a vertex with the polygon of interest. Orange numbers indicate the number of neighbors for each polygon. Note that the top most county has no neighbors when a neighborhood definition of a 100 km distance band is used (i.e. no centroids are within a 100 km search radius) Once we’ve defined a neighborhood for our analysis, we identify the neighbors for each polygon in our dataset then summaries the values for each neighborhood cluster (by computing their mean values, for example). This summarized neighborhood value is sometimes referred to as a spatially lagged value (Xlag). In our working example, we adopt a contiguity neighborhood and compute the average neighboring income value (Incomelag) for each county in our dataset. We then plot Incomelag against Income for each county. The Moran’s I coefficient between Incomelag and Income is nothing more than the slope of the least squares regression line that best fits the points after standardizing both variables (i.e., converting them to z-scores), which centers them on the mean and scales them by their standard deviation. Figure 13.3: Scatter plot of spatially lagged income (neighboring income) vs. each countie’s income. If we equalize the spread between both axes (i.e. convert to a z-value) the slope of the regression line represents the Moran’s I statistic. If there is no degree of association between Income and Incomelag, the slope will be close to flat (resulting in a Moran’s I value near 0). In our working example, the slope is far from flat with a Moran’s I value is 0.28. So this raises the question: how significant is this Moran’s I value (i.e. is the computed slope significantly different from 0)? There are two approaches to estimating the significance: an analytical solution and a Monte Carlo solution. The analytical solution makes some restrictive assumptions about the data and thus cannot always be reliable. Another approach (and the one favored here) is a Monte Carlo test. This approach avoids parametric assumptions about the data distribution or spatial layout, relying instead on the assumption that values are exchangeable under the null hypothesis. 13.2.2 Monte Carlo approach to estimating significance In a Monte Carlo test–specifically a permutation-based bootstrap test–we assess the significance of an observed Moran’s I statistic by comparing it to a distribution of values generated under the null hypothesis of spatial randomness. This is done by permuting the attribute values across spatial units while keeping the spatial structure (e.g., neighborhood relationships) fixed. For each permutation, the attribute values are shuffled among the polygons and a new Moran’s I value is computed. Figure 13.4: Results from 199 permutations. Plot shows Moran’s I lines (in gray) computed from each random permutation of income values. The observed Moran’s I line for the original dataset is shown in red. Repeating this process many times yields a sampling distribution of Moran’s I values that would be expected if the observed values were randomly distributed across space. Figure 13.5: Histogram shows the distribution of Moran’s I values for all 199 permutations; red vertical line shows our observed Moran’s I value of 0.28. In our working example, 199 simulations indicate that our observed Moran’s I value of 0.28 is not a value we would expect to compute if the income values were randomly distributed across each county. A pseudo p-value can easily be computed from the simulation results: \\[ \\dfrac{N_{extreme}+1}{N+1} \\] where \\(N_{extreme}\\) is the number of simulated Moran’s I values more extreme than our observed statistic and \\(N\\) is the total number of simulations. Here, out of 199 simulations, just three simulated I values were more extreme than our observed statistic, \\(N_{extreme}\\) = 3, so \\(p\\) is equal to (3 + 1) / (199 + 1) = 0.02. This is interpreted as “there is a 2% probability that we would be wrong in rejecting the null hypothesis Ho.” Note that in this permutation example, we shuffled the observed income values among polygons without replacement–this is referred to as a permutation-based randomization. This should not be confused with distribution-based simulation where new values are randomly generated from a theoretical distribution and assigned to features. Alternatively, one can choose to randomly assign a set of values to each feature in a data layer from a theorized distribution (for example, a Normal distribution). This may result in a completely different set of values for each permutation outcome. Note that you would only adopt this approach if the theorized distribution underpinning the value of interest is known a priori. Another important consideration when computing a permutation-based p-value from a permutation test is the number of simulations to perform. In the above example we ran 199 permutations, thus, the smallest p-value we could possibly come up with is 1 / (199 + 1) or a p-value of 0.005. You should therefore chose a number of permutations, \\(N\\), large enough to allow for finer resolution of p-values and more robust inference. 13.3 Moran’s I at different distance bands So far we have looked at spatial autocorrelation where we define neighbors as all polygons sharing a boundary with the polygon of interest. We may also be interested in studying the ranges of autocorrelation values as a function of distance. The steps for this type of analysis are straightforward: Define a neighborhood structure based on a specific distance band Compute lagged values for the defined set of neighbors. Calculate the Moran’s I value for the defined neighborhood. Repeat for additional distance bands to observe how autocorrelation changes with spatial scale. For example, the Moran’s I values for income distribution in the state of Maine at distances of 75, 125, up to 325 km are presented in the following plot: Figure 13.6: Moran’s I at different spatial lags defined by a 50 km width annulus at 50 km distance increments. Red dots indicate Moran I values for which a P-value was 0.05 or less. The plot suggests that there is significant spatial autocorrelation between counties within 25 km of one another, but as the distances between counties increases, autocorrelation shifts from positive to negative, indicating that nearby counties tend to have similar income levels, while counties farther apart tend to be more dissimilar. 13.4 Local Moran’s I We can decompose the global Moran’s I into a localized measure of autocorrelation–i.e. a map of “hot spots” and “cold spots”. A local Moran’s I analysis is best suited for relatively large datasets, especially if a hypothesis test is to be implemented. We’ll therefore switch to another dataset: Massachusetts household income data. Applying a contiguity based definition of a neighbor, we get the following scatter plot of spatially lagged income vs. income. Figure 13.7: Grey vertical and horizontal lines define the mean values for both axes values. Red points highlight counties with relatively high income values (i.e. greater than the mean) surrounded by counties whose average income value is relatively high. Likewise, dark blue points highlight counties with relatively low income values surrounded by counties whose average income value is relatively low. Note that the mean value for Income, highlighted as light grey vertical and horizontal lines in the above plot, carve up the four quadrant defining the low-low, high-low, high-high and low-high quadrants when starting from the bottom-left quadrant and working counterclockwise. Note that other measures of centrality, such as the median, could be used to delineate these quadrants. The values in the above scatter plot can be mapped to each polygon in the dataset as shown in the following figure. Figure 13.8: A map view of the low-low (blue), high-low (light-blue), high-high (red) and low-high (orange) counties. Each spatial unit (e.g., polygon) has its own Local Moran’s I statistic, denoted \\(I_i\\), which quantifies the degree of spatial autocorrelation around that unit. The calculation of \\(I_i\\) is shown later in the chapter. While the scatterplot helps visually identify potential clusters, statistical significance must be assessed to determine whether these patterns are likely to have occurred by chance. As with the global Moran’s I, there is both an analytical and Monte Carlo approach to computing significance of \\(I_i\\). In the case of a Monte Carlo approach, the values of all other units are shuffled while keeping the value of unit \\(i\\) fixed. For each permutation, a new \\(I_i\\) is computed based on the randomized neighborhood values. For each permutation, we compare the value at \\(y_i\\) to the average value of its neighboring values. From the permutations, we generate a distribution of \\(I_i\\) values (for each \\(y_i\\) feature) we would expect to get if the values were randomly distributed across all features. To illustrate, consider a polygon in eastern Massachusetts with a high observed \\(I_i\\) value. We assess its significance by comparing it to a distribution of \\(I_i\\) values generated through permutation. Figure 13.9: Polygon whose significance value we are asessing in this example. Its local Moran’s I statistic is 0.85. A permutation test shuffles the income values around it, all the while keeping its value constant. An example of an outcome of a few permutations follows: Figure 13.10: Local Moran’s I outcomes of a few permutations of income values. You’ll note that even though the income value remains the same in the polygon of interest, its local Moran’s I statistic will change because of the changing income values in its surrounding polygons. If we perform many more permutations, we come up with a distribution of \\(I_i\\) values under the null that the income values are randomly distributed across the state of Massachusetts. The distribution of \\(I_i\\) for the above example is plotted using a histogram. Figure 13.11: Distribution of \\(I_i\\) values under the null hypothesis that income values are randomly distributed across the study extent. The red vertical line shows the observed \\(I_i\\) for comparison. About 9.3% of the simulated values are more extreme than our observed \\(I_i\\) giving us a permutation-based p-value of 0.09. If we perform this permutation for all polygons in our dataset, we can map the permutation-based p-values for each polygon. Note that here, we are mapping the probability that the observed \\(I_i\\) value is more extreme than expected (equivalent to a one-tail test). Figure 13.12: Map of the permutation-based p-values for each polygons’ \\(I_i\\) statistic. One can use the computed permutation-based p-values to filter the \\(I_i\\) values based on a desired level of significance. For example, the following scatter plot and map shows the high/low “hotspots” for which a permutation-based p-value of 0.05 or less was computed from the above simulation. Figure 13.13: Local Moran’s I values having a signifcance level of 0.05 or less. You’ll note that the levels of significance do not apply to just the high-high and low-low regions, they can apply to all combinations of highs and lows. Another example, where the \\(I_i\\) values are filtered based on a more stringent significance level of 0.01, is shown in the following figure. Figure 13.14: Local Moran’s I values having a signifcance level of 0.01 or less. 13.4.1 A note about interpreting the permutation-based p-value While the permutation technique highlighted in the previous section used in assessing the significance of localized spatial autocorrelation does a good job in circumventing limitations found in a parametric approach to inference (e.g. skewness in the data, irregular grids, etc…), it still has some limitations when one interprets this level of significance across all features. For example, if the underlying process that generated the distribution of income values was truly random, the outcome of such a realization could look like this: Figure 13.15: Realization of a random process. Left map is the distribution of income values under a random process. Right map is the outcome of a permutation test showing the computed permutation-based p-values. Each permutation test runs 9999 permutations. Note how some \\(I_i\\) values are associated with low permutation-based p-value. In fact, one polygon’s \\(I_i\\) value is associated with permutation-based p-value of 0.001 or less! This is to be expected since the likelihood of finding a significant \\(I_i\\) increases the more permutation tests are performed. Here we have 343 polygons, thus we ran 343 tests. This is analogous to having multiple opportunities to pick an ace of spades from a deck of cards–the more opportunities you have, the greater the chance of grabbing an ace of spades. Another example of a local Moran’s I test on a realization of a randomly reshuffled set of income values is shown next: Figure 13.16: Another realization of a random process. Left map is the distribution of income values under a random process. Right map is the outcome of a permutation test showing computed permutation-based p-values. We have quite a few \\(I_i\\) values associated with a very low permutation-based p-value (The smallest p-value in this example is 0.0006). This would lead one to assume that several polygons exhibit significant levels of spatial autocorrelation with their neighbors when in fact this is due to chance–a classic example of a Type I error resulting from multiple comparisons. To further illustrate the issue, consider generating 200 independent realizations of a random spatial process. Each realization involves computing permutation-based p-values for all polygons, resulting in a large number of tests and an increased likelihood of false positives. This generates 200 x 323 permutation-based p-values. With this working example, about 10% of the outcomes result in a permutation-based p-value of 0.05 or less. Figure 13.17: Distribution of permutation-based p-values following 200 realizations of a random process. Blue numbers are the p-value bins, red numbers under each bins are calculated percentages. This problem is known in the field of statistics as the multiple comparison problem. Several solutions have been offered, each with their pros and cons. One solution that seems to have gained traction as of late is the False Discoverer Rate correction (FDR). The FDR correction aims to control the expected proportion of false positives among the set of features deemed significant. There are at least a couple of different implementation of FDR. One implementation starts by ranking the permutation-based p-values, \\(p\\), from smallest to largest and assigning a rank, \\(i\\), to each value. Next, a reference value is computed for each rank as \\(i(\\alpha/n)\\) where \\(\\alpha\\) is the desired significance cutoff value (0.05, for example) and \\(n\\) is the total number of observations (e.g. polygons) for which a permutation-based p-value has been computed (343 in our working example). All computed \\(p_i\\) values that are less than \\(i(\\alpha/n)\\) are deemed significant at the desired \\(\\alpha\\) value. Applying the FDR correction to the permutation-based p-value cutoff of 0.05 in the household income data gives us the following cluster map. Figure 13.18: Clusters deemed significant at the p=0.05 level when applying the FDR correction. Applying the FDR correction to the permutation-based p-value cutoff of 0.01 generates even fewer clusters: Figure 13.19: Clusters deemed significant at the p=0.01 level when applying the FDR correction. It’s important to note that there is no one perfect solution to the multiple comparison problem. This, and other challenges facing inferential interpretation in a spatial framework such as the potential for overlapping neighbors (i.e. same polygons used in separate permutation tests) make traditional approaches to interpreting significance challenging. It is thus recommended to interpret these tests with caution. 13.5 Moran’s I equation explained The Moran’s I statistic can be expressed in several mathematically equivalent forms, each offering different insights into its structure and interpretation. One commonly used formulation is: \\[ I = \\frac{N}{\\sum\\limits_i (X_i-\\bar X)^2} \\frac{\\sum\\limits_i \\sum\\limits_j w_{ij}(X_i-\\bar X)(X_j-\\bar X)}{\\sum\\limits_i \\sum\\limits_j w_{ij}} \\tag{1} \\] Here, \\(N\\) is the total number of spatial units, \\(X_i\\) and \\(X_j\\) are the attribute values at locations \\(i\\) and \\(j\\), \\(\\bar{X}\\) is the mean of \\(X\\), and \\(w_{ij}\\) is the spatial weight between locations \\(i\\) and \\(j\\)–typically defined by contiguity, distance, or other spatial relationships. There a few key components of Equation (1) worth highlighting. First, you’ll note the standardization of both sets of values by the subtraction of each value in \\(X_i\\) or \\(X_j\\) by the mean of \\(X\\). This highlights the fact that we are seeking to compare the deviation of each value from an overall mean and not the deviation of their absolute values. Second, you’ll note an inverted variance term on the left-hand side of equation (1)–this is a measure of spread. You might recall from an introductory statistics course that the variance can be computed as: \\[ s^2 = \\frac{\\sum\\limits_i (X_i-\\bar X)^2}{N}\\tag{2} \\] Note that a more common measure of variance, the sample variance, where one divides the above numerator by \\((n-1)\\) can also be adopted in the Moran’s I calculation. Equation (1) is thus dividing the large fraction on the right-hand side by the variance. This has for effect of limiting the range of possible Moran’s I values between -1 and 1 (note that in some extreme cases, \\(I\\) can take on a value more extreme than [-1; 1]). We can re-write the Moran’s I equation by plugging in \\(s^2\\) as follows: \\[ I = \\frac{\\sum\\limits_i \\sum\\limits_j w_{ij}\\frac{(X_i-\\bar X)}{s}\\frac{(X_j-\\bar X)}{s}}{\\sum\\limits_i \\sum\\limits_j w_{ij}} \\tag{3} \\] Note that here \\(s\\times s = s^2\\). You might recognize the numerator as a sum of the product of standardized z-values between neighboring features. If we let \\(z_i = \\frac{(X_i-\\bar X)}{s}\\) and \\(z_j = \\frac{(X_j-\\bar X)}{s}\\), The Moran’s I equation can be reduced to: \\[ I = \\frac{\\sum\\limits_i \\sum\\limits_j w_{ij}(z_i\\ z_j)}{\\sum\\limits_i \\sum\\limits_j w_{ij}} \\tag{4} \\] Recall that we are comparing a variable \\(X\\) at \\(i\\) to all of its neighboring values at \\(j\\). More specifically, we are computing a summary value (such as the mean) of the neighboring values at \\(j\\) and multiplying that by \\(X_i\\). So, if we let \\(y_i = \\sum\\limits_j w_{ij} z_j\\), the Moran’s I coefficient can be rewritten as: \\[ I = \\frac{\\sum\\limits_i z_i y_i}{\\sum\\limits_i \\sum\\limits_j w_{ij}} \\tag{5} \\] So, \\(y_i\\) is the average z-value for the neighboring features thus making the product \\(z_i y_i\\) nothing more than a correlation coefficient. The product \\(z_iy_i\\) is a local measure of spatial autocorrelation, \\(I_i\\). If we don’t summarize across all locations \\(i\\), we get our local I statistic, \\(I_i\\): \\[ I_i = z_iy_i \\tag{6} \\] The global Moran’s I statistic, \\(I\\), is thus the average of all \\(I_i\\) values. \\[ I = \\frac{\\sum\\limits_i I_i}{\\sum\\limits_i \\sum\\limits_j w_{ij}} \\tag{5} \\] Let’s explore elements of the Moran’s I equation using the following sample dataset. Figure 13.20: Simulated spatial layer. The figure on the left shows each cell’s ID value. The figure in the middle shows the values for each cell. The figure on the right shows the standardized values using equation (2). The first step in the computation of a Moran’s I index is the generation of a spatial weights matrix. The weights can take on many different values. For example, one could assign a value of 1 to a neighboring cell as shown in the following matrix. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 4 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 5 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 6 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 7 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 8 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 9 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 10 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 11 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 12 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 13 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 14 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 15 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 16 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 For example, cell ID 1 (whose value is 25 and whose standardized value, \\(z_1\\), is 0.21) has for neighbors cells 2, 5 and 6. Computationally (working with the standardized values), this gives us a summarized neighboring value (aka lagged value), \\(y_1(lag)\\) of: \\[ \\begin{align*} y_1 = \\sum\\limits_j w_{1j} z_j {}={} &amp; (0)(0.21)+(1)(1.17)+(0)(1.5)+ ... + \\\\ &amp; (1)(0.69)+(1)(0.93)+(0)(-0.36)+...+ \\\\ &amp; (0)(-0.76) = 2.79 \\end{align*} \\] Computing the spatially lagged values for the other 15 cells generates the following scatterplot: Figure 13.21: Moran’s I scatterplot using a binary weight. The red point is the (\\(z_1\\), \\(y_1\\)) pair computed for cell 1. You’ll note that the range of neighboring values along the \\(y\\)-axis is much greater than that of the original values on the \\(x\\)-axis. This is not necessarily an issue given that the Moran’s \\(I\\) correlation coefficient standardizes the values by recentering them on the overall mean \\((X - \\bar{X})/s\\). This is simply to re-emphasize that we are interested in how a neighboring value varies relative to a feature’s value, regardless of the scale of values in either batches. If there is a downside to adopting a binary weight, it’s the bias that the different number of neighbors can introduce in the calculation of the spatially lagged values. In other words, a feature with 5 polygons (such as feature ID 12) will have a larger neighboring value than a feature with 3 neighbors (such as feature ID 1) whose neighboring value will be less if there was no spatial autocorrelation in the dataset. A more natural weight is one where the values are standardized across each row of the weights matrix such that the weights across each row sum to one. For example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 0 0.333 0 0 0.333 0.333 0 0 0 0 0 0 0 0 0 0 2 0.2 0 0.2 0 0.2 0.2 0.2 0 0 0 0 0 0 0 0 0 3 0 0.2 0 0.2 0 0.2 0.2 0.2 0 0 0 0 0 0 0 0 4 0 0 0.333 0 0 0 0.333 0.333 0 0 0 0 0 0 0 0 5 0.2 0.2 0 0 0 0.2 0 0 0.2 0.2 0 0 0 0 0 0 6 0.125 0.125 0.125 0 0.125 0 0.125 0 0.125 0.125 0.125 0 0 0 0 0 7 0 0.125 0.125 0.125 0 0.125 0 0.125 0 0.125 0.125 0.125 0 0 0 0 8 0 0 0.2 0.2 0 0 0.2 0 0 0 0.2 0.2 0 0 0 0 9 0 0 0 0 0.2 0.2 0 0 0 0.2 0 0 0.2 0.2 0 0 10 0 0 0 0 0.125 0.125 0.125 0 0.125 0 0.125 0 0.125 0.125 0.125 0 11 0 0 0 0 0 0.125 0.125 0.125 0 0.125 0 0.125 0 0.125 0.125 0.125 12 0 0 0 0 0 0 0.2 0.2 0 0 0.2 0 0 0 0.2 0.2 13 0 0 0 0 0 0 0 0 0.333 0.333 0 0 0 0.333 0 0 14 0 0 0 0 0 0 0 0 0.2 0.2 0.2 0 0.2 0 0.2 0 15 0 0 0 0 0 0 0 0 0 0.2 0.2 0.2 0 0.2 0 0.2 16 0 0 0 0 0 0 0 0 0 0 0.333 0.333 0 0 0.333 0 The spatially lagged value for cell ID 1 is thus computed as: \\[ \\begin{align*} y_1 = \\sum\\limits_j w_{1j} z_j {}={} &amp; (0)(0.21)+(0.333)(1.17)+(0)(1.5)+...+ \\\\ &amp; (0.333)(0.69)+(0.333)(0.93)+(0)(-0.36)+...+ \\\\ &amp; (0)(-0.76) = 0.93 \\end{align*} \\] Multiplying each neighbor by the standardized weight, then summing these values, is simply computing the neighbor’s mean value. Using the standardized weights generates the following scatter plot. Plot on the left shows the raw values on the x and y axes; plot on the right shows the standardized values \\(z_i\\) and \\(y_i = \\sum\\limits_j w_{ij} z_j\\). You’ll note that the shape of the point cloud is the same in both plots given that the axes on the left plot are scaled such as to match the standardized scales in both axes. Figure 13.22: Moran’s scatter plot with original values on the left and same Moran’s I scatter plot on the right using the standardized values \\(z_i\\) and \\(y_i\\). Note the difference in the point cloud pattern in the above plot from the one generated using the binary weights. Other weights can be used such as inverse distance and k-nearest neighbors to name just a few. However, must software implementations of the Moran’s I statistic will adopt the row standardized weights. 13.5.1 Local Moran’s I Once a spatial weight is chosen, and both \\(z_i\\) and \\(y_i\\) are computed. We can compute the \\(z_iy_i\\) product for all locations of \\(i\\) thus giving us a measure of the local Moran’s I statistic. Taking feature ID of 1 in our example, we compute \\(I_1(lag) = 0.21 \\times 0.93 = 0.19\\). Computing \\(I_i\\) for all cells gives us the following plot. Figure 13.23: The left plot shows the Moran’s I scatter plot with the point colors symbolizing the \\(I_i\\) values. The figure on the right shows the matching \\(I_i\\) values mapped to each respective cell. Here, we are adopting a different color scheme from that used earlier. Green colors highlight features whose values are surrounded by similar values. These can be either positive values surrounded by standardized values that tend to be positive or negative values surrounded by values that tend to be negative. In both cases, the calculated \\(I_i\\) will be positive. Red colors highlight features whose values are surrounded by dissimilar values. These can be either negative values surrounded by values that tend to be positive or positive values surrounded by values that tend to be negative. In both cases, the calculated \\(I_i\\) will be negative. In our example, two features have a negative Moran’s I coefficient: cell IDs 7 and 12. 13.5.2 Global Moran’s I The Global Moran’s I coefficient, \\(I\\) is nothing more than a summary of the local Moran’s I coefficients. Using a standardized weight, \\(I\\) is the average of all \\(I_i\\) values. \\[ \\begin{pmatrix} \\frac{0.19+0.7+1.15+0.68+0.18+0.15+-0.24+0.44+0.25+0.12+0.14+-0.29+1.18+1.39+0.71+0.39}{\\sum\\limits_i\\sum\\limits_j w_{ij}} = 0.446 \\end{pmatrix} \\] In this example, \\(\\sum\\limits_i \\sum\\limits_j w_{ij}\\) is the sum of all 256 values in Table (2) which, using standardized weights, sums to 16. \\(I\\) is thus the slope that best fits the data. This can be plotted using either the standardized values or the raw values. Figure 13.24: Moran’s scatter with fitted Moran’s I slope (red line). The left plot uses the raw values \\((X_i,X_i(lag))\\) for its axes labels. Right plot uses the standardized values \\((z_i,y_i)\\) for its axes labels. 13.6 Summary This chapter introduced spatial autocorrelation as a key concept for understanding second-order spatial structure–how values at one location relate to values at nearby locations. Using Moran’s I as a central tool, we explored both global and local measures of spatial association, emphasizing the importance of spatial weights and neighborhood definitions. Through examples and permutation-based inference, we demonstrated how spatial autocorrelation can be quantified, tested, and interpreted across scales. The chapter concluded with a detailed breakdown of the Moran’s I equation, reinforcing its interpretation as a spatially weighted correlation and laying the groundwork for more advanced spatial statistical techniques. References Tobler, Waldo R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46 (2): 234–40. http://www.geog.ucsb.edu/~tobler/publications/pdf_docs/A-Computer-Movie.pdf. "],["chp14_0.html", "Chapter 14 First-Order Point Pattern Analysis: Modeling Spatial Intensity 14.1 Introduction 14.2 Centrography 14.3 Understanding Intensity and Density in Point Patterns 14.4 Global density 14.5 Local density 14.6 Hypothesis Testing for Covariate Effects 14.7 Summary", " Chapter 14 First-Order Point Pattern Analysis: Modeling Spatial Intensity 14.1 Introduction In spatial point pattern analysis, first-order effects refer to variation in the intensity of points across a study area due to external factors. These effects describe how the likelihood of observing a point changes from one location to another, independent of the presence or absence of other points. For example, oak trees may be more densely distributed in areas with favorable soil conditions, or retail stores may cluster in regions with higher population density. In both cases, the spatial variation is driven by underlying environmental or socioeconomic covariates rather than interactions between the points themselves. This chapter focuses on techniques for detecting and modeling these first-order effects. We begin with simple descriptive tools such as global and local density measures, then move to more refined methods like kernel density estimation and covariate-adjusted intensity modeling. Finally, we introduce statistical models–such as the Poisson point process–that allow us to formally test hypotheses about how external variables influence point intensity. By isolating first-order effects, we lay the groundwork for more advanced analyses of second-order properties, which examine how points interact with one another. 14.2 Centrography Before diving into formal models of spatial intensity, it’s helpful to begin with simple descriptive tools that summarize the overall distribution of points. This approach, known as centrography, includes techniques such as the mean center, standard distance and standard deviational ellipse. These measures provide insight into the central tendency and dispersion of a point pattern, offering a useful starting point for exploring spatial variation. These point pattern analysis techniques were popular before computers were ubiquitous as hand calculations were relatively simple. However, these summary statistics are too concise and hide far more valuable information about the observed pattern. In this section, we introduce density-based methods as a bridge between raw spatial data and more formal modeling of intensity. 14.3 Understanding Intensity and Density in Point Patterns In point pattern analysis, the terms intensity and density are closely related but conceptually distinct. Understanding the difference is important when interpreting spatial patterns and building models. Density refers to the observed number of points per unit area in a dataset. It’s a descriptive statistic–what we actually count from the data. For example, if there are 31 trees in a 100 square meter plot, the density is 0.31 trees per square meter. Intensity, on the other hand, refers to the underlying rate or likelihood that a point occurs at a location. It’s a property of the process that generated the pattern, not just the pattern itself. Intensity is often denoted by the symbol \\(\\lambda\\) and can vary across space. When we estimate intensity from observed data, we use \\(\\widehat{\\lambda}\\) to indicate that it’s an approximation. Think of it this way: density is what we observe, intensity is what we infer. If the intensity is constant across the study area, we say the process is homogeneous. If it varies, the process is inhomogeneous, and we need tools that can model how intensity changes with location or covariates. 14.4 Global density A basic estimate of a point pattern’s intensity, \\(\\widehat{\\lambda}\\), is its global density–the average number of points per unit area across the entire study region. It is computed as the ratio of the total number of observed points, \\(n\\), to the surface area of the study region, \\(a\\): \\(\\begin{equation} \\widehat{\\lambda} = \\frac{n}{a} \\label{eq:global-density} \\end{equation}\\) This estimate assumes that the underlying process is homogeneous, meaning the intensity is constant throughout the region. Figure 14.1: An example of a point pattern where n = 31 and the study area (defined by a square boundary) is 10 units squared. The point density is thus 31/100 = 0.31 points per unit area. While useful as a summary statistic, global density can mask important spatial variation, which motivates the need for local density measures introduced in the next section. 14.5 Local density A point pattern’s density can vary across space, and measuring it at different locations allows us to assess whether the underlying process’s local intensity, \\(\\widehat{\\lambda}_i\\), is spatially uniform or not. This spatial variation is a key aspect of first-order effects, where external factors may influence the likelihood of observing a point in different parts of the study area. Identifying non-uniform intensity is important because it can confound distance-based analyses (covered in the next chapter) which assume stationarity–the assumption that the underlying process generating the points has a constant intensity across the study area. Several techniques are available for estimating local density. In this chapter, we focus on two widely used methods: quadrat density, which divides the study area into discrete subregions, and kernel density, which uses a moving window to produce a smooth surface of estimated intensity. 14.5.1 Quadrat density Quadrat density analysis involves dividing the study area into sub-regions called quadrats. For each quadrat, the local density is computed by dividing the number of points within the quadrat by its area. This provides a spatially localized estimate of point intensity. This method assumes that the intensity of the point process is roughly constant within each quadrat, which may not hold if quadrats are too large or poorly aligned with underlying spatial variation. Quadrats can take various shapes such as hexagons and triangles. In the following example density is computed for square shaped quadrats. Figure 14.2: An example of a quadrat count where the study area is divided into four equally sized quadrats whose area is 25 square units each. The density in each quadrat can be computed by dividing the number of points in each quadrat by that quadrat’s area. The choice of quadrat numbers and quadrat shape can influence the measure of local density and must be chosen with care. If very small quadrat sizes are used, you risk having many empty quadrats which may prove uninformative. If very large quadrat sizes are used, you risk missing subtle changes in spatial density distributions such as an east-west gradient in density values. Instead of dividing the study area into uniform shapes, quadrats can also be defined based on an underlying covariate–—a spatial variable believed to influence point intensity (e.g., elevation, land cover, or population density). For example, if it’s believed that the underlying point pattern process is driven by elevation, quadrats can be defined by sub-regions such as different ranges of elevation values (labeled 1 through 4 on the right-hand plot in the following example). This can result in quadrats with non-uniform shape and area. Converting a continuous field into discretized areas is sometimes referred to as tessellation. The end product is a tessellated surface. Figure 14.3: Example of a covariate. Figure on the left shows the elevation map. Figure on the right shows elevation broken down into four sub-regions (a tessellated surface) for which local density values will be computed. If the local intensity changes across the tessellated covariate, then there is evidence of a dependence between the process that generated the point pattern and the covariate. In our example, sub-regions 1 through 4 have surface areas of 23.54, 25.2, 25.21, 26.06 map units respectively. To compute these regions’ point densities, we simply divide the number of points by the respective area values. Figure 14.4: Figure on the left displays the number of points in each elevation sub-region (sub-regions are coded as values ranging from 1 to 4). Figure on the right shows the density of points (number of points divided by the area of the sub-region). We can plot the relationship between point density and elevation regions to help assess any dependence between the variables. Figure 14.5: Plot of point density vs elevation regions. It’s important to note that how one chooses to tessellate a surface can have an influence on the resulting density distribution. For example, dividing the elevation into seven sub-regions produces the following density values. Figure 14.6: Same analysis as last figure using different sub-regions. Note the difference in density distribution. While the high density in the western part of the study area remains, the density values to the east are no longer consistent across the other three regions. The quadrat analysis approach has advantages–it is easy to compute and interpret. However, it does suffer from the modifiable areal unit problem (MAUP) as highlighted in the last two examples. Quadrat density is most useful when the study area can be meaningfully divided into regions of interest, or when a covariate provides a natural way to segment space. However, it may be less effective in detecting subtle or continuous spatial variation, which motivates the use of kernel density methods introduced next. 14.5.2 Kernel density The kernel density approach is a method for estimating the spatial intensity of a point pattern. Like quadrat density, it computes localized density values, but it does so using a moving window that overlaps across space, producing a smoother and more continuous estimate of intensity. This moving window is defined by a kernel. A kernel is a mathematical function that defines how much influence each point has on the estimated intensity at a given location. The kernel density method produces a grid of intensity estimates where each cell’s value reflects the weighted contribution of nearby points within the kernel window centered on that cell. The simplest kernel function is a basic kernel where each point in the kernel window is assigned equal weight. Figure 14.7: An example of a basic 3x3 kernel density map where each point is assigned an equal weight. For example, the cell centered at location x=1.5 and y =7.5 has one point within a 3x3 unit (pixel) region and thus has a local density of 1/9 = 0.11. More common kernel functions assign weights to points that are inversely proportional to their distances to the kernel window’s center. A few such kernel functions follow a gaussian or quartic like distribution function. These functions tend to produce a smoother density map. Figure 14.8: An example of a kernel function is the 3x3 quartic kernel function where each point in the kernel window is weighted based on its proximity to the kernel’s center cell (typically, closer points are weighted more heavily). Kernel functions, like the quartic, tend to generate smoother surfaces. 14.5.3 Non-parametric Intensity Modeling with Covariates In the previous section, we learned that we could use a covariate, like elevation, to define the sub-regions (quadrats) within which densities were computed. Here, instead of dividing the study region into discrete sub-regions, we estimate an intensity function, denoted \\(\\rho(z)\\), that describes how point intensity varies as a function of a covariate \\(z\\) (e.g., elevation). This function is then used to model the spatial intensity \\(\\widehat{\\lambda}(x)\\) at each location \\(x\\) in the study area. In this framework, \\(\\rho(z)\\) captures how intensity varies with the covariate, while \\(\\widehat{\\lambda}(x)\\) applies this relationship spatially across the study region using the covariate’s spatial distribution. \\(\\rho(z)\\) can be estimated in one of three different ways–by ratio, re-weight and transform methods. We will not delve into the differences between these methods, but note that there is more than one way to estimate \\(\\rho(z)\\) in the presence of a covariate. In the following example, the elevation raster is used as the covariate in the \\(\\rho(z)\\) function using the ratio method (The ratio method compares the observed density of points at different covariate values to the density of the covariate itself, producing an estimate of relative intensity). The right-most plot maps the modeled intensity as a function of elevation. Figure 14.9: An estimate of \\(\\rho\\) using the ratio method. The figure on the left shows the point distribution superimposed on the elevation layer. The middle figure plots the estimated \\(\\rho\\) as a function of elevation. The envelope shows the 95% confidence interval. The figure on the right shows the modeled density of \\(\\widehat{\\lambda}\\) which is a function of the elevation raster (i.e. \\(\\widehat{\\lambda}=\\widehat{\\rho}_{elevation}\\)). We can compare the modeled intensity function to the kernel density function of the observed point pattern via a scatter plot. A red one-to-one diagonal line is added to the plot. The scatter plot compares the predicted intensity (based on elevation) to the observed kernel density. While the general trend is positive, the relationship is not strictly linear. This deviation may reflect uncertainty in regions with sparse data, such as high elevations, where fewer points lead to less reliable estimates. This uncertainty is very apparent in the \\(\\rho_{elev}\\) vs. elevation plot where the 95% confidence interval envelope widens at higher elevation values (indicating the greater uncertainty in our estimated \\(\\rho\\) value at those higher elevation values). The widening of the confidence envelope at higher elevations reflects increased uncertainty due to fewer observations in those regions–a common issue in spatial modeling when covariate values are unevenly distributed. Modeling intensity as a function of a covariate allows us to explore first-order effects more rigorously. It helps determine whether external factors–like elevation or population density–can explain the spatial distribution of points. This approach provides a non-parametric estimate of how point intensity varies with a covariate, offering insight into first-order effects. The function \\(\\rho(z)\\) captures this relationship without assuming a specific parametric form. In the next section, we explore a complementary approach–Poisson point process modeling–which allows us to formally specify intensity as a log-linear function of covariates. 14.5.4 Parametric Intensity Modeling with Covariates Up to this point, we have focused on techniques that describe the distribution of points across a region of interest. However, it is often more insightful to model the relationship between point locations and an underlying covariate using a formal statistical framework. This can be done by exploring the changes in point density as a function of a covariate, however, unlike techniques explored thus far, this approach makes use of a statistical model. One such model is a Poisson point process model (PPM) which can take the form: \\[ \\begin{equation} \\lambda(i) = e^{\\alpha + \\beta Z(i)} \\label{eq:density-covariate} \\end{equation} \\] This equation defines how the expected number of points per unit area (intensity) varies as a function of the covariate \\(Z(i)\\). The term \\(\\lambda(i)\\) is the modeled intensity at location \\(i\\), \\(e^{\\alpha}\\) (the exponent of \\(\\alpha\\)) is the base intensity when the covariate is zero and \\(e^{\\beta}\\) is the multiplier by which the intensity increases (or decreases) for each 1 unit increase in the covariate \\(Z(i)\\). The parameters \\(\\alpha\\) and \\(\\beta\\) are estimated from the data using maximum likelihood, which finds the values that best explain the observed point pattern given the covariate surface. Note that the units of intensity are derived from the coordinate reference system (e.g., stores per square meter), and the interpretation of \\(\\beta\\) depends on the scale and units of the covariate. The equation is a form of a log-linear model, used in spatial statistics to model intensity. While it resembles logistic regression in structure, the left-hand side represents intensity, not probability. Note that taking the log of both sides of the equation yields the more familiar linear regression model where \\(\\alpha + \\beta Z(i)\\) is the linear predictor. Note: While the Poisson point process model uses a log-linear form to model intensity, logistic regression models the probability of occurrence. These two frameworks are related through the transformation \\(\\lambda = P / (1 - P)\\), which leads to: \\[ P(i) = \\frac{e^{\\alpha + \\beta Z(i)}}{1 + e^{\\alpha + \\beta Z(i)}} \\] This relationship is useful when interpreting intensity in terms of probability, especially in presence/absence modeling. This PPM assumes that events occur independently, and that the intensity varies smoothly with the covariate. It does not account for interactions between points (i.e., second-order effects). Let’s work with the point distribution of Starbucks cafes in the state of Massachusetts. The point pattern clearly exhibits a non-random distribution. It might be helpful to compare this distribution to some underlying covariate such as the population density distribution. Figure 14.10: Location of Starbucks relative to population density. Note that the classification scheme follows a log scale to more easily differentiate population density values. We can fit a poisson point process model to these data where the modeled intensity takes on the form: \\[ \\begin{equation} Starbucks\\ density(i) = e^{\\alpha + \\beta\\ population(i)} \\label{eq:walmart-model} \\end{equation} \\] The parameters \\(\\alpha\\) and \\(\\beta\\) are estimated from a method called maximum likelihood. Its implementation is not covered here but is widely covered in general statistics text books. The index \\((i)\\) serves as a reminder that the point density and the population distribution both can vary as a function of location \\(i\\). The estimated value for \\(\\alpha\\) in our example is -18.966. This is interpreted as stating that given a population density of zero, the base intensity of the point process is e-18.966 or 5.79657e-09 cafes per square meter (the units are derived from the point’s reference system)–a number close to zero (as one would expect). The estimated value for \\(\\beta\\) is 0.00017. This is interpreted as stating that for every unit increase in the population density derived from the raster, the intensity of the point process increases by a factor of e0.00017 or 1.00017. The plot below shows the fitted intensity curve as a function of population density, along with a confidence envelope. This visualization helps assess whether the relationship is approximately linear and whether uncertainty increases at extreme covariate values. Figure 14.11: Poisson point process model fitted to the relationship between Starbucks store locations and population density. The model assumes a log-linear relationship. Note that the density is reported in number of stores per map unit area (the map units are in meters). In the next section, we explore how to formally compare this covariate-based model to a simpler null model (e.g., homogeneous intensity) using statistical tests. This allows us to assess whether the covariate meaningfully improves our understanding of the spatial distribution of points. 14.6 Hypothesis Testing for Covariate Effects A Poisson point process model can be fit to any observed point pattern, but fitting a model alone does not guarantee that it provides a meaningful explanation of the spatial distribution. To evaluate whether a covariate-based model improves our understanding of the point pattern, we compare it to a simpler baseline model–typically one that assumes homogeneous intensity across space. In this framework, the homogeneous model serves as the null hypothesis, while the covariate-based model represents the alternative hypothesis. For example, we may want to assess whether the spatial distribution of Walmart stores is better explained by population density (the alternative hypothesis) than by a model that assumes no spatial preference (the null hypothesis). To test this, we fit both models to the observed data and compare their performance using a likelihood ratio test. A Poisson point process model (of the the Walmart point pattern) implemented using spatstat’s ppm function (Baddeley, Rubak, and Turner 2016) produces the following output for the null model: Stationary Poisson process Fitted to point pattern dataset &#39;P&#39; Intensity: 2.1276e-09 Estimate S.E. CI95.lo CI95.hi Ztest Zval log(lambda) -19.96827 0.1507557 -20.26375 -19.6728 *** -132.4545 and the following output for the alternative model. Nonstationary Poisson process Fitted to point pattern dataset &#39;P&#39; Log intensity: ~pop Fitted trend coefficients: (Intercept) pop -2.007063e+01 1.043115e-04 Estimate S.E. CI95.lo CI95.hi Ztest (Intercept) -2.007063e+01 1.611991e-01 -2.038657e+01 -1.975468e+01 *** pop 1.043115e-04 3.851572e-05 2.882207e-05 1.798009e-04 ** Zval (Intercept) -124.508332 pop 2.708284 Problem: Values of the covariate &#39;pop&#39; were NA or undefined at 0.7% (4 out of 572) of the quadrature points Thus, the null model (homogeneous intensity) takes on the form: \\[ \\lambda(i) = e^{-19.96} \\] and the alternate model takes on the form: \\[ \\lambda(i) = e^{-20.1 + 1.04^{-4}population} \\] To better understand how the two models differ, we can visualize the estimated intensity (\\(\\widehat{\\lambda}\\)) as a function of population density for both the null and alternative models. (#fig:f11b_Ho_vs_H)Estimated intensity functions from the Poisson point process model: the left plot shows the covariate-based model (intensity as a function of population density), while the right plot shows the null model (constant intensity). Note the broader confidence interval in the covariate-based model, indicating greater uncertainty at extreme population values (the map units are in meters). The shapes of the fitted model clearly differ between the models, but this does not necessarily imply that the model that includes population density is significantly different from the null model. The relatively wide confidence intervals in the covariate-based model, especially at higher population densities, suggest uncertainty in the estimated relationship. This highlights the importance of formal statistical testing to determine whether the observed differences are meaningful. To formally assess whether the covariate-based model provides a significantly better fit than the null model, we use a likelihood ratio test. This test compares the log-likelihoods of the two models and evaluates whether the improvement in fit is statistically significant. Npar Df Deviance Pr(&gt;Chi) 5 NA NA NA 6 1 4.253072 0.0391794 The value under the heading PR(&gt;Chi) is the p-value which gives us the probability we would be wrong in rejecting the null. The resulting p-value of 0.039 indicates that, if the null model were true (i.e., if Walmart locations were randomly distributed with constant intensity), there is only a 3.9% chance of observing a pattern that fits the covariate-based model this well. This provides some evidence that population density improves the model’s ability to explain the spatial distribution of Walmart stores. In summary, the likelihood ratio test suggests that population density is a meaningful covariate in explaining the spatial distribution of Walmart stores, though some uncertainty remains in the estimated relationship. 14.7 Summary This chapter introduced methods for analyzing first-order properties of spatial point patterns—those driven by external factors rather than interactions between points. We began with descriptive tools such as global and local density measures, including quadrat and kernel density estimation. These methods help identify spatial variation in point intensity and can reveal associations with underlying covariates. We then explored how covariates can be used to model intensity more formally, both non-parametrically and through parametric approaches like the Poisson point process model (PPM). The PPM allows us to test hypotheses about whether a covariate meaningfully explains the spatial distribution of points, using statistical tools such as the likelihood ratio test. Importantly, this chapter is limited to first-order effects. While second-order properties—such as point interactions—are central to spatial analysis, they are addressed in the next chapter. References Baddeley, Adrian, Ege Rubak, and Rolf Turner. 2016. Spatial Point Patterns, Methodology and Applications with r. Florida: CRC Press. "],["chp15_0.html", "Chapter 15 Second-Order Point Pattern Analysis: Modeling Distance 15.1 Introduction 15.2 Average Nearest Neighbor 15.3 K and L functions 15.4 The Pair Correlation Function 15.5 Summary", " Chapter 15 Second-Order Point Pattern Analysis: Modeling Distance 15.1 Introduction An alternative to the density-based methods explored thus far are the distance-based methods for analyzing spatial point patterns. These approaches shift the focus from how points are distributed relative to the study area (a first-order property) to how points are distributed relative to one another–a hallmark of second-order analysis. This shift enables us to explore whether events exhibit spatial interaction, such as clustering or dispersion, across various spatial scales. Distance-based methods are particularly useful when the goal is to assess whether the observed pattern deviates from what would be expected under complete spatial randomness (CSR), a benchmark model in spatial statistics. Rather than estimating intensity across space, these methods examine the spatial arrangement of points directly through inter-point distances. In the sections that follow, we introduce three widely used distance-based techniques: Average Nearest Neighbor (ANN) Ripley’s K and L functions The Pair Correlation Function (PCF) Each of these methods provides a different lens through which to assess the spatial structure of a point pattern, and each is sensitive to different aspects of spatial interaction. 15.2 Average Nearest Neighbor Average Nearest Neighbor (ANN) analysis measures the average distance from each point in a study area to its nearest neighboring point. This provides a simple yet powerful summary of the spatial arrangement of points, helping to distinguish between clustered, random, or regularly spaced patterns. In the example below, the average nearest neighbor distance for all points is 13,295 meters. Reading layer Walmarts' from data sourceC:.shp’ using driver `ESRI Shapefile’ Simple feature collection with 44 features and 40 fields Geometry type: POINT Dimension: XY Bounding box: xmin: 640627.2 ymin: 4614803 xmax: 869353.2 ymax: 4738331 Projected CRS: NAD83 / UTM zone 18N Figure 15.1: A subset of distances between each point and its closest point. For example, fig.height=3.0, fig.width=5, message=FALSE, warning=FALSE, the point closest to point 3 is point 4 which is 16,616 meters away. 15.2.1 Extending the Concept: Higher-Order Neighbors While the basic ANN statistic captures only the nearest neighbor relationship, extending the concept to include second, third, and higher order neighbors reveals more nuanced spatial structures. This produces a curve that reflects how inter-point distances change with neighbor order. Figure 15.2: ANN values for different neighbor order numbers. For example, the ANN for the first closest neighbor for all points is 13,295 meters; the ANN for the 2nd closest neighbor for all points is 18850 meters, and the ANN for the 43rd closest neighbor (the furthest neighbor) for all points is 182,818 meters,and so forth. The shape of the ANN curve provides insight into the spatial structure of the pattern. A steep initial slope followed by a gradual increase may indicate tight clustering, while a more linear increase suggests regular spacing. To illustrate how ANN curves vary with different spatial arrangements, consider three point patterns of 20 points each: A single cluster A dual cluster A random scatter Figure 15.3: Three different point patterns: a single cluster, a dual cluster and a randomly scattered pattern. The following figure illustrates how the ANN curve varies across three spatial arrangements, highlighting the sensitivity of ANN to clustering structure. Figure 15.4: Three different ANN vs. neighbor order plots. The black ANN line is for the first point pattern (single cluster); the blue line is for the second point pattern (double cluster) and the red line is for the third point pattern. The black line (single cluster) shows short distances across all neighbor orders, indicating tight clustering. The blue line (dual cluster) shows intermediate distances, reflecting the presence of two separate clusters. The red line (random scatter) shows longer distances overall, consistent with a more dispersed arrangement. The perceived spatial structure of a point pattern is highly sensitive to the definition of the study area. In the example below, the same clustered pattern is shown within two different bounding boxes: Figure 15.5: The same point pattern presented with two different study areas. How differently would you describe the point pattern in both cases? When the study area is tightly fitted around the cluster, the pattern appears more dispersed. This highlights the modifiable areal unit problem (MAUP), a key concern in spatial analysis. ANN analysis assumes that the underlying point process is stationary—that is, the intensity of the process does not vary across space. If the process is non-stationary, observed clustering may reflect underlying environmental gradients or covariates rather than true inter-point interaction. Correcting for non-stationarity when performing hypothesis tests is discussed in the next chapter. 15.2.2 Testing for Complete Spatial Randomness 15.2.2.1 Analytical approach The analytical approach to testing for Complete Spatial Randomness (CSR) using the Average Nearest Neighbor (ANN) statistic involves comparing the observed mean nearest neighbor distance to the expected mean distance under a CSR model. The null hypothesis assumes that the observed point pattern is a realization of a spatially uniform and independent process. Under CSR, the expected mean nearest neighbor distance is: \\[ ANN_{expected}=\\dfrac{1}{2\\sqrt{n/A}} \\] where: \\(n\\) is the number of points, \\(A\\) is the area of the study region The test statistic is the ratio: \\[ R = \\frac{ANN_{observed}}{ANN_{expected]}} \\] Interpretation: \\(R &lt; 1\\): suggests clustering \\(R &gt; 1\\): suggests dispersion \\(R \\approx 1\\) suggests consistent with a completely random process To assess statistical significance, a z-score is computed: \\[ z = \\frac{ANN_observed - ANN_expected}{SE} \\] where the standard error (SE) of the expected distance under CSR is: \\[ SE = \\frac{0.26136}{\\sqrt{n\\lambda}} \\] This z-score is then compared to a standard normal distribution to determine whether the observed pattern significantly deviates from CSR. While analytically elegant, the ANN hypothesis test has several limitations that constrain its interpretive power: Sensitivity to Study Area Definition: The expected distance depends on the area \\(A\\) which must be defined explicitly. Shape Effects: The formula assumes a regular-shaped study region. Irregular boundaries distort the expected distance and introduce edge effects, where points near the boundary have artificially inflated nearest neighbor distances. Stationarity Assumption: The test assumes a stationary process–i.e., constant intensity across space. If the underlying process is inhomogeneous, with point density varying due to environmental or socioeconomic factors, clustering may reflect first-order effects rather than true inter-point interaction. 15.2.2.2 A better approach: a Monte Carlo test While the analytical approach to ANN hypothesis testing offers a convenient benchmark for assessing spatial randomness, its reliance on idealized assumptions–such as uniform study area shape–can limit its applicability in real-world contexts. When such assumptions are violated, the analytical test may yield misleading results. To address these limitations, spatial analysts often turn to simulation-based methods, such as Monte Carlo techniques, which offer a more flexible and empirically grounded framework for hypothesis testing. These methods allow us to generate synthetic point patterns under a specified null model and compare our observed statistic to the distribution of simulated outcomes, providing a more robust basis for inference. The Monte Carlo technique involves three steps: First, we postulate a process–our null hypothesis, \\(Ho\\). For example, we hypothesize that the distribution of Walmart stores is consistent with a completely random process (CSR). Next, we simulate many realizations of our postulated process and compute a statistic (e.g. ANN) for each realization. Finally, we compare our observed data to the patterns generated by our simulated processes and assess (via a measure of probability) if our pattern is a likely realization of the hypothesized process. Using the Walmart store distribution example, we randomly re-position the location of the Walmart points 1000 times (or as many times computationally practical) following a completely random process–our hypothesized process, \\(Ho\\)–while making sure to keep the points confined to the study extent (the state of Massachusetts). Figure 15.6: Three different outcomes from simulated patterns following a CSR point process. These maps help answer the question how would Walmart stores be distributed if their locations were not influenced by the location of other stores and by any local factors (such as population density, population income, road locations, etc…) For each realization of our process, we compute an ANN value. Each simulated pattern results in a different ANN value under CSR. These values form an empirical distribution representing the null hypothesis. We plot the simulated ANN value with a histogram (this is our \\(Ho\\) sample distribution), then compare our observed ANN value of 13,294 m to this distribution. Figure 15.7: Histogram of simulated ANN values (from 1000 simulations). This is the sample distribution of the null hypothesis, ANNsimulated (under CSR). The red line shows our observed (Walmart) ANN value. About 32% of the simulated values are greater (more extreme) than our observed ANN value. By using the same study region in each simulation, Monte Carlo methods automatically account for edge effects and irregular study area shapes—issues that are difficult to correct analytically. 15.2.2.3 Extracting a p-value from a Monte Carlo test The Monte Carlo p-value represents the proportion of simulated patterns that yield a test statistic more extreme than the observed one, under the null hypothesis. Specifically, we count how many simulated values are more extreme than the observed statistic–either greater or smaller, depending on the direction of the test. For a one-sided test, the empirical p-value is calculated as: \\[ p = \\dfrac{N_{extreme}+1}{N+1} \\] where Nextreme is the number of simulated values more extreme than our observed statistic, and \\(N\\) is the total number of simulations. The \\(+1\\) is added to ensure that the p-value can never be exactly 0. A practical and more generalized form of the equation looks like this: \\[ p = \\dfrac{min(N_{greater}+1 , N + 1 - N_{greater})}{N+1} \\] where \\(min(N_{greater}+1 , N + 1 - N_{greater})\\) is the smallest of the two values \\(N_{greater}+1\\) and \\(N + 1 - N_{greater}\\), and \\(N_{greater}\\) is the number of simulated values greater than the observed value. It’s best to implement this form of the equation in a scripting program to avoid the need to visually seek the side of the distribution closest to our observed statistic. For example, if we ran 1000 simulations in our ANN analysis and found that 319 of those were more extreme (on the right side of the simulated ANN distribution) than our observed ANN value, our p-value would be (319 + 1) / (1000 + 1) or p = 0.32. This can be interpreted as “there being a 32% chance of observing a test statistic as extreme as ours if the null hypothesis (CSR) were true.” This suggests that rejecting the null hypothesis that a CSR process could have generated our observed Walmart point distribution may be unwarranted. But this is not to say that the Walmart stores were in fact placed across the state of Massachusetts randomly (it’s doubtful that Walmart executives make such an important decision purely by chance), all we are saying is that a CSR process could have been one of many processes that generated the observed point pattern. If a two-sided test is desired, then the equation for the \\(p\\) value takes on the following form: \\[ 2 \\times \\dfrac{min(N_{greater}+1 , N + 1 - N_{greater})}{N+1} \\] where we are simply multiplying the one-sided p-value by two. Monte Carlo tests provide a robust alternative to analytical methods, especially when regular study area shape are violated. By simulating point patterns under realistic null models, we can better assess the plausibility of observed spatial structures. 15.2.3 Controlling for First-Order Effects in Second-Order Analysis The assumption of CSR provides a useful baseline for spatial analysis, but it’s often unrealistic in real-world settings. Many real-world spatial processes exhibit both first-order effects and second-order effects. When first-order effects are present, the underlying point process is non-stationary or inhomogeneous. In such cases, hypothesis tests must be adjusted to account for this spatial heterogeneity. Figure 15.8: Walmart store distribution overlayed on top of a population density layer. Could population density distribution explain the distribution of Walmart stores? For example, if there are reasons to believe that the placement of Walmart stores can be influenced by the distribution of the population density (a first-order effect) we must account for this spatial heterogeneity when testing for clustering or dispersion. This involves simulating point patterns that follow the same population density distribution and comparing the observed pattern to these simulations using Monte Carlo (MC) techniques. Figure 15.9: Examples of simulated point patterns generated under a non-stationary process, with population density serving as the spatial intensity function. Note that even though we are no longer referring to a CSR point process, we are still treating this as a random point process since the points are randomly placed following the underlying population density distribution. Using the Monte Carlo (MC) techniques used with an earlier section, we can simulate thousands of such point patterns (following the population density) and compare our observed ANN value to those computed from our MC simulations. Figure 15.10: Distribution of ANN values under a null model where population density governs point intensity. In this example, our observed ANN value falls far to the right of our simulated ANN values indicating that our points are more dispersed than would be expected had population density distribution been the sole driving process. The percentage of simulated values more extreme than our observed value is 0% (i.e. a p-value \\(\\backsimeq\\) 0.0). Another plausible hypothesis is that median household income influenced the placement of Walmart stores. Figure 15.11: Observed Walmart store locations overlaid on a map of median household income distribution. Running an MC simulation using median income distribution as the underlying intensity process yields an ANN distribution where about 16% of the simulated values are more extreme than our observed ANN value (i.e., one-sided p-value = 0.16): Figure 15.12: Histogram of ANN values from simulated point patterns generated under a median income-based intensity model. Note that we now have two competing hypotheses: a CSR/IRP process and median income distribution process. Both cannot be rejected. This serves as a reminder that a hypothesis test cannot tell us if a particular process is the process involved in the generation of our observed point pattern; instead, it tells us that the hypothesis is one of many plausible processes. It’s important to remember that the ANN tool is a distance based approach to point pattern analysis. Even though we are randomly generating points following some underlying probability distribution map we are still concerning ourselves with the repulsive/attractive forces that might dictate the placement of Walmarts relative to one another–i.e. we are not addressing the question “can some underlying process explain the X and Y placement of the stores” (addressed in section 14.6). Instead, we are controlling for the 1st order effect defined by population density and income distributions. 15.2.4 Controlling for Second-Order Effects in First-Order Analysis While it is common practice to control for first-order effects when testing for second-order spatial interaction, the reverse (testing for first-order effects while accounting for second-order interaction) is far less straightforward. Most hypothesis tests for first-order effects assume that points are independently distributed, an assumption that is violated when second-order effects (such as clustering or inhibition) are present. In such cases, model-based approaches offer a more appropriate framework. These methods jointly estimate both the intensity (first-order structure) and the interaction between points (second-order structure). One example involves models where “parent” points are first generated (often following a homogeneous or inhomogeneous Poisson process) and “child” points are then clustered around each parent. However, in practice, only the child locations are used in the Monte Carlo tests. These techniques go beyond the scope of this course. For now, it is important to recognize that second-order effects can confound first-order inference and that careful model specification is essential when both types of spatial structure are present. 15.3 K and L functions The ANN statistic is one of several distance-based measures used to describe spatial point patterns. While ANN focuses on the distance to the nth closest neighbor, the K and L functions offer a more comprehensive view by considering all inter-point distances up to a given threshold. This makes it particularly useful for detecting clustering or regularity across multiple spatial scales. In the sections that follow, we explore how the K and L functions are computed, interpreted, and used in hypothesis testing. 15.3.1 K function The K function summarizes the spatial dependence between points by measuring the expected number of points within a given distance of each point, normalized by the overall point density. The K function is a theoretical property of the underlying point process. However, as is the case with a theoretical property of a process, we rely on the observed pattern (one of many realizations of the underlying process) to estimate this function. Figure 15.13: Illustration of how the K function summarizes the number of points within a specified distance: Three concentric circles are drawn around point \\(i\\), and the number of neighboring points within each radius is tallied. In this example, no points fall within 10 km of \\(i\\), three points fall within 30 km, and seven points fall within 50 km. The K function estimate, \\(\\hat{\\lambda}\\), can be computed from an observed point pattern as follows: \\[ \\hat{K}(d) = \\frac{A}{n(n-1)} \\sum_{i=1}^{n} \\sum_{j \\neq i}^n I(d_{ij} \\leq d) \\] where: \\(i\\) refers to the reference point from which distanced \\(d\\) are measured. \\(j\\) refers to the other points in the pattern that are being evaluated to see whether they fall within a circle of radius \\(d\\) centered on \\(i\\). \\(d\\) is the search distance for which we are counting neighboring points. \\(d_{ij}\\) is the distance between point \\(i\\) and point \\(j\\). \\(I(d_{ij} \\le d)\\) is an indicator function that equals 1 if \\(d_{ij} \\le d\\) and 0 otherwise. \\(\\sum_{i=1}^{n}\\) is the outer summation It means “for each point \\(i\\) from the first point to the \\(n\\)^th point, do the following …” \\(\\sum_{j \\neq i}\\) is the inner summation. For the current point \\(i\\) selected by the outer summation, sum the results of the indicator function over all points \\(j\\) in the dataset, excluding the case where \\(j\\) is the same as \\(i\\). \\(n(n-1)\\) in the denominator is normalizing factor. It represents the total number of ordered pairs of distinct points in the dataset. Dividing the total count from the double summation by \\(n(n-1)\\) gives the observed fraction of pairs separated by a distance less than or equal to \\(d\\). \\(A\\) is the area of the study region. Multiplying the proportion by \\(A\\) scales the result into units of area, which makes comparison of the K function between datasest and theoretical K functions possible. The units of the K function is area whose units are usually defined by the point layer’s coordinate system. You might see K function sometimes expressed as, \\[ \\widehat{K}(d) = \\frac{1}{\\widehat\\lambda n} \\sum_{i=1}^{n} \\sum_{j \\neq i} I(d_{ij} \\leq d) \\] Note that the intensity estimator used here, \\(\\hat{\\lambda} = (n-1) / A\\), is slightly different from the more intuitive “natural” estimator, \\(\\hat{\\lambda} = n/A\\), that we’ve used previously. This is a deliberate statistical adjustment related to the goal of the K-function. The K-function describes the characteristics of a pattern from the perspective of the events themselves, looking at the distances between them. Think of it this way: when we stand at a specific event \\(i\\) and count its neighbors, we are looking at the density of the other \\(n-1\\) events in the study area \\(A\\). Figure 15.14: The K function estimated from the Walmart stores point distribution in MA for 512 distinct distances \\(d\\) ranging from 0 km to 50 km A few important considerations are to be taken: The K function, as presented here, assumes a homogeneous underlying first-order process given that \\(\\hat{\\lambda}\\) is assumed uniform for all locations with the study extent. The underlying process is assumed isotropic meaning that the relationship between points is a function of distance and not direction. Given that distance measurements are to be performed, coordinate systems used with the point pattern needs to preserve distance unless geodesic distances are to be computed. The K-function can be quite sensitive to edge effects. A correction is often applied to \\(\\hat{K}\\) to correct for this bias. A standard and widely used method applies an isotropic correction weight that is the proportion of the circumference of the circle centered on reference point \\(i\\) with radius \\(d\\) that lies within the study area (e.g. the state of Massachusetts in our working example). The corrected K function estimator takes the form: \\[ \\hat{K}(d) = \\frac{A}{n(n-1)} \\sum_{i=1}^{n} \\sum_{j \\neq i} \\frac{I(d_{ij} \\leq d)}{w_{ij}} \\] where \\(w_{ij}\\) is the isotropic correct weight that is the proportion of the circumference of the circle centered at \\(i\\) with radius \\(d\\) that lies within the study area \\(A\\). Note that the proportion is that of the circle’s perimeter length and not the circle’s area. Figure 15.15: The K-function estimated from the Walmart stores point distribution in MA (shown in black), and the isotropic edge corrected K function for the same point layer (shown in red) Under the assumption of Complete Spatial Randomness (CSR), the expected value of the K function is: \\[ K_{expected}(d) = \\pi d^2 \\] This provides a benchmark for comparison. If the observed K function exceeds this expected value, it suggests clustering of points at distance \\(d\\). If it falls below, it suggests or dispersion of points at distance \\(d\\). Figure 15.16: The K-function estimated from the Walmart stores point distribution in MA (shown in black), and the isotropic edge corrected K function for the same point layer (shown in red). The thick grey line represents the theoretical K function under the assumption of **Complete Spatial Randomness. Note how not correcting for edge effect can mis-characterize the nature of the point pattern. The uncorrected K function suggests a pattern more disperesed than expected under CSR assumption for all distance values while the isotropic weighted K function suggests clustering, 15.3.2 L function One limitation with the K function is that the shape of the function tends to curve upward (quadratically) making it difficult to see small differences between \\(K\\) and \\(K_{expected}\\). To address this, the L function is used as a variance-stabilizing transformation. A form of the L function follows: \\[ L=\\sqrt{\\dfrac{K(d)}{\\pi}}-d \\] Technically, the fundamental L function is written as \\(L=\\sqrt{\\dfrac{K(d)}{\\pi}}\\), however, the centered version of the function presented above has the benefit of rotating the plot such that the baseline under CSR is rendered horizontal. This provides more separation between the estimated K functions an and the theoretical K function across the range of distances. Under CSR, \\(L_{expected}(d) \\approx 0\\). Deviations from zero indicate departures from randomness: \\(L(d) &gt; 0\\): clustering at distance \\(d\\) \\(L(d) &lt; 0\\): dispersion at distance \\(d\\) Figure 15.17: L function rendering of the K function plot shown above. Both the edge corrected (red line) and standard L functions (black line) are shown. The theoretical L function under CSR is shown as a horizontal line centered on 0. The L function suggests that Walmart locations are more dispersed than expected under CSR up to a distance of 12 km but more clustered at distances greater than 12 km. 15.3.3 Monte Carlo test for the K and L functions MC techniques are not unique to average nearest neighbor analysis. In fact, they can be implemented with many other statistical measures including the K and L functions. However, unlike the ANN analysis, the K and L functions consist of multiple test statistics (one for each distance \\(d\\)). This results in not one but in as many number of simulated distributions as there are distances for which K and L are computed. Typically, these distributions are presented as envelopes superimposed on the estimated \\(K\\) or \\(L\\) functions. However, since we cannot easily display the full distribution at each \\(d\\) interval, we usually limit the envelope to a pre-defined acceptance interval. For example, if we choose a two-sided significance level of 0.05, then we eliminate the smallest and largest 2.5% of the simulated K values computed for for each \\(d\\) interval (hence the reason you might sometimes see such envelopes referred to as pointwise envelopes). This tends to generate a “saw-tooth” like envelope. Figure 15.18: Simulation results for the CSR hypothesized process. The yellow line shows the edge corrected L function. The gray envelope in the plot covers the 95% significance level. If the observed L lies outside of this envelope at distance \\(d\\), then there is less than a 5% chance that our observed point pattern resulted from the simulated process at that distance. The interpretation of these plots is straightforward: if \\(\\hat K\\) or \\(\\hat L\\) lies outside of the envelope at some distance \\(d\\), then this suggests that the point pattern may not be consistent with \\(H_o\\) (the hypothesized process) at distance \\(d\\) at the significance level defined for that envelope (0.05 in this example). One important assumption underlying the K and L functions is that the process is uniform across the region. If there is reason to believe this is not the case, then the K function analysis needs to be controlled for inhomogeneity in the process. For example, we might hypothesize that population density dictates the spatial distribution of the Walmart stores across the region. We therefore run an MC test by randomly re-assigning Walmart point locations using the population distribution map as the underlying point density distribution (in other words, we expect the MC simulation to locate a greater proportion of the points where population density is the greatest). Figure 15.19: The L-function plot and simulation results for an inhomogeneous hypothesized process. When controlled for population density, the significance test suggests that the inter-distance of Walmarts is more dispersed than expected under the null up to a distance of 30 km. It may be tempting to scan across a K or L function plot with pointwise envelopes, identify distances \\(d\\) where the observed function deviates from the null model, and report those intervals as statistically significant. However, this approach can be misleading. For example, based on the results in the previous figure, we might be inclined to conclude that the pattern is more dispersed than expected between distances of 5 and 30 kilometers at the 5% significance level. In reality, such a conclusion may only be justified at a higher significance level due to the increased chance of false positives when making multiple comparisons across distance bands. This issue, known as the multiple comparison problem, is a statistical concern that arises when testing many hypotheses simultaneously. To address this, global envelopes are constructed, which provide a single, valid significance test across the entire range of distances by summarizing the maximum deviation from the null model. 15.4 The Pair Correlation Function While Ripley’s K and L functions provide cumulative measures of spatial dependence, the Pair Correlation Function (PCF) offers a more localized, non-cumulative view of spatial structure. It is particularly useful for identifying the specific distances at which clustering or regularity occurs in a point pattern. The PCF, denoted \\(g(d)\\), describes the density of point pairs separated by a distance \\(d\\), relative to what would be expected under CSR. It is derived from the K function as: \\[ g(d) = \\frac{1}{2\\pi d}\\frac{dK(d)}{dd} \\] Here, \\(\\frac{dK(d)}{dd}\\) denotes the first derivative of the K-function with respect to the distance \\(d\\), representing its instantaneous rate of change. The notation \\(dd\\) in the denominator is a single, indivisible symbol indicating differentiation with respect to \\(d\\), and not a product. This formulation normalizes the rate of change in the cumulative K function by the circumference of a ring at distance \\(d\\), yielding a measure of point interaction at that specific scale. \\(g(d) = 1\\): The number of point pairs at distance \\(d\\) matches CSR expectations. \\(g(d) &gt; 1\\): More point pairs than expected at distance \\(d\\) suggesting clustering. \\(g(d) &lt; 1\\): Fewer point pairs than expected at distance \\(d\\) suggesting dispersion. Figure 15.20: Difference in how the \\(K\\) and \\(g\\) functions aggregate points at distance \\(d\\) (\\(d\\) = 30 km in this example). All points up to \\(d\\) contribute to \\(K\\) whereas just the points in the annulus band at \\(d\\) contribute to \\(g\\). It is important to acknowledge that, much like the K function from which it is derived, the pair correlation function \\(g\\) is also susceptible to edge effects. These arise because observations close to the study area’s boundaries have an incomplete neighborhood compared to those in the interior. When counting pairs of events within a certain distance \\(d\\), circles centered on boundary points may extend outside the observed region, leading to an underestimation of interactions at those distances. Since \\(g\\) is essentially a normalized derivative of the K function, any biases introduced in the K function due to these edge phenomena will inherently affect the pair correlation function as well. Therefore, similar to K-function estimation, methods like edge corrections or the use of guard areas are often necessary to ensure accurate and unbiased estimates of \\(g\\), especially at larger distance. The plot of the \\(g\\) function for Walmart stores follows. Both the edge uncorrected and corrected \\(g\\) functions are shown. Figure 15.21: Estimated \\(g\\) function of the Massachusetts Walmart point data. Its interpretation is similar to that of the \\(K\\) function. Here, we observe distances between stores greater than expected under CSR up to about 5 km. Note that this cutoff is shorter than the 12 km threshold observed with the \\(K\\) function. The edge uncorrected \\(g\\) function is depicted by a black line, while the corrected \\(g\\) function is shown in a red line. The analysis of the \\(g\\) function suggests that Walmart stores exhibit greater clustering than expected at distances exceeding 5 kilometers. Like its \\(K\\), \\(L\\) and ANN counterparts, the \\(g\\)-function assumes stationarity and isotropy in the underlying point process. 15.4.1 Hypothesis test for the PCF Monte Carlo (MC) simulation techniques provide a robust framework for hypothesis testing with the pair correlation function (\\(g(d)\\)), similar to their application with the K function and ANN analyses. To assess whether an observed point pattern deviates significantly from a specified null hypothesis (e.g, CRS), you can generate numerous simulated point patterns under that null model. For each simulation, the \\(g\\) function is computed, creating a distribution of expected \\(g\\) values at each distance \\(d\\). These simulated distributions are also visualized as simulation envelopes (or bands) that depict the range of \\(g\\) values expected under the null hypothesis, often representing minimum and maximum values or specific percentiles (e.g., 5th and 95th) at each distance. If the observed \\(g\\) curve falls outside these envelopes at certain distances, it suggests a statistically significant departure from the null hypothesis at those spatial scales. When interpreting these graphical tests, here too caution is necessary as simply scanning pointwise envelopes across multiple distances can lead to an inflated Type I error rate (multiple comparison problem). Global envelopes can be used to address this. 15.5 Summary It is crucial to recognize that different distance-based spatial statistics, fundamental to second-order point pattern analysis, may yield varying interpretations of the same point pattern by focusing on distinct aspects of spatial interaction. For instance, the K function provides a cumulative summary of all point pairs up to a given distance, which can effectively detect patterns across multiple spatial scales but may smooth over more localized effects. In contrast, the pair correlation function offers a non-cumulative view, isolating interactions at specific distances, thus providing a more granular understanding of clustering or dispersion at particular scales. Similarly, Average Nearest Neighbor (ANN) statistics are sensitive to short-range interactions but might overlook broader spatial structures within the pattern. A critical consideration for all these methods is the reliance on underlying assumptions, most notably stationarity and isotropy. The assumption of stationarity is particularly vital as observed patterns can otherwise be confounded by first-order spatial variation (inhomogeneity) rather than true inter-event interaction. Another challenge is edge effects where observations near study boundaries can lead to biased estimates, especially at larger distances thus necessitating appropriate corrections to ensure accurate results. These inherent differences and sensitivities underscore that no single statistic provides a complete picture of a spatial point pattern. Therefore, you should employ multiple complementary measures, examine their underlying assumptions, and, where possible, utilize simulation envelopes or hypothesis tests to robustly assess the statistical significance and nature of observed patterns across various spatial scales "],["chp16_0.html", "Chapter 16 Spatial Interpolation 16.1 Deterministic Approach to Interpolation 16.2 Statistical Approach to Interpolation 16.3 Summary", " Chapter 16 Spatial Interpolation Given a distribution of point meteorological stations showing precipitation values, how I can estimate the precipitation values where data were not observed? Figure 16.1: Average yearly precipitation (reported in inches) for several meteorological sites in Texas. To help answer this question, we need to clearly define the nature of our point dataset. We’ve already encountered point data earlier in the course where our interest was in creating point density maps using different kernel windows. However, the point data used represented a complete enumeration of discrete events or observations–i.e. the entity of interest only occurred a discrete locations within a study area and therefore could only be measured at those locations. Here, our point data represents sampled observations of an entity that can be measured anywhere within our study area. So creating a point density raster from this data would only make sense if we were addressing the questions like “where are the meteorological stations concentrated within the state of Texas?”. Another class of techniques used with points that represent samples of a continuous field are interpolation methods. There are many interpolation tools available, but these tools can usually be grouped into two categories: deterministic and statistical interpolation methods. 16.1 Deterministic Approach to Interpolation We will explore two deterministic methods: proximity (aka Thiessen) techniques and inverse distance weighted techniques (IDW for short). 16.1.1 Proximity interpolation This is probably the simplest (and possibly one of the oldest) interpolation method. It was introduced by Alfred H. Thiessen more than a century ago. The goal is simple: Assign to all unsampled locations the value of the closest sampled location. This generates a tessellated surface whereby lines that split the midpoint between each sampled location are connected thus enclosing an area. Each area ends up enclosing a sample point whose value it inherits. Figure 16.2: Tessellated surface generated from discrete point samples. This is also known as a Thiessen interpolation. One problem with this approach is that the surface values change abruptly across the tessellated boundaries. This is not representative of most surfaces in nature. Thiessen’s method was very practical in his days when computers did not exist. But today, computers afford us more advanced methods of interpolation as we will see next. 16.1.2 Inverse Distance Weighted (IDW) The IDW technique computes an average value for unsampled locations using values from nearby weighted locations. The weights are proportional to the proximity of the sampled points to the unsampled location and can be specified by the IDW power coefficient. The larger the power coefficient, the stronger the weight of nearby points as can be gleaned from the following equation that estimates the value \\(z\\) at an unsampled location \\(j\\): \\[ \\hat{Z_j} = \\frac{\\sum_i{Z_i/d^n_{ij}}}{\\sum_i{1/d^n_{ij}}} \\] The carat \\(\\hat{}\\) above the variable \\(z\\) reminds us that we are estimating the value at \\(j\\). The parameter \\(n\\) is the weight parameter that is applied as an exponent to the distance thus amplifying the irrelevance of a point at location \\(i\\) as distance to \\(j\\) increases. So a large \\(n\\) results in nearby points wielding a much greater influence on the unsampled location than a point further away resulting in an interpolated output looking like a Thiessen interpolation. On the other hand, a very small value of \\(n\\) will give all points within the search radius equal weight such that all unsampled locations will represent nothing more than the mean values of all sampled points within the search radius. In the following figure, the sampled points and values are superimposed on top of an (IDW) interpolated raster generated with a \\(n\\) value of 2. Figure 16.3: An IDW interpolation of the average yearly precipitation (reported in inches) for several meteorological sites in Texas. An IDW power coefficient of 2 was used in this example. In the following example, an \\(n\\) value of 15 is used to interpolate precipitation. This causes nearby points to exert greater influence on the unsampled locations. Note the similarity in output to the proximity (Thiessen) interpolation. Figure 16.4: An IDW interpolation of the average yearly precipitation (reported in inches) for several meteorological sites in Texas. An IDW power coefficient of 15 was used in this example. 16.1.3 Fine tuning the interpolation parameters Finding the best set of input parameters to create an interpolated surface can be a subjective proposition. Beyond visual inspection, how can you quantify the accuracy of the estimated values? One option is to split the points into two sets: the points used in the interpolation operation and the points used to validate the results. While this method is easily implemented (even via a pen and paper adoption) it does suffer from significant loss in power–i.e. we are using just half of the information to estimate the unsampled locations. A better approach (and one easily implemented in a computing environment) is to remove one data point from the dataset and interpolate its value using all other points in the dataset then repeating this process for each point in that dataset (while making sure that the interpolator parameters remain constant across each interpolation). The interpolated values are then compared with the actual values from the omitted point. This method is sometimes referred to as jackknifing or leave-one-out cross-validation. The performance of the interpolator can be summarized by computing the root-mean of squared residuals (RMSE) from the errors as follows: \\[ RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat {Z_{i}} - Z_i)^2}{n}} \\] where \\(\\hat {Z_{i}}\\) is the interpolated value at the unsampled location i (i.e. location where the sample point was removed), \\(Z_i\\) is the true value at location i and \\(n\\) is the number of points in the dataset. We can create a scatterplot of the predicted vs. expected precipitation values from our dataset. The solid diagonal line represents the one-to-one slope (i.e. if the predicted values matched the true values exactly, then the points would fall on this line). The red dashed line is a linear fit to the points which is here to help guide our eyes along the pattern generated by these points. Figure 16.5: Scatter plot pitting predicted values vs. the observed values at each sampled location following a leave-one-out cross-validation analysis. The computed RMSE from the above working example is 6.989 inches. We can extend our exploration of the interpolator’s accuracy by creating a map of the confidence intervals. This involves stacking all \\(n\\) interpolated surfaces from the aforementioned jackknife technique, then computing the confidence interval for each location ( pixel) in the output map (raster). If the range of interpolated values from the jackknife technique for an unsampled location \\(i\\) is high, then this implies that this location is highly sensitive to the presence or absence of a single point from the sample point locations thus producing a large confidence interval (i.e. we can’t be very confident of the predicted value). Conversely, if the range of values estimated for location \\(i\\) is low, then a small confidence interval is computed (providing us with greater confidence in the interpolated value). The following map shows the 95% confidence interval for each unsampled location (pixel) in the study extent. Figure 16.6: In this example an IDW power coefficient of 2 was used and the search parameters were confined to a minimum number of points of 10 and a maximum number of points of 15. The search window was isotropic. Each pixel represents the range of precipitation values (in inches) around the expected value given a 95% confidence interval. IDW interpolation is probably one of the most widely used interpolators because of its simplicity. In many cases, it can do an adequate job. However, the choice of power remains subjective. There is another class of interpolators that makes use of the information provided to us by the sample points–more specifically, information pertaining to 1st and 2nd order behavior. These interpolators are covered next. 16.2 Statistical Approach to Interpolation The statistical interpolation methods include surface trend and Kriging. 16.2.1 Trend Surfaces It may help to think of trend surface modeling as a regression on spatial coordinates where the coefficients apply to those coordinate values and (for more complicated surface trends) to the interplay of the coordinate values. We will explore a 0th order, 1st order and 2nd order surface trend in the following sub-sections. 16.2.1.1 0th Order Trend Surface The first model (and simplest model), is the 0th order model which takes on the following expression: Z = a where the intercept a is the mean precipitation value of all sample points (27.1 in our working example). This is simply a level (horizontal) surface whose cell values all equal 27.1. Figure 16.7: The simplest model where all interpolated surface values are equal to the mean precipitation. This makes for an uninformative map. A more interesting surface trend map is one where the surface trend has a slope other than 0 as highlighted in the next subsection. 16.2.1.2 1st Order Trend Surface The first order surface polynomial is a slanted flat plane whose formula is given by: Z = a + bX + cY where X and Y are the coordinate pairs. Figure 16.8: Result of a first order interpolation. The 1st order surface trend does a good job in highlighting the prominent east-west trend. But is the trend truly uniform along the X axis? Let’s explore a more complicated surface: the quadratic polynomial. 16.2.1.3 2nd Order Trend Surface The second order surface polynomial (aka quadratic polynomial) is a parabolic surface whose formula is given by: \\(Z = a + bX + cY + dX^2 + eY^2 + fXY\\) Figure 16.9: Result of a second order interpolation This interpolation picks up a slight curvature in the east-west trend. But it’s not a significant improvement over the 1st order trend. 16.2.2 Ordinary Kriging Several forms of Kriging interpolators exist: ordinary, universal and simple just to name a few. This section will focus on ordinary Kriging (OK) interpolation. This form of Kriging usually involves four steps: Removing any spatial trend in the data (if present). Computing the experimental variogram, \\(\\gamma\\), which is a measure of spatial autocorrelation. Defining an experimental variogram model that best characterizes the spatial autocorrelation in the data. Interpolating the surface using the experimental variogram. Adding the kriged interpolated surface to the trend interpolated surface to produce the final output. These steps our outlined in the following subsections. 16.2.2.1 De-trending the data One assumption that needs to be met in ordinary Kriging is that the mean and the variation in the entity being studied is constant across the study area. In other words, there should be no global trend in the data (the term drift is sometimes used to describe the trend in other texts). This assumption is clearly not met with our Texas precipitation dataset where a prominent east-west gradient is observed. This requires that we remove the trend from the data before proceeding with the Kriging operations. Many pieces of software will accept a trend model (usually a first, second or third order polynomial). In the steps that follow, we will use the first order fit computed earlier to de-trend our point values (recall that the second order fit provided very little improvement over the first order fit). Removing the trend leaves us with the residuals that will be used in Kriging interpolation. Note that the modeled trend will be added to the kriged interpolated surface at the end of the workflow. Figure 16.10: Map showing de-trended precipitation values (aka residuals). These detrended values are then passed to the ordinary Kriging interpolation operations. You can think of these residuals as representing variability in the data not explained by the global trend. If variability is present in the residuals then it is best characterized as a distance based measure of variability (as opposed to a location based measure). 16.2.2.2 Experimental Variogram In Kriging interpolation, we focus on the spatial relationship between location attribute values. More specifically, we are interested in how these attribute values (precipitation residuals in our working example) vary as the distance between location point pairs increases. We can compute the difference, \\(\\gamma\\), in precipitation values by squaring their differences then dividing by 2. For example, if we take two meteorological stations (one whose de-trended precipitation value is -1.2 and the other whose value is 1.6), Figure 16.11: Locations of two sample sites used to demonstrate the calculation of gamma. we can compute their difference (\\(\\gamma\\)) as follows: \\[ \\gamma = \\frac{(Z_2 - Z_1)^2}{2} = \\frac{(-1.2 - (1.6))^2}{2} = 3.92 \\] We can compute \\(\\gamma\\) for all point pairs then plot these values as a function of the distances that separate these points: Figure 16.12: Experimental variogram plot of precipitation residual values. The red point in the plot is the value computed in the above example. The distance separating those two points is about 209 km. This value is mapped in 16.12 as a red dot. The above plot is called an experimental semivariogram cloud plot (also referred to as an experimental variogram cloud plot). The terms semivariogram and variogram are often used interchangeably in geostatistics (we’ll use the term variogram henceforth since this seems to be the term of choice in current literature). Also note that the word experimental is sometimes dropped when describing these plots, but its use in our terminology is an important reminder that the points we are working with are just samples of some continuous field whose spatial variation we are attempting to model. 16.2.2.3 Sample Experimental Variogram Cloud points can be difficult to interpret due to the sheer number of point pairs (we have 465 point pairs from just 50 sample points, and this just for 1/3 of the maximum distance lag!). A common approach to resolving this issue is to “bin” the cloud points into intervals called lags and to summarize the points within each interval. In the following plot, we split the data into 15 bins then compute the average point value for each bin (displayed as red points in the plot). The red points that summarize the cloud are the sample experimental variogram estimates for each of the 15 distance bands and the plot is referred to as the sample experimental variogram plot. Figure 16.13: Sample experimental variogram plot of precipitation residual values. 16.2.2.4 Experimental Variogram Model The next step is to fit a mathematical model to our sample experimental variogram. Different mathematical models can be used; their availability is software dependent. Examples of mathematical models are shown below: Figure 16.14: A subset of variogram models available in R’s gstat package. The goal is to apply the model that best fits our sample experimental variogram. This requires picking the proper model, then tweaking the partial sill, range, and nugget parameters (where appropriate). The following figure illustrates a nonzero intercept where the nugget is the distance between the \\(0\\) variance on the \\(y\\) axis and the variogram’s model intercept with the \\(y\\) axis. The partial sill is the vertical distance between the nugget and the part of the curve that levels off. If the variogram approaches \\(0\\) on the \\(y\\)-axis, then the nugget is \\(0\\) and the partial sill is simply referred to as the sill. The distance along the \\(x\\) axis where the curve levels off is referred to as the range. Figure 16.15: Graphical description of the range, sill and nugget parameters in a variogram model. In our working example, we will try to fit the Spherical function to our sample experimental variogram. This is one of three popular models (the other two being linear and gaussian models.) Figure 16.16: A spherical model fit to our residual variogram. 16.2.2.5 Kriging Interpolation The variogram model is used by the Kriging interpolator to provide localized weighting parameters. Recall that with the IDW, the interpolated value at an unsampled site is determined by summarizing weighted neighboring points where the weighting parameter (the power parameter) is defined by the user and is applied uniformly to the entire study extent. Kriging uses the variogram model to compute the weights of neighboring points based on the distribution of those values–in essence, Kriging is letting the localized pattern produced by the sample points define the weights (in a systematic way). The detailed mathematical implementation is beyond the scope of this chapter (it’s quite involved), but the resulting output is shown in the following figure: Figure 16.17: Krige interpolation of the residual (detrended) precipitation values. Recall that the Kriging interpolation was performed on the de-trended data. In essence, we predicted the precipitation values based on localized factors. We now need to combine this interpolated surface with that produced from the trend interpolated surface to produce the following output: Figure 16.18: The final kriged surface. A valuable by-product of the Kriging operation is the variance map which gives us a measure of uncertainty in the interpolated values. The smaller the variance, the better (note that the variance values are in squared units). Figure 16.19: Variance map resulting from the Kriging analysis. 16.3 Summary Spatial interpolation techniques are used to estimate values at unsampled locations from point data that represent a continuous field. These methods fall into two main categories: deterministic and statistical. Deterministic methods include proximity (Thiessen) interpolation, which assigns the value of the closest sampled point to an unsampled location, creating a tessellated surface with abrupt changes. Inverse Distance Weighted (IDW) is another deterministic method, calculating an estimated value as a weighted average of nearby points, where weights are inversely proportional to their distance raised to a power coefficient. Jackknifing (leave-one-out cross-validation) and Root Mean Squared Error (RMSE) can quantify the accuracy of these interpolations. Statistical interpolation methods include trend surfaces and Kriging. Trend surfaces use regression on spatial coordinates to model global trends, with different orders (0th, 1st, 2nd, etc…) representing varying complexity. Ordinary Kriging is a more advanced statistical method that often involves de-trending the data, computing an experimental variogram to measure spatial autocorrelation, fitting a mathematical model (defining nugget, partial sill, and range) to this variogram, and then using this model to calculate localized weighting parameters for interpolation. The final Kriged surface combines the interpolated residuals with the original trend, and a variance map is produced to indicate uncertainty. "],["references.html", "Chapter 17 References", " Chapter 17 References Anselin, Luc, and John O’Loughlin. 1992. “Geography of international conflict and cooperation: spatial dependence and regional context in Africa.” The New Geopolitics, 39–75. Baddeley, Adrian, Ege Rubak, and Rolf Turner. 2016. Spatial Point Patterns, Methodology and Applications with r. Florida: CRC Press. Bailey, Trevor C., and Anthony C. Gatrell. 1995. Interactive Spatial Data Analysis. England: Prentice Hall. C. K. Wikle, N. Cressie, A. Zammit-Mangion. 2019. Spatio-Temporal Statistics with r. Chapman &amp; Hall/CRC. https://spacetimewithr.org/. Dykes, J. A., and D. J. Unwin. 2001. “Maps of the Census: A Rough Guide.” In Case Studies of Visualization in the Social Sciences: Technical Report 43 (43): 29–54. http://www.agocg.ac.uk/reports/visual/casestud/dykes/dykes.pdf. Freedman, David A. 1999. “Ecological Inference and the Ecological Fallacy.” Technical Report 549. University of California, Berkeley. https://statistics.berkeley.edu/sites/default/files/tech-reports/549.pdf. Haining, Robert. 2004. Spatial Data Analysis: Theory and Practice. Cambridge. O’Sullivan, David, and David Unwin. 2010. Geographic Information Analysis. New Jersey, USA: Wiley. Openshaw, Stan. 1983. “The Modifiable Areal Unit Problem.” Concepts and Techniques in Modern Geography 38. Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009. Pebesma, Edzer, and Roger Bivand. 2021. Spatial Data Science (with Applications in r). https://r-spatial.org/book/. Sun, M., and W. S. Wong. 2010. “Incorporating Data Quality Information in Mapping American Community Survey Data.” Cartography and Geography Information Science 37 (4): 285–300. Tobler, Waldo R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46 (2): 234–40. http://www.geog.ucsb.edu/~tobler/publications/pdf_docs/A-Computer-Movie.pdf. Tomlin, Dana C. 1990. GIS and Cartographic Modeling. New Jersey: Prentice Hall. Tukey, John W. 1972. “Some Graphic and Semigraphic Displays.” In Statistical Papers in Honor of George w. Snedecor, edited by T. A. Bancroft, 293–316. August 1969. Waller, Lance A., and Carol A. Gotway. 2004. Applied Spatial Statistics for Public Health Data. Wiley. "],["reading-and-writing-spatial-data-in-r.html", "A Reading and writing spatial data in R Sample files for this exercise Introduction Creating spatial objects Converting from an sf object Converting to an sf object Dissecting the sf file object Exporting to different data file formats", " A Reading and writing spatial data in R R sf terra tidygeocoder spatstat 4.5.1 1.0.21 1.8.60 1.0.6 3.4.0 Sample files for this exercise First, you will need to download some sample files from the github repository. Make sure to set your R session folder to the directory where you will want to save the sample files before running the following code chunks. download.file(&quot;https://github.com/mgimond/Spatial/raw/main/Data/Income_schooling.zip&quot;, destfile = &quot;Income_schooling.zip&quot; , mode=&#39;wb&#39;) unzip(&quot;Income_schooling.zip&quot;, exdir = &quot;.&quot;) file.remove(&quot;Income_schooling.zip&quot;) download.file(&quot;https://github.com/mgimond/Spatial/raw/main/Data/rail_inters.gpkg&quot;, destfile = &quot;./rail_inters.gpkg&quot;, mode=&#39;wb&#39;) download.file(&quot;https://github.com/mgimond/Spatial/raw/main/Data/elev.img&quot;, destfile = &quot;./elev.img&quot;, mode=&#39;wb&#39;) Introduction There are several different R spatial formats to choose from. Your choice of format will largely be dictated by the package(s) and or function(s) used in your workflow. A breakdown of formats and intended use are listed below. Data format Used with… Used in package… Used for… Comment sf vector sf, others visualizing, manipulating, querying This is the new spatial standard in R. Will also read from spatially enabled databases such as postgresSQL. raster raster raster, others visualizing, manipulating, spatial statistics This has been the most popular raster format fo rmany years. But, it is gradually being supplanted by terra SpatRaster terra terra, others visualizing, manipulating, spatial statistics This is gradually replacing raster SpatialPoints* SpatialPolygons* SpatialLines* SpatialGrid* vector and raster sp, spdep Visualizing, spatial statistics These are legacy formats. spdep now accepts sf objects ppp owin vector spatstat Point pattern analysis/statistics NA im raster spatstat Point pattern analysis/statistics NA 1 The spatial* format includes SpatialPointsDataFrame, SpatialPolygonsDataFrame, SpatialLinesDataFrame, etc… There is an attempt at standardizing the spatial format in the R ecosystem by adopting a well established set of spatial standards known as simple features. This effort results in a recently developed package called sf (Pebesma 2018). It is therefore recommended that you work in an sf framework when possible. As of this writing, most of the basic data manipulation and visualization operations can be successfully conducted using sf spatial objects. Some packages such as spdep and spatstat require specialized data object types. This tutorial will highlight some useful conversion functions for this purpose. Creating spatial objects The following sections demonstrate different spatial data object creation strategies. Reading a shapefile Shapefiles consist of many files sharing the same core filename and different suffixes (i.e. file extensions). For example, the sample shapefile used in this exercise consists of the following files: [1] &quot;Income_schooling.dbf&quot; &quot;Income_schooling.prj&quot; &quot;Income_schooling.sbn&quot; &quot;Income_schooling.sbx&quot; [5] &quot;Income_schooling.shp&quot; &quot;Income_schooling.shx&quot; Note that the number of files associated with a shapefile can vary. sf only needs to be given the *.shp name. It will then know which other files to read into R such as projection information and attribute table. library(sf) s.sf &lt;- st_read(&quot;Income_schooling.shp&quot;) Let’s view the first few records in the spatial data object. head(s.sf, n=4) # List spatial object and the first 4 attribute records Simple feature collection with 4 features and 5 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: 379071.8 ymin: 4936182 xmax: 596500.1 ymax: 5255569 Projected CRS: NAD83 / UTM zone 19N NAME Income NoSchool NoSchoolSE IncomeSE geometry 1 Aroostook 21024 0.01338720 0.00140696 250.909 MULTIPOLYGON (((513821.1 51... 2 Somerset 21025 0.00521153 0.00115002 390.909 MULTIPOLYGON (((379071.8 50... 3 Piscataquis 21292 0.00633830 0.00212896 724.242 MULTIPOLYGON (((445039.5 51... 4 Penobscot 23307 0.00684534 0.00102545 242.424 MULTIPOLYGON (((472271.3 49... Note that the sf object stores not only the geometry but the coordinate system information and attribute data as well. These will be explored later in this exercise. Reading a GeoPackage A geopackage can store more than one layer. To list the layers available in the geopackage, type: st_layers(&quot;rail_inters.gpkg&quot;) Driver: GPKG Available layers: layer_name geometry_type features fields crs_name 1 Interstate Multi Line String 35 1 NAD83 2 Rail Multi Line String 730 3 NAD83 / UTM zone 19N In this example, we have two separate layers: Interstate and Rail. We can extract each layer separately via the layer= parameter. inter.sf &lt;- st_read(&quot;rail_inters.gpkg&quot;, layer=&quot;Interstate&quot;) rail.sf &lt;- st_read(&quot;rail_inters.gpkg&quot;, layer=&quot;Rail&quot;) Reading a raster In earlier versions of this tutorial, the raster package was used to read raster files. This is being supplanted by terra which will be the package used in this and in subsequent exercises. terra will read many different raster file formats such as geoTiff, Imagine and HDF5 just to name a few. To see a list of supported raster file formats on your computer simply run: terra::gdal(drivers = TRUE) |&gt; subset(type == &quot;raster&quot;) In the following example, an Imagine raster file is read into R using the rast function. library(terra) elev.r &lt;- rast(&quot;elev.img&quot;) The object class is of type SpatRaster. class(elev.r) [1] &quot;SpatRaster&quot; attr(,&quot;package&quot;) [1] &quot;terra&quot; What sets a SpatRaster object apart from other R data file objects is its storage. By default, data files are loaded into memory, but SpatRaster objects are not. This can be convenient when working with raster files too large for memory. But this comes at a performance cost. If your RAM is large enough to handle your raster file, it’s best to load the entire dataset into memory. To check if the elev.r object is loaded into memory, run: inMemory(elev.r) [1] FALSE An output of FALSE indicates that it is not. To force the raster into memory use set.values: set.values(elev.r) class : SpatRaster size : 994, 652, 1 (nrow, ncol, nlyr) resolution : 500, 500 (x, y) extent : 336630.3, 662630.3, 4759303, 5256303 (xmin, xmax, ymin, ymax) coord. ref. : NAD83 / UTM zone 19N (EPSG:26919) source(s) : memory varname : elev name : Layer_1 min value : 0 max value : 1546 Let’s check that the raster is indeed loaded into memory: inMemory(elev.r) [1] TRUE Now let’s look at the raster’s properties: elev.r class : SpatRaster size : 994, 652, 1 (nrow, ncol, nlyr) resolution : 500, 500 (x, y) extent : 336630.3, 662630.3, 4759303, 5256303 (xmin, xmax, ymin, ymax) coord. ref. : NAD83 / UTM zone 19N (EPSG:26919) source(s) : memory varname : elev name : Layer_1 min value : 0 max value : 1546 The raster object returns its grid dimensions (number of rows and columns), pixel size/resolution (in the layer’s coordinate system units), geographic extent, native coordinate system (UTM NAD83 Zone 19 with units of meters) and min/max raster values. Creating a spatial object from a data frame Geographic point data locations recorded in a spreadsheet can be converted to a spatial point object. Note that it’s important that you specify the coordinate system used to record the coordinate pairs since such information is not stored in a data frame. In the following example, the coordinate values are recorded in a WGS 1984 geographic coordinate system (crs = 4326). # Create a simple dataframe with lat/long values df &lt;- data.frame(lon = c(-68.783, -69.6458, -69.7653), lat = c(44.8109, 44.5521, 44.3235), Name= c(&quot;Bangor&quot;, &quot;Waterville&quot;, &quot;Augusta&quot;)) # Convert the dataframe to a spatial object. Note that the # crs= 4326 parameter assigns a WGS84 coordinate system to the # spatial object p.sf &lt;- st_as_sf(df, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) p.sf Simple feature collection with 3 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: -69.7653 ymin: 44.3235 xmax: -68.783 ymax: 44.8109 Geodetic CRS: WGS 84 Name geometry 1 Bangor POINT (-68.783 44.8109) 2 Waterville POINT (-69.6458 44.5521) 3 Augusta POINT (-69.7653 44.3235) Geocoding street addresses The tidygeocoder package will convert street addresses to latitude/longitude coordinate pairs using a wide range of geocoding services such as the US census and Google. Some of these geocoding services will require an API key, others will not. Click here to see the list of geocoding services supported by tidygeocoder and their geocoding limitations. In the example that follows, the osm geocoding service is used by default. library(tidygeocoder) options(pillar.sigfig = 7) # Increase significant digits in displayed output dat &lt;- data.frame( name = c(&quot;Colby College&quot;, &quot;Bates College&quot;, &quot;Bowdoin College&quot;), address = c(&quot;4000 Mayflower drive, Waterville, ME , 04901&quot;, &quot;275 College st, Lewiston, ME 04240&quot;, &quot;255 Maine St, Brunswick, ME 04011&quot;)) geocode(.tbl = dat, address = address, method = &quot;osm&quot;) # A tibble: 3 × 4 name address lat long &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Colby College 4000 Mayflower drive, Waterville, ME , 04901 44.56848 -69.66003 2 Bates College 275 College st, Lewiston, ME 04240 44.10284 -70.20947 3 Bowdoin College 255 Maine St, Brunswick, ME 04011 43.90126 -69.95984 Another free (but manual) alternative, is to use the US Census Bureau’s web geocoding service for creating lat/lon values from a file of US street addresses. This needs to be completed via their web interface and the resulting data table (a CSV file) would then need to be loaded into R as a data frame. Converting from an sf object Packages such as spdep (older versions only) and spatsat do not support sf objects. The following sections demonstrate methods to convert from sf to other formats. Converting an sf object to a Spatial* object (spdep/sp) The following code will convert point, polyline or polygon features to a spatial* object. While the current version of spdep will now accept sf objects, converting to spatial* objects will be necessary with legacy spdep packages. In this example, an sf polygon feature is converted to a SpatialPolygonsDataFrame object. s.sp &lt;- as_Spatial(s.sf) class(s.sp) [1] &quot;SpatialPolygonsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; Converting an sf polygon object to an owin object The spatstat package is used to analyze point patterns however, in most cases, the study extent needs to be explicitly defined by a polygon object. The polygon should be of class owin. library(spatstat) s.owin &lt;- as.owin(s.sf) class(s.owin) [1] &quot;owin&quot; Note the loading of the package spatstat. This is required to access the as.owin.sf method for sf. Note too that the attribute table gets stripped from the polygon data. This is usually fine given that the only reason for converting a polygon to an owin format is for delineating the study boundary. Converting an sf point object to a ppp object The spatstat package is currently designed to work with projected (planar) coordinate system. If you attempt to convert a point object that is in a geographic coordinate system, you will get the following error message: p.ppp &lt;- as.ppp(p.sf) Error: Only projected coordinates may be converted to spatstat class objects The error message reminds us that a geographic coordinate system (i.e. one that uses angular measurements such as latitude/longitude) cannot be used with this package. If you encounter this error, you will need to project the point object to a projected coordinate system. In this example, we’ll project the p.sf object to a UTM coordinate system (epsg=32619). Coordinate systems in R are treated in a later appendix. p.sf.utm &lt;- st_transform(p.sf, 32619) # project from geographic to UTM p.ppp &lt;- as.ppp(p.sf.utm) # Create ppp object class(p.ppp) [1] &quot;ppp&quot; Note that if the point layer has an attribute table, its attributes will be converted to ppp marks. These attribute values can be accessed via marks(p.ppp). Converting a SpatRaster object to an im object To create a spatstat im raster object from a SpatRaster object, you will need to first create a three column dataframe from the SpatRaster objects with the first two columns defining the X and Y coordinate values of each cell, and the third column defining the cell values df &lt;- as.data.frame(elev.r,xy=TRUE) elev.im &lt;- as.im(df) class(elev.im) [1] &quot;im&quot; Converting to an sf object All aforementioned spatial formats, except owin, can be coerced to an sf object via the st_as_sf function. for example: st_as_sf(p.ppp) # For converting a ppp object to an sf object st_as_sf(s.sp) # For converting a Spatial* object to an sf object Dissecting the sf file object head(s.sf,3) Simple feature collection with 3 features and 5 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: 379071.8 ymin: 4936182 xmax: 596500.1 ymax: 5255569 Projected CRS: NAD83 / UTM zone 19N NAME Income NoSchool NoSchoolSE IncomeSE geometry 1 Aroostook 21024 0.01338720 0.00140696 250.909 MULTIPOLYGON (((513821.1 51... 2 Somerset 21025 0.00521153 0.00115002 390.909 MULTIPOLYGON (((379071.8 50... 3 Piscataquis 21292 0.00633830 0.00212896 724.242 MULTIPOLYGON (((445039.5 51... The first line of output gives us the geometry type, MULTIPOLYGON, a multi-polygon data type. This is also referred to as a multipart polygon. A single-part sf polygon object will adopt the POLYGON geometry. The next few lines of output give us the layer’s bounding extent in the layer’s native coordinate system units. You can extract the extent via the st_bbox() function as in st_bbox(s.sf). The following code chunk can be used to extract addition coordinate information from the data. st_crs(s.sf) Depending on the version of the PROJ library used by sf, you can get two different outputs. If your version of sf is built with a version of PROJ older than 6.0, the output will consist of an epsg code (when available) and a proj4 string as follows: Coordinate Reference System: EPSG: 26919 proj4string: &quot;+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs&quot; If your version of sf is built with a version of PROJ 6.0 or greater, the output will consist of a user defined CS definition (e.g. an epsg code), if available, and a Well Known Text (WKT) formatted coordinate definition that consists of a series of [ ] tags as follows: Coordinate Reference System: User input: NAD83 / UTM zone 19N wkt: PROJCRS[&quot;NAD83 / UTM zone 19N&quot;, BASEGEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,26919]] The WKT format will usually start with a PROJCRS[...] tag for a projected coordinate system, or a GEOGCRS[...] tag for a geographic coordinate system. More information on coordinate systems in R can be found in the coordinate systems appendix. What remains of the sf summary output is the first few records of the attribute table. You can extract the object’s table to a dedicated data frame via: s.df &lt;- data.frame(s.sf) class(s.df) [1] &quot;data.frame&quot; head(s.df, 5) NAME Income NoSchool NoSchoolSE IncomeSE geometry 1 Aroostook 21024 0.01338720 0.001406960 250.909 MULTIPOLYGON (((513821.1 51... 2 Somerset 21025 0.00521153 0.001150020 390.909 MULTIPOLYGON (((379071.8 50... 3 Piscataquis 21292 0.00633830 0.002128960 724.242 MULTIPOLYGON (((445039.5 51... 4 Penobscot 23307 0.00684534 0.001025450 242.424 MULTIPOLYGON (((472271.3 49... 5 Washington 20015 0.00478188 0.000966036 327.273 MULTIPOLYGON (((645446.5 49... The above chunk will also create a geometry column. This column is somewhat unique in that it stores its contents as a list of geometry coordinate pairs (polygon vertex coordinate values in this example). str(s.df) &#39;data.frame&#39;: 16 obs. of 6 variables: $ NAME : chr &quot;Aroostook&quot; &quot;Somerset&quot; &quot;Piscataquis&quot; &quot;Penobscot&quot; ... $ Income : int 21024 21025 21292 23307 20015 21744 21885 23020 25652 24268 ... $ NoSchool : num 0.01339 0.00521 0.00634 0.00685 0.00478 ... $ NoSchoolSE: num 0.001407 0.00115 0.002129 0.001025 0.000966 ... $ IncomeSE : num 251 391 724 242 327 ... $ geometry :sfc_MULTIPOLYGON of length 16; first list element: List of 1 ..$ :List of 1 .. ..$ : num [1:32, 1:2] 513821 513806 445039 422284 424687 ... ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;XY&quot; &quot;MULTIPOLYGON&quot; &quot;sfg&quot; You can also opt to remove this column prior to creating the dataframe as follows: s.nogeom.df &lt;- st_set_geometry(s.sf, NULL) class(s.nogeom.df) [1] &quot;data.frame&quot; head(s.nogeom.df, 5) NAME Income NoSchool NoSchoolSE IncomeSE 1 Aroostook 21024 0.01338720 0.001406960 250.909 2 Somerset 21025 0.00521153 0.001150020 390.909 3 Piscataquis 21292 0.00633830 0.002128960 724.242 4 Penobscot 23307 0.00684534 0.001025450 242.424 5 Washington 20015 0.00478188 0.000966036 327.273 Exporting to different data file formats You can export an sf object to many different spatial file formats such as a shapefile or a geopackage. st_write(s.sf, &quot;shapefile_out.shp&quot;, driver = &quot;ESRI Shapefile&quot;) # create to a shapefile st_write(s.sf, &quot;s.gpkg&quot;, driver = &quot;GPKG&quot;) # Create a geopackage file If the file you are writing to already exists, the above will throw an error. To force an overwrite, simply add the delete_layer = TRUE argument to the st_write function. You can see a list of writable vector formats via: gdal(drivers = TRUE) |&gt; subset(can %in% c(&quot;write&quot;, &quot;read/write&quot; ) &amp; type == &quot;vector&quot;) The value in the name column is the driver name to pass to the driver = argument in the st_write() function. To export a raster to a data file, use writeRaster() function. writeRaster(elev.r, &quot;elev_out.tif&quot;, gdal = &quot;GTiff&quot; ) # Create a geoTiff file writeRaster(elev.r, &quot;elev_out.img&quot;, gdal = &quot;HFA&quot; ) # Create an Imagine raster file You can see a list of writable raster formats via: gdal(drivers = TRUE) |&gt; subset(can %in% c(&quot;write&quot;, &quot;read/write&quot; ) &amp; type == &quot;raster&quot;) The value in the name column is the driver name to pass to the gdal = argument in the writeRaster() function. References Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009. "],["mapping-data-in-r.html", "B Mapping data in R Sample files for this exercise tmap ggplot2 plot_sf", " B Mapping data in R R sf terra tmap ggplot2 4.5.1 1.0.21 1.8.60 4.1 3.5.2 There are many mapping environments that can be adopted in R. Three are presented in this tutorial: tmap, ggplot2 and plot_sf. Sample files for this exercise Data used in the following exercises can be loaded into your current R session by running the following chunk of code. library(sf) library(terra) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/elev.RDS&quot;)) elev.r &lt;- unwrap(readRDS(z)) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/inter_sf.RDS&quot;)) inter.sf &lt;- readRDS(z) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/rail_sf.RDS&quot;)) rail.sf &lt;- readRDS(z) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/s_sf.RDS&quot;)) s.sf &lt;- readRDS(z) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/p_sf.RDS&quot;)) p.sf &lt;- readRDS(z) The data objects consist of five layers: an elevation raster (elev.r), an interstate polyline layer (inter.sf), a point cities layer (p.sf), a railroad polyline layer (rail.sf) and a Maine counties polygon layer (s.sf). All vector layers are sf objects. All layers are in a UTM/NAD83 projection (Zone 19N) except p.sf which is in a WGS 1984 geographic coordinate system. tmap The tmap package is specifically developed for mapping spatial data. As such, it offers the greatest mapping options. The package recognizes sf, raster and Spatial* objects. The basics To map the counties polygon layer using a grey color scheme, type: library(tmap) tm_shape(s.sf) + tm_polygons(col=&quot;grey&quot;, border.col=&quot;white&quot;) The tm_shape function loads the spatial object (vector or raster) into the mapping session. The tm_polygons function is one of many tmap functions that dictates how the spatial object is to be mapped. The col parameter defines either the polygon fill color or the spatial object’s attribute column to be used to define the polygons’ color scheme. For example, to use the Income attribute value to define the color scheme, type: tm_shape(s.sf) + tm_polygons(col=&quot;Income&quot;, border.col = &quot;white&quot;) Note the + symbol used to piece together the functions (this is similar to the ggplot2 syntax). You can customize the map by piecing together various map element functions. For example, to move the legend box outside of the main map body add the tm_legend(outside = TRUE) function to the mapping operation. tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, border.col = &quot;white&quot;) + tm_legend(outside = TRUE) You can also choose to omit the legend box (via the legend.show = FALSE parameter) and the data frame border (via the tm_layout(frame = FALSE) function): tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, border.col = &quot;white&quot;, legend.show=FALSE) + tm_layout(frame = FALSE) If you want to omit the polygon border lines from the plot, simply add the border.col = NULL parameter to the tm_polygons function. tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, border.col = NULL) + tm_legend(outside = TRUE) Note that the tm_fill function is nearly identical to the tm_polygons function with the difference being that the tm_fill function does not draw polygon borders. Combining layers You can easily stack layers by piecing together additional tm_shapefunctions. In the following example, the railroad layer and the point layer are added to the income map. The railroad layer is mapped using the tm_lines function and the cities point layer is mapped using the tm_dots function. Note that layers are pieced together using the + symbol. tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, border.col = NULL) + tm_legend(outside = TRUE) + tm_shape(rail.sf) + tm_lines(col=&quot;grey70&quot;) + tm_shape(p.sf) + tm_dots(size=0.3, col=&quot;black&quot;) Layers are stacked in the order in which they are listed. In the above example, the point layer is the last layer called therefore it is drawn on top of the previously drawn layers. Note that if a layer’s coordinate system is properly defined, tmap will reproject, on-the-fly, any layer whose coordinate system does not match that of the first layer in the stack. In this example, s.sf defines the map’s coordinate system (UTM/NAD83). p.sf is in a geographic coordinate system and is thus reprojected on-the-fly to properly overlap the other layers in the map. Tweaking classification schemes You can control the classification type, color scheme, and bin numbers via the tm_polygons function. For example, to apply a quantile scheme with 6 bins and varying shades of green, type: tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, style = &quot;quantile&quot;, n = 6, palette = &quot;Greens&quot;) + tm_legend(outside = TRUE) Other style classification schemes include fixed, equal, jenks, kmeans and sd. If you want to control the breaks manually set style=fixed and specify the classification breaks using the breaks parameter. For example, tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, style = &quot;fixed&quot;,palette = &quot;Greens&quot;, breaks = c(0, 23000, 27000, 100000 )) + tm_legend(outside = TRUE) If you want a bit more control over the legend elements, you can tweak the labels parameter as in, tm_shape(s.sf) + tm_polygons(&quot;Income&quot;, style = &quot;fixed&quot;,palette = &quot;Greens&quot;, breaks = c(0, 23000, 27000, 100000 ), labels = c(&quot;under $23,000&quot;, &quot;$23,000 to $27,000&quot;, &quot;above $27,000&quot;), text.size = 1) + tm_legend(outside = TRUE) Tweaking colors There are many color schemes to choose from, but you will probably want to stick to color swatches established by Cynthia Brewer. These palettes are available in tmap and their names are listed below. For sequential color schemes, you can choose from the following palettes. For divergent color schemes, you can choose from the following palettes. For categorical color schemes, you can choose from the following palettes. For example, to map the county names using the Pastel1 categorical color scheme, type: tm_shape(s.sf) + tm_polygons(&quot;NAME&quot;, palette = &quot;Pastel1&quot;) + tm_legend(outside = TRUE) To map the percentage of the population not having attained a high school degree (column labeled NoSchool in s.sf) using a YlOrBr palette with 8 bins while modifying the legend title to read “Fraction without a HS degree”, type: tm_shape(s.sf) + tm_polygons(&quot;NoSchool&quot;, style=&quot;quantile&quot;, palette = &quot;YlOrBr&quot;, n=8, title=&quot;Fraction without \\na HS degree&quot;) + tm_legend(outside = TRUE) The character \\n in the “Fraction without \\na HS degree” string is interpreted by R as a new line (carriage return). If you want to reverse the color scheme simply add the minus symbol - in front of the palette name as in palette = \"-YlOrBr\" Adding labels You can add text and labels using the tm_text function. In the following example, point labels are added to the right of the points with the text left justified (just = \"left\") and with an x offset of 0.5 units for added buffer between the point and the text. tm_shape(s.sf) + tm_polygons(&quot;NAME&quot;, palette = &quot;Pastel1&quot;, border.col = &quot;white&quot;) + tm_legend(outside = TRUE) + tm_shape(p.sf) + tm_dots(size= .3, col = &quot;red&quot;) + tm_text(&quot;Name&quot;, just = &quot;left&quot;, xmod = 0.5, size = 0.8) The tm_text function accepts an auto placement option via the parameter auto.placement = TRUE. This uses a simulated annealing algorithm. Note that this automated approach may not generate the same text placement after each run. Adding a grid or graticule You can add a grid or graticule to the map using the tm_grid function. You will need to modify the map’s default viewport setting via the tm_layout function to provide space for the grid labels. In the following example, the grid is generated using the layer’s UTM coordinate system and is divided into roughly four segments along the x-axis and five segments along the y-axis. The function will adjust the grid placement so as to generate “pretty” label values. tm_shape(s.sf) + tm_polygons(&quot;NAME&quot;, palette = &quot;Pastel1&quot;) + tm_legend(outside = TRUE) + tm_layout(outer.margins = c(.1,.1,.1,.1)) + tm_grid(labels.inside.frame = FALSE, n.x = 4, n.y = 5) To generate a graticule (lines of latitude and longitude), simply modify the grid’s coordinate system to a geographic one using either an EPSG defined coordinate system, or a PROJ4 formatted string. But note that the PROJ string syntax is falling out of favor in current and future R spatial environments so, if possible, adopt an EPSG (or OGC) code. Here, we’ll use EPSG:4326 which defines the WGS 1984 geographic coordinate system. We will also modify the grid placement by explicitly specifying the lat/long grid values. tm_shape(s.sf) + tm_polygons(&quot;NAME&quot;, palette = &quot;Pastel1&quot;) + tm_legend(outside = TRUE) + tm_layout(outer.margins = c(.1,.1,.1,.1)) + tm_grid(labels.inside.frame = FALSE, x = c(-70.5, -69, -67.5), y = c(44, 45, 46, 47), projection = &quot;EPSG:4326&quot;) Adding the ° symbol to the lat/long values requires a bit more code: tm_shape(s.sf) + tm_polygons(&quot;NAME&quot;, palette = &quot;Pastel1&quot;) + tm_legend(outside = TRUE) + tm_layout(outer.margins = c(.1,.1,.1,.1)) + tm_grid(labels.inside.frame = FALSE, x = c(-70.5, -69, -67.5) , y = c(44, 45, 46, 47), projection = &quot;+proj=longlat&quot;, labels.format = list(fun=function(x) {paste0(x,intToUtf8(176))} ) ) Here, we use the unicode decimal representation of the ° symbol (unicode 176) and pass it to the intToUtf8 function. A list of unicode characters and their decimal representation can be found on this Wikipedia page. Adding statistical plots A histogram of the variables being mapped can be added to the legend element. By default, the histogram will inherit the colors used in the classification scheme. tm_shape(s.sf) + tm_polygons(&quot;NoSchool&quot;, palette = &quot;YlOrBr&quot;, n = 6, legend.hist = TRUE, title = &quot;% no school&quot;) + tm_legend(outside = TRUE, hist.width = 2) Mapping raster files Raster objects can be mapped by specifying the tm_raster function. For example to plot the elevation raster and assign 64 continuous shades of the built-in terrain color ramp, type: tm_shape(elev.r) + tm_raster(style = &quot;cont&quot;, title = &quot;Elevation (m)&quot;, palette = terrain.colors(64))+ tm_legend(outside = TRUE) Note the use of another style parameter option: cont for continuous color scheme. You can choose to symbolize the raster using classification breaks instead of continuous colors. For example, to manually set the breaks to 50, 100, 500, 750, 1000, and 15000 meters, type: tm_shape(elev.r) + tm_raster(style = &quot;fixed&quot;, title = &quot;Elevation (m)&quot;, breaks = c(0, 50, 100, 500, 750, 1000, 15000), palette = terrain.colors(5))+ tm_legend(outside = TRUE) Other color gradients that R offers include, heat.colors, rainbow, and topo.colors. You can also create your own color ramp via the colorRampPalette function. For example, to generate a 12 bin quantile classification scheme using a color ramp that changes from darkolivegreen4 to yellow to brown (these are built-in R colors), and adding a histogram to view the distribution of colors across pixels, type: tm_shape(elev.r) + tm_raster(style = &quot;quantile&quot;, n = 12, title = &quot;Elevation (m)&quot;, palette = colorRampPalette( c(&quot;darkolivegreen4&quot;,&quot;yellow&quot;, &quot;brown&quot;))(12), legend.hist = TRUE)+ tm_legend(outside = TRUE, hist.width = 2) Note that the Brewer palette names can also be used with rasters. Changing coordinate systems tmap can change the output’s coordinate system without needing to reproject the data layers. In the following example, the elevation raster, railroad layer and point city layer are mapped onto a USA Contiguous Albers Equal Area Conic projection. A lat/long grid is added as a reference. # Define the Albers coordinate system aea &lt;- &quot;+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +ellps=GRS80 +datum=NAD83&quot; # Map the data tm_shape(elev.r, projection = aea) + tm_raster(style = &quot;quantile&quot;, n = 12, palette = colorRampPalette( c(&quot;darkolivegreen4&quot;,&quot;yellow&quot;, &quot;brown&quot;))(12), legend.show = FALSE) + tm_shape(rail.sf) + tm_lines(col = &quot;grey70&quot;)+ tm_shape(p.sf) +tm_dots(size=0.5) + tm_layout(outer.margins = c(.1,.1,.1,.1)) + tm_grid(labels.inside.frame = FALSE, x = c(-70.5, -69, -67.5), y = c(44, 45, 46, 47), projection = &quot;+proj=longlat&quot;) The first data layer’s projection= parameter will define the map’s coordinate system. Note that this parameter does not need to be specified in the other layers taking part in the output map. If a projection is not explicitly defined in the first call to tm_shape, then the output map will default to the first layer’s reference system. Side-by-side maps You can piece maps together side-by-side using the tmap_arrange function. You first need to save each map to a separate object before combining them. For example: inc.map &lt;- tm_shape(s.sf) + tm_polygons(col=&quot;Income&quot;)+ tm_legend(outside=TRUE) school.map &lt;- tm_shape(s.sf) + tm_polygons(col=&quot;NoSchool&quot;)+ tm_legend(outside=TRUE) name.map &lt;- tm_shape(s.sf) + tm_polygons(col=&quot;NAME&quot;)+ tm_legend(outside=TRUE) tmap_arrange(inc.map, school.map, name.map) Splitting data by polygons or group of polygons You can split the output into groups of features based on a column attribute. For example, to split the income map into individual polygons via the NAME attribute, type: tm_shape(s.sf) + tm_polygons(col = &quot;Income&quot;) + tm_legend(outside = TRUE) + tm_facets( by = &quot;NAME&quot;, nrow = 2) The order of the faceted plot follows the alphanumeric order of the faceting attribute values. If you want to change the faceted order, you will need to change the attribute’s level order. ggplot2 If you are already familiar with ggplot2, you will find it easy to transition to spatial data visualization. The key geom used when mapping spatial data is geom_sf(). The basics If you wish to simply plot the geometric elements of a layer, type: library(ggplot2) ggplot(data = s.sf) + geom_sf() As with any ggplot operation, you can also pass the object’s name to the geom_sf() instead of the ggplot function as in: ggplot() + geom_sf(data = s.sf) This will prove practical later in this exercise when multiple layers are plotted on the map. By default, ggplot will add a graticule to the plot, even if the coordinate system associated with the layer is in a projected coordinate system. You can adopt any one of ggplot2’s gridline removal strategies to eliminate the grid from the plot. Here, we’ll make use of the theme_void() function. ggplot(data = s.sf) + geom_sf() + theme_void() If you want to have ggplot adopt the layer’s native coordinate system (UTM NAD 1983 in this example) instead of the default geographic coordinate system, type: ggplot(data = s.sf) + geom_sf() + coord_sf(datum = NULL) Or, you can explicitly assign the data layer’s datum via a call to st_crs as in ... + coord_sf(datum = st_crs(s.sf)) By setting datum to NULL, you prevent ggplot from figuring out how to convert the layer’s native coordinate system to a geographic one. You can control grid/graticule intervals using ggplot’s scale_..._continuous functions. For example: ggplot(data = s.sf) + geom_sf() + scale_x_continuous(breaks = c(-70, -69, -68)) + scale_y_continuous(breaks = 44:47) If you wish to apply a grid native to the layer’s coordinate system, type: ggplot(data = s.sf) + geom_sf() + coord_sf(datum = NULL) + scale_x_continuous(breaks = c(400000, 500000, 600000)) + scale_y_continuous(breaks = c(4900000, 5100000)) To symbolize a layer’s geometries using one of the layer’s attributes, add the aes() function. ggplot(data = s.sf, aes(fill = Income)) + geom_sf() Note that the data and aesthetics can be defined in the geom_sf function as well: ggplot() + geom_sf(data = s.sf, aes(fill = Income)) To change the border color, type: ggplot(data = s.sf, aes(fill = Income)) + geom_sf(col = &quot;white&quot;) To remove outlines, simply pass NA to col (e.g. col = NA) in the geom_sf function. Tweaking classification schemes To bin the color scheme by assigning ranges of income values to a unique set of color swatches defined by hex values, use one of the scale_fill_steps* family of functions. ggplot(data = s.sf, aes(fill = Income)) + geom_sf() + scale_fill_stepsn(colors = c(&quot;#D73027&quot;, &quot;#FC8D59&quot;, &quot;#FEE08B&quot;, &quot;#D9EF8B&quot;, &quot;#91CF60&quot;) , breaks = c(22000, 25000, 27000, 30000)) You can adopt Brewer’s color schemes by applying one of the scale_..._fermenter() functions and specifying the classification type (sequential, seq; divergent, div; or categorical, qual) and the palette name. For example, to adopt a divergent color scheme using the \"PRGn\" colors, type: ggplot(data = s.sf, aes(fill = Income)) + geom_sf() + scale_fill_fermenter(type = &quot;div&quot;, palette = &quot;PRGn&quot;, n.breaks = 4) The flip the color scheme set direction to 1. ggplot(data = s.sf, aes(fill = Income)) + geom_sf() + scale_fill_fermenter(type = &quot;div&quot;, palette = &quot;PRGn&quot;, n.breaks = 4, direction = 1) ggplot offers many advanced options. For example, we can modify the bin intervals by generating a non-uniform classification scheme and scale the legend bar so as to reflect the non-uniform intervals using the guide_coloursteps() function and its even.steps = FALSE argument. We’ll also modify the legend bar dimensions and title in this code chunk. ggplot(data = s.sf, aes(fill = Income)) + geom_sf() + scale_fill_stepsn(colors = c(&quot;#D73027&quot;, &quot;#FC8D59&quot;, &quot;#FEE08B&quot;, &quot;#D9EF8B&quot;, &quot;#91CF60&quot;, &quot;#1A9850&quot;) , breaks = c(22000, 25000, 26000, 27000, 30000), values = scales::rescale(c(22000, 25000, 26000, 27000, 30000), c(0,1)), guide = guide_coloursteps(even.steps = FALSE, show.limits = TRUE, title = &quot;Per capita Income \\n(US dollars)&quot;, barheight = unit(2.2, &quot;in&quot;), barwidth = unit(0.15, &quot;in&quot;))) Combining layers You can overlap layers in the map by adding calls to geom_sf. In such a scenario, it might be best for readability sake to specify the layer name in the geom_sf function instead of the ggplot function. ggplot() + geom_sf(data = s.sf, aes(fill = Income)) + geom_sf(data = rail.sf, col = &quot;white&quot;) + geom_sf(data = p.sf, col = &quot;green&quot;) Note that ggplot will convert coordinate systems on-the-fly as needed. Here, p.sf is in a coordinate system different from the other layers. You can also add raster layers to the map. However, the raster layer must be in a dataframe format with x, y and z columns. The elev.r raster is in a SpatRaster format and will need to be converted to a dataframe using the as.data.frame function from the raster package. This function has a special method for raster layers, as such, it adds parameters unique to this method. These include xy = TRUE which instructs the function to create x and y coordinate columns from the data, and na.rm = TRUE which removes blank cells (this will help reduce the size of our dataframe given that elev.r does not fill its extent’s rectangular outline). Since the layers are drawn in the order listed, we will move the rail.sf vector layer to the bottom of the stack. ggplot() + geom_raster(data = as.data.frame(elev.r, xy=TRUE, na.rm = TRUE), aes(x = x, y = y, fill = elev)) + scale_fill_gradientn(colours = terrain.colors(7)) + geom_sf(data = rail.sf, col = &quot;white&quot;) + geom_sf(data = p.sf, col = &quot;black&quot;) + theme(axis.title = element_blank()) # Removes axes labels plot_sf The sf package has its own plot method. This is a convenient way to generate simple plots without needing additional plotting packages. The basics By default, when passing an sf object to `plot, the function will generate as may plots as there are attribute columns. For example plot(s.sf) To limit the plot to just one of the attribute columns, limit the dataset using basic R indexing techniques. For example, to plot the Income column, type plot(s.sf[&quot;Income&quot;]) To limit the output to just the layer’s geometry, wrap the object name with the st_geometry function. plot(st_geometry(s.sf)) You can control the fill and border colors using the col and border parameters respectively. plot(st_geometry(s.sf), col =&quot;grey&quot;, border = &quot;white&quot;) Adding a graticule You can add a graticule by setting the graticule parameter to TRUE. To add graticule labels, set axes to TRUE. plot(st_geometry(s.sf), col =&quot;grey&quot;, border = &quot;white&quot;, graticule = TRUE, axes= TRUE) Combining layers To add layers, generate a new call to plot with the add parameter set to TRUE. For example, to add rail.sf and p.sf to the map, type: plot(st_geometry(s.sf), col =&quot;grey&quot;, border = &quot;white&quot;, graticule = TRUE, axes= TRUE) plot(rail.sf, col = &quot;grey20&quot;, add = TRUE) Note that plot_sf requires that the layers be in the same coordinate system. For example, adding p.sf will not show the points on the map given that it’s in a different coordinate system. sf layers can be combined with raster layers. The order in which layers are listed will matter. You will usually want to map the raster layer first, then add the vector layer(s). plot(elev.r, col = terrain.colors(30)) plot(st_geometry(rail.sf), col =&quot;grey&quot;, border = &quot;white&quot;, add = TRUE) Tweaking colors You can tweak the color schemes as well as the legend display. The latter will require the use of R’s built-in par function whereby the las = 1 parameter will render the key labels horizontal, and the omi parameter will prevent the legend labels from being cropped. OP &lt;- par(las = 1, omi=c(0,0,0,0.6)) p1 &lt;- plot(s.sf[&quot;Income&quot;], breaks = c(20000, 22000, 25000, 26000, 27000, 30000, 33000), pal = c(&quot;#D73027&quot;, &quot;#FC8D59&quot;, &quot;#FEE08B&quot;, &quot;#D9EF8B&quot;, &quot;#91CF60&quot;, &quot;#1A9850&quot;), key.width = 0.2, at = c(20000, 22000, 25000, 26000, 27000, 30000, 33000)) par(OP) While plot_sf offers succinct plotting commands and independence from other mapping packages, it is limited in its customization options. "],["anatomy-of-simple-feature-objects.html", "C Anatomy of simple feature objects Creating point ‘sf’ objects Creating polyline ‘sf’ objects Creating polygon ‘sf’ objects Extracting geometry from an sf object Alternative syntax Additional resources", " C Anatomy of simple feature objects R sf ggplot2 4.5.1 1.0.21 3.5.2 This tutorial exposes you to the building blocks of simple feature objects via the the creation of point, polyline and polygon features from scratch. Creating point ‘sf’ objects We will start off by exploring the creation of a singlepart point feature object. There are three phases in creating a point simple feature (sf) object: Defining the coordinate pairs via a point geometry object, sfg; Creating a simple feature column object, sfc, from the point geometries; Creating the simple feature object, sf. Step 1: Create the point geometry: sfg Here, we’ll create three separate point objects. We’ll adopt a geographic coordinate system, but note that we do not specify the coordinate system just yet. library(sf) p1.sfg &lt;- st_point(c(-70, 45)) p2.sfg &lt;- st_point(c(-69, 44)) p3.sfg &lt;- st_point(c(-69, 45)) Let’s check the class of one of these point geometries. class(p1.sfg) [1] &quot;XY&quot; &quot;POINT&quot; &quot;sfg&quot; What we are looking for is a sfg class. You’ll note other classes associated with this object such as POINT which defines the geometric primitive. You’ll see examples of other geometric primitives later in this tutorial. Note that if a multipart point feature object is desired, the st_multipoint() function needs to be used instead of st_point() with the coordinate pairs defined in matrix as in st_multipoint(matrix( c(-70, 45, -69, 44, -69, 45), ncol = 2, byrow = TRUE ) ). Step 2: Create a column of simple feature geometries: sfc Next, we’ll combine the point geometries into a single object. Note that if you are to define a coordinate system for the features, you can do so here via the crs= parameter. We use the WGS 1984 reference system (EPSG code of 4326). p.sfc &lt;- st_sfc( list(p1.sfg, p2.sfg, p3.sfg), crs = 4326 ) class(p.sfc) [1] &quot;sfc_POINT&quot; &quot;sfc&quot; The object is a simple feature column, sfc. More specifically, we’ve combined the point geometries into a single object whereby each geometry is assigned its own row or, to be technical, each point was assigned its own component via the list function. You can can confirm that each point geometry is assigned its own row in the following output. p.sfc Geometry set for 3 features Geometry type: POINT Dimension: XY Bounding box: xmin: -70 ymin: 44 xmax: -69 ymax: 45 Geodetic CRS: WGS 84 You can access each point using list operations. For example, to access the second point, type: p.sfc[[2]] Step 3: Create the simple feature object sf The final step is to create the simple feature object. p.sf &lt;- st_sf(p.sfc) p.sf Simple feature collection with 3 features and 0 fields Geometry type: POINT Dimension: XY Bounding box: xmin: -70 ymin: 44 xmax: -69 ymax: 45 Geodetic CRS: WGS 84 p.sfc 1 POINT (-70 45) 2 POINT (-69 44) 3 POINT (-69 45) Renaming the geometry column The above step generated a geometry column named after the input sfc object name (p.sfc in our example). This is perfectly functional since the sf object knows that this is the geometry column. We can confirm this by checking out p.sf’s attributes. attributes(p.sf) $names [1] &quot;p.sfc&quot; $row.names [1] 1 2 3 $class [1] &quot;sf&quot; &quot;data.frame&quot; $sf_column [1] &quot;p.sfc&quot; $agr factor() Levels: constant aggregate identity What we are looking for is the $sf_column attribute which is , in our example, pointing to the p.sfc column. This attribute is critical in a spatial operation that makes use of the dataframe’s spatial objects. Functions that recognize sf objects will look for this attribute to identify the geometry column. You might chose to rename the column to something more meaningful such as coords (note that some spatially enabled databases adopt the name geom). You can use the names() function to rename that column, but note that you will need to re-define the geometry column in the attributes using the st_geometry() function. names(p.sf) &lt;- &quot;coords&quot; st_geometry(p.sf) &lt;- &quot;coords&quot; p.sf Simple feature collection with 3 features and 0 fields Geometry type: POINT Dimension: XY Bounding box: xmin: -70 ymin: 44 xmax: -69 ymax: 45 Geodetic CRS: WGS 84 coords 1 POINT (-70 45) 2 POINT (-69 44) 3 POINT (-69 45) Adding attributes to an sf object The p.sf object is nothing more than a dataframe with a geometry column of list data type. typeof(p.sf$coords) [1] &quot;list&quot; Storing spatial features in a dataframe has many benefits, one of which is operating on the features’ attribute values. For example, we can add a new column with attribute values for each geometry entry. Here, we’ll assign letters to each point. Note that the order in which the attribute values are passed to the dataframe must match that of the geometry elements. p.sf$val1 &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) p.sf Simple feature collection with 3 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: -70 ymin: 44 xmax: -69 ymax: 45 Geodetic CRS: WGS 84 coords val1 1 POINT (-70 45) A 2 POINT (-69 44) B 3 POINT (-69 45) C We can use sf’s plot function to view the points. plot(p.sf, pch = 16, axes = TRUE, main = NULL) Adding a geometry column to an existing non-spatial dataframe A nifty property of the sfc object created in step 2 above is the ability to append it to an existing dataframe using the st_geometry() function. In the following example, we’ll create a dataframe, then append the geometry column to that dataframe. df &lt;- data.frame(col1 = c(&quot;A&quot;, &quot;B&quot;,&quot;C&quot;)) st_geometry(df) &lt;- p.sfc Note that once we’ve added the geometry column, df becomes a spatial feature object and the geometry column is assigned the name geometry. df Simple feature collection with 3 features and 1 field Geometry type: POINT Dimension: XY Bounding box: xmin: -70 ymin: 44 xmax: -69 ymax: 45 Geodetic CRS: WGS 84 col1 geometry 1 A POINT (-70 45) 2 B POINT (-69 44) 3 C POINT (-69 45) Creating polyline ‘sf’ objects The steps are similar to creating a point object. You first create the geometry(ies), you then combine the geometry(ies) into a spatial feature column before creating the simple feature object. First, we need to define the vertices that will define each line segment of the polyline. The order in which the vertices are defined matters: The order defines each connecting line segment ends. The coordinate pairs of each vertex are stored in a matrix. l &lt;- rbind( c(-70, 45), c(-69, 44), c(-69, 45) ) Next, we create a polyline geometry object. l.sfg &lt;- st_linestring(l) Next, we create the simple feature column. We also add the reference system definition (crs = 4326). l.sfc &lt;- st_sfc(list(l.sfg), crs = 4326) Finally, we create the simple feature object. l.sf &lt;- st_sf(l.sfc) l.sf Simple feature collection with 1 feature and 0 fields Geometry type: LINESTRING Dimension: XY Bounding box: xmin: -70 ymin: 44 xmax: -69 ymax: 45 Geodetic CRS: WGS 84 l.sfc 1 LINESTRING (-70 45, -69 44,... Even though we have multiple line segments, they are all associated with a single polyline feature, hence they each share the same attribute. plot(l.sf, type = &quot;b&quot;, pch = 16, main = NULL, axes = TRUE) Creating branching polyline features You can also create polyline features with branching segments (i.e. where at least one vertex is associated with more than two line segments). You simply need to make sure that the coordinate values for the overlapping vertices share the exact same values. # Define coordinate pairs l1 &lt;- rbind( c(-70, 45), c(-69, 44), c(-69, 45) ) l2 &lt;- rbind( c(-69, 44), c(-70, 44) ) l3 &lt;- rbind( c(-69, 44), c(-68, 43) ) # Create simple feature geometry object l.sfg &lt;- st_multilinestring(list(l1, l2, l3)) # Create simple feature column object l.sfc &lt;- st_sfc(list(l.sfg), crs = 4326) # Create simple feature object l.sf &lt;- st_sf(l.sfc) # Plot the data plot(l.sf, type = &quot;b&quot;, pch = 16, axes = TRUE) Creating polygon ‘sf’ objects General steps in creating a polygon sf spatial object from scratch include: Defining the vertices of each polygon in a matrix; Creating a list object from each matrix object (the list structure will differ between POLYGON and MULTIPOLYGON geometries); Creating an sfg polygon geometry object from the list; Creating an sf spatial object. Defining a polygon’s geometry is a bit more involved than a polyline in that a polygon defines an enclosed area. By convention, simple features record vertices coordinate pairs in a counterclockwise direction such that the area to the left of a polygon’s perimeter when traveling in the direction of the recorded vertices is the polygon’s “inside”. This is counter to the order in which vertices are recorded in a shapefile whereby the area to the right of the traveled path along the polygon’s perimeter is deemed “inside”. A polygon hole has its ring defined in the opposite direction: clockwise for a simple feature object and counterclockwise for a shapefile. For many applications in R, the ring direction will not matter, but for a few they might. So when possible, adopt the simple feature paradigm when defining the coordinate pairs. Note that importing a shapefile into an R session will usually automatically reverse the polygons’ ring direction. There are two types of polygon geometries that can be adopted depending on your needs: POLYGON and MULTIPOLYGON. POLYGON simple feature A plain polygon We’ll first create a simple polygon shaped like a triangle. The sf output structure will be similar to that for the POINT and POLYLINE objects with the coordinate pairs defining the polygon vertices stored in a geometry column. The polygon coordinate values are defined in a matrix. The last coordinate pair must match the first coordinate pair. The coordinate values will be recorded in a geographic coordinate system (latitude, longitude) but the reference system won’t be defined until the creation of the sfc object. poly1.crd &lt;- rbind( c(-66, 43), c(-70, 47), c(-70,43), c(-66, 43) ) Next, we create the POLYGON geometries. The polygon matrix needs to be wrapped in a list object. poly1.geom &lt;- st_polygon( list(poly1.crd ) ) We now have a polygon geometry. poly1.geom Next, we create a simple feature column from the polygon geometry. We’ll also define the coordinate system used to report the coordinate values. poly.sfc &lt;- st_sfc( list(poly1.geom), crs = 4326 ) poly.sfc Geometry set for 1 feature Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 Finally, to create the sf object, run the st_sf() function. poly.sf &lt;- st_sf(poly.sfc) poly.sf Simple feature collection with 1 feature and 0 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 poly.sfc 1 POLYGON ((-66 43, -70 47, -... The coordinates column is assigned the name poly.sfc by default. If you wish to change the column name to coords, for example, type the following: names(poly.sf) &lt;- &quot;coords&quot; st_geometry(poly.sf) &lt;- &quot;coords&quot; poly.sf Simple feature collection with 1 feature and 0 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 coords 1 POLYGON ((-66 43, -70 47, -... plot(poly.sf, col = &quot;bisque&quot;, axes = TRUE) A polygon with a hole In this example, we’ll add a hole to the polygon. Recall that its outer ring will need to be recorded in a counterclockwise direction and its hole in a clockwise direction. The resulting data object will have the following structure. # Polygon 1 poly1.outer.crd &lt;- rbind( c(-66, 43),c(-70, 47), c(-70,43), c(-66, 43) ) # Outer ring poly1.inner.crd &lt;- rbind( c(-68, 44), c(-69,44), c(-69, 45), c(-68, 44) ) # Inner ring Next, we combine the ring coordinates into a single geometric element. Note that this is done by combining the two coordinate matrices into a single list object. poly1.geom &lt;- st_polygon( list(poly1.outer.crd, poly1.inner.crd)) We now create the simple feature column object. poly.sfc &lt;- st_sfc( list(poly1.geom), crs = 4326 ) Finally, to create the sf object, run the st_sf() function. poly.sf &lt;- st_sf(poly.sfc) We’ll take the opportunity to rename the coordinate column (even though this is not necessary). names(poly.sf) &lt;- &quot;coords&quot; st_geometry(poly.sf) &lt;- &quot;coords&quot; poly.sf Simple feature collection with 1 feature and 0 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 coords 1 POLYGON ((-66 43, -70 47, -... Let’s now plot the sf object. plot(poly.sf, col = &quot;bisque&quot;, axes = TRUE) Combining polygons: singlepart features In this example, we’ll create two distinct polygons by adding a second polygon to the one created in the last step. The output will be a singlepart polygon feature (i.e. each polygon can be assigned its own unique attribute value). We’ll create the second polygon (the first polygon having already been created in the previous section). # Define coordinate matrix poly2.crd &lt;- rbind( c(-67, 45),c(-67, 47), c(-69,47), c(-67, 45) ) # Create polygon geometry poly2.geom &lt;- st_polygon( list(poly2.crd)) Next, we combine the geometries into a simple feature column, sfc. poly.sfc &lt;- st_sfc( list(poly1.geom , poly2.geom), crs = 4326 ) Each polygon has its own row in the sfc object. poly.sfc Geometry set for 2 features Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 Finally, to create the sf object, run the st_sf() function. poly.sf &lt;- st_sf(poly.sfc) poly.sf Simple feature collection with 2 features and 0 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 poly.sfc 1 POLYGON ((-66 43, -70 47, -... 2 POLYGON ((-67 45, -67 47, -... We’ll go ahead and rename the geometry column to coords. names(poly.sf) &lt;- &quot;coords&quot; st_geometry(poly.sf) &lt;- &quot;coords&quot; poly.sf Simple feature collection with 2 features and 0 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 coords 1 POLYGON ((-66 43, -70 47, -... 2 POLYGON ((-67 45, -67 47, -... plot(poly.sf, col = &quot;bisque&quot;, axes = TRUE) Adding attributes As with the point sf object created earlier in this exercise, we can append columns to the polygon sf object. But make sure that the order of the attribute values match the order in which the polygons are stored in the sf object. poly.sf$id &lt;- c(&quot;A&quot;, &quot;B&quot;) poly.sf Simple feature collection with 2 features and 1 field Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 coords id 1 POLYGON ((-66 43, -70 47, -... A 2 POLYGON ((-67 45, -67 47, -... B plot(poly.sf[&quot;id&quot;], axes = TRUE, main = NULL) MULTIPOLYGON simple feature: multipart features If multiple polygons are to share the same attribute record (a scenario referred to as multipart geometry in some GIS applications), you need to use the st_multipolygon() function when creating the sfg object. In this example, we’ll combine the two polygon created in the last example into a single geometry element. The multipolygon function groups polygons into a single list. If one of the polygons is made up of more than one ring (e.g. a polygon with a whole), its geometry is combined into a single sub-list object. # Create multipolygon geometry mpoly1.sfg &lt;- st_multipolygon( list( list( poly1.outer.crd, # Outer loop poly1.inner.crd), # Inner loop list( poly2.crd)) ) # Separate polygon # Create simple feature column object mpoly.sfc &lt;- st_sfc( list(mpoly1.sfg), crs = 4326) # Create simple feature object mpoly.sf &lt;- st_sf(mpoly.sfc) mpoly.sf Simple feature collection with 1 feature and 0 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 Geodetic CRS: WGS 84 mpoly.sfc 1 MULTIPOLYGON (((-66 43, -70... Note the single geometric entry in the table. Mixing singlepart and multipart elements A MULTIPOLGON geometry can be used to store a single polygon as well. In this example, we’ll create a MULTIPOLYGON sf object that will combine multipart and singlepart polygons. To make this example more interesting, we’ll have one of the elements (poly4.coords) overlapping several polygons. Note that any overlapping polygon needs to be in its own MULTIPOLYGON or POLYGON entry–if it’s added to an existing entry (i.e. combined with another polygon geometry), it may be treated as a hole, even if the coordinate values are recorded in a counterclockwise direction. poly3.coords &lt;- rbind( c(-66, 44), c(-64, 44), c(-66,47), c(-66, 44) ) poly4.coords &lt;- rbind( c(-67, 43), c(-64, 46), c(-66.5,46), c(-67, 43) ) Note the embedded list() functions in the following code chunk. mpoly1.sfg &lt;- st_multipolygon( list( list( poly1.outer.crd, # Outer loop poly1.inner.crd), # Inner loop list( poly2.crd)) ) # Separate poly mpoly2.sfg &lt;- st_multipolygon( list( list(poly3.coords))) # Unique polygon mpoly3.sfg &lt;- st_multipolygon( list( list(poly4.coords)) ) # Unique polygon Finally, we’ll generate the simple feature object, sf, via the creation of the simple feature column object, sfc. We’ll also assign the WGS 1984 geographic coordinate system (epsg = 4326). mpoly.sfc &lt;- st_sfc( list(mpoly1.sfg, mpoly2.sfg, mpoly3.sfg), crs = 4326) mpoly.sf &lt;- st_sf(mpoly.sfc) Next, we’ll add attribute values to each geometric object before generating a plot. We’ll apply a transparency to the polygons to reveal the overlapping geometries. mpoly.sf$ids &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) plot(mpoly.sf[&quot;ids&quot;], axes = TRUE, main = NULL, pal = sf.colors(alpha = 0.5, categorical = TRUE)) Note how polygon C overlaps the other polygon elements. We can check that this does not violate simple feature rules via the st_is_valid() function. st_is_valid(mpoly.sf) [1] TRUE TRUE TRUE This returns three boolean values, one for each element. A value of TRUE indicates that the geometry does not violate any rule. Avoid storing overlapping polygons in a same MULTIPOLYGON geometry. Doing so will create an “invalid” sf object which may pose problems with certain functions. Extracting geometry from an sf object You can extract the geometry from an sf object via the st_geometry function. For example, # Create sfc from sf st_geometry(mpoly.sf) Geometry set for 3 features Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -64 ymax: 47 Geodetic CRS: WGS 84 To extract coordinates from a single record in a WKT (well known text) format, type: st_geometry(mpoly.sf)[[1]] If you want the extract the coordinate pairs of the first element in a list format type: st_geometry(mpoly.sf)[[1]][] [[1]] [[1]][[1]] [,1] [,2] [1,] -66 43 [2,] -70 47 [3,] -70 43 [4,] -66 43 [[1]][[2]] [,1] [,2] [1,] -68 44 [2,] -69 44 [3,] -69 45 [4,] -68 44 [[2]] [[2]][[1]] [,1] [,2] [1,] -67 45 [2,] -67 47 [3,] -69 47 [4,] -67 45 Alternative syntax In this tutorial, you were instructed to define the coordinate pairs in matrices. This is probably the simplest way to enter coordinate values manually. You can, however, bypass the creation of a matrix and simply define the coordinate pairs using the WKT syntax. For example, to generate the POLYGON geometry object from above, you could simply type: st_as_sfc( &quot;POLYGON ((-66 43, -70 47, -70 43, -66 43), (-68 44, -69 44, -69 45, -68 44))&quot; ) Geometry set for 1 feature Geometry type: POLYGON Dimension: XY Bounding box: xmin: -70 ymin: 43 xmax: -66 ymax: 47 CRS: NA Note that the WKT syntax is that listed in the sfc and sf geometry columns. Also note that the function st_as_sfc is used as opposed to the st_sfc function used with matrices in earlier steps. Additional resources Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data”, The R Journal, pages 439-446. Pebesma, Edzar and Bivand, Roger. “Spatial Data Science: with applications in R”, https://keen-swartz-3146c4.netlify.app/ "],["vector-operations-in-r.html", "D Vector operations in R Dissolving geometries Subsetting by attribute Intersecting layers Clipping spatial objects using other spatial objects Unioning layers Buffering geometries", " D Vector operations in R R sf ggplot2 4.5.1 1.0.21 3.5.2 .scroll1 { max-height: 100px; overflow-y: auto; background-color: inherit; } Earlier versions of this tutorial made use of a combination of packages including raster and rgeos to perform most vector operations highlighted in this exercise. Many of these vector operations can now be performed using the sf package. As such, all code chunks in this tutorial make use sf for most vector operations. We’ll first load spatial objects used in this exercise. These include: A polygon layer that delineates Maine counties (USA), s1.sf; A polygon layer that delineates distances to Augusta (Maine) as concentric circles, s2.sf; A polyline layer of the interstate highway system that runs through Maine. These data are stored as sf objects. library(sf) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/Income_schooling_sf.rds&quot;)) s1.sf &lt;- readRDS(z) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/Dist_sf.rds&quot;)) s2.sf &lt;- readRDS(z) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/Highway_sf.rds&quot;)) l1.sf &lt;- readRDS(z) A map of the above layers is shown below. We’ll use the ggplot2 package too generate this and subsequent maps in this tutorial. library(ggplot2) ggplot() + geom_sf(data = s1.sf) + geom_sf(data = s2.sf, alpha = 0.5, col = &quot;red&quot;) + geom_sf(data = l1.sf, col = &quot;blue&quot;) The attributes table for both polygon objects (s1.sf and s2.sf) are shown next. Note that each shape object has a unique set of attributes as well as a unique number of records Figure 2.7: Attribute tables for the Maine spatial object, s1.sf, (left table) and the distance to Augusta spatial object, s2.sf (right table). Dissolving geometries Dissolving by contiguous shape There are two different ways to dissolve geometries that share a common boundary. Both are presented next. Option 1 To dissolve all polygons that share at least one line segment, simply pass the object name to sf’s st_union function while making sure that the by_feature option is set to FALSE. In this example, we dissolve all polygons to create a single outline of the state of Maine. ME &lt;- st_union(s1.sf, by_feature = FALSE) ggplot(ME) + geom_sf(fill = &quot;grey&quot;) Note that the dissolving process removed all attributes from the original spatial object. You’ll also note that st_union returns an sfc object even though the input object is sf. You can convert the output to an sf object using the st_sf() function as in st_sf(ME). Option 2 Another approach is to make use of the dplyr package and its group_by/summarise functions. library(dplyr) ME &lt;- s1.sf %&gt;% group_by() %&gt;% summarise() ggplot(ME) + geom_sf(fill = &quot;grey&quot;) Note that this option will also remove any attributes associated with the input spatial object, however, the output remains an sf object (this differs from the st_union output). Dissolving by attribute You can also choose to dissolve based on an attribute’s values. First, we’ll create a new column whose value will be binary (TRUE/FALSE) depending on whether or not the county income is below the counties’ median income value. s1.sf$med &lt;- s1.sf$Income &gt; median(s1.sf$Income) ggplot(s1.sf) + geom_sf(aes(fill = med)) Next, we’ll dissolve all polygons by the med attribute. Any polygons sharing at least one line segment that have the same med value will be dissolved into a single polygon. Two approaches are presented here: one using sf’s aggregate function, the other using the dplyr approach adopted in the previous section. Option 1 ME.inc &lt;- aggregate(s1.sf[&quot;med&quot;], by = list(diss = s1.sf$med), FUN = function(x)x[1], do_union = TRUE) This option will create a new field defined in the by = parameter (diss in this working example). st_drop_geometry(ME.inc) # Print the layer&#39;s attributes table diss med 1 FALSE FALSE 2 TRUE TRUE Option 2 ME.inc &lt;- s1.sf %&gt;% group_by(med) %&gt;% summarise() This option will limit the attributes to that/those listed in the group_by function. st_drop_geometry(ME.inc) # A tibble: 2 × 1 med * &lt;lgl&gt; 1 FALSE 2 TRUE A map of the resulting layer follows. ggplot(ME.inc) + geom_sf(aes(fill = med)) The dissolving (aggregating) operation will, by default, eliminate all other attribute values. If you wish to summarize other attribute values along with the attribute used for dissolving, use the dplyr piping operation option. For example, to compute the median Income value for each of the below/above median income groups type the following: ME.inc &lt;- s1.sf %&gt;% group_by(med) %&gt;% summarize(medinc = median(Income)) ggplot(ME.inc) + geom_sf(aes(fill = medinc)) To view the attributes table with both the aggregate variable, med, and the median income variable, Income, type: st_drop_geometry(ME.inc) # A tibble: 2 × 2 med medinc * &lt;lgl&gt; &lt;dbl&gt; 1 FALSE 21518 2 TRUE 27955 Subsetting by attribute You can use conventional R dataframe manipulation operations to subset by attribute values. For example, to subset by county name (e.g. Kennebec county), type: ME.ken &lt;- s1.sf[s1.sf$NAME == &quot;Kennebec&quot;,] You can, of course, use piping operations to perform the same task as follows: ME.ken &lt;- s1.sf %&gt;% filter(NAME == &quot;Kennebec&quot;) ggplot(ME.ken) + geom_sf() To subset by a range of attribute values (e.g. subset by income values that are less than the median value), type: ME.inc2 &lt;- s1.sf %&gt;% filter(Income &lt; median(Income)) ggplot(ME.inc2) + geom_sf() Intersecting layers To intersect two polygon objects, use sf’s st_intersection function. clp1 &lt;- st_intersection(s1.sf, s2.sf) ggplot(clp1) + geom_sf() st_intersection keeps all features that overlap along with their combined attributes. Note that new polygons are created which will increase the size of the attributes table beyond the size of the combined input attributes table. st_drop_geometry(clp1) NAME Income NoSchool NoSchoolSE IncomeSE med distance 8 Kennebec 25652 0.00570358 0.000917087 360.000 TRUE 20 12 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 20 14 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 20 1 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 50 5 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 50 6 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 50 7 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 50 8.1 Kennebec 25652 0.00570358 0.000917087 360.000 TRUE 50 9 Androscoggin 24268 0.00830953 0.001178660 460.606 TRUE 50 11 Knox 27141 0.00652269 0.001863920 684.849 TRUE 50 12.1 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 50 13 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 50 14.1 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 50 1.1 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 80 2 Piscataquis 21292 0.00633830 0.002128960 724.242 FALSE 80 3 Penobscot 23307 0.00684534 0.001025450 242.424 FALSE 80 5.1 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 80 6.1 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 80 7.1 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 80 9.1 Androscoggin 24268 0.00830953 0.001178660 460.606 TRUE 80 10 Hancock 28071 0.00238996 0.000784584 585.455 TRUE 80 11.1 Knox 27141 0.00652269 0.001863920 684.849 TRUE 80 12.2 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 80 13.1 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 80 14.2 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 80 1.2 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 120 2.1 Piscataquis 21292 0.00633830 0.002128960 724.242 FALSE 120 3.1 Penobscot 23307 0.00684534 0.001025450 242.424 FALSE 120 5.2 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 120 6.2 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 120 7.2 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 120 10.1 Hancock 28071 0.00238996 0.000784584 585.455 TRUE 120 13.2 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 120 15 York 28496 0.00529228 0.000737195 332.121 TRUE 120 Clipping spatial objects using other spatial objects The st_intersection can also be used to clip an input layer using another layer’s outer geometry boundaries as the “cookie cutter”. But note that the latter must be limited to its outer boundaries which may require that it be run through a dissolving operation (shown earlier in this tutorial) to dissolve internal boundaries. To clip s2.sf using the outline of s1.sf, type: clp2 &lt;- st_intersection(s2.sf, st_union(s1.sf)) ggplot(clp2) + geom_sf() The order the layers are passed to the st_intersection function matters. Flipping the input layer in the last example will clip s1.sf to s2.sf’s bounding polygon(s). clp2 &lt;- st_intersection(s1.sf, st_union(s2.sf)) ggplot(clp2) + geom_sf() Line geometries can also be clipped to polygon features. The output will be a line object that falls within the polygons of the input polygon object. For example, to output all line segments that fall within the concentric distance circles of s2.sf, type: clp3 &lt;- st_intersection(l1.sf, st_union(s2.sf)) A plot of the clipped line features is shown with the outline of the clipping feature. ggplot(clp3) + geom_sf(data = clp3) + geom_sf(data = st_union(s2.sf), col = &quot;red&quot;, fill = NA ) Unioning layers To union two polygon objects, use sf’s st_union function. For example, un1 &lt;- st_union(s2.sf,s1.sf) ggplot(un1) + geom_sf(aes(fill = NAME), alpha = 0.4) This produces the following attributes table. distance NAME Income NoSchool NoSchoolSE IncomeSE med 1 20 Aroostook 21024 0.01338720 0.001406960 250.909 FALSE 2 50 Aroostook 21024 0.01338720 0.001406960 250.909 FALSE 3 80 Aroostook 21024 0.01338720 0.001406960 250.909 FALSE 4 120 Aroostook 21024 0.01338720 0.001406960 250.909 FALSE 1.1 20 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 2.1 50 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 3.1 80 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 4.1 120 Somerset 21025 0.00521153 0.001150020 390.909 FALSE 1.2 20 Piscataquis 21292 0.00633830 0.002128960 724.242 FALSE 2.2 50 Piscataquis 21292 0.00633830 0.002128960 724.242 FALSE 3.2 80 Piscataquis 21292 0.00633830 0.002128960 724.242 FALSE 4.2 120 Piscataquis 21292 0.00633830 0.002128960 724.242 FALSE 1.3 20 Penobscot 23307 0.00684534 0.001025450 242.424 FALSE 2.3 50 Penobscot 23307 0.00684534 0.001025450 242.424 FALSE 3.3 80 Penobscot 23307 0.00684534 0.001025450 242.424 FALSE 4.3 120 Penobscot 23307 0.00684534 0.001025450 242.424 FALSE 1.4 20 Washington 20015 0.00478188 0.000966036 327.273 FALSE 2.4 50 Washington 20015 0.00478188 0.000966036 327.273 FALSE 3.4 80 Washington 20015 0.00478188 0.000966036 327.273 FALSE 4.4 120 Washington 20015 0.00478188 0.000966036 327.273 FALSE 1.5 20 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 2.5 50 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 3.5 80 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 4.5 120 Franklin 21744 0.00508507 0.001641740 530.909 FALSE 1.6 20 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 2.6 50 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 3.6 80 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 4.6 120 Oxford 21885 0.00700822 0.001318160 536.970 FALSE 1.7 20 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 2.7 50 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 3.7 80 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 4.7 120 Waldo 23020 0.00498141 0.000918837 450.909 FALSE 1.8 20 Kennebec 25652 0.00570358 0.000917087 360.000 TRUE 2.8 50 Kennebec 25652 0.00570358 0.000917087 360.000 TRUE 3.8 80 Kennebec 25652 0.00570358 0.000917087 360.000 TRUE 4.8 120 Kennebec 25652 0.00570358 0.000917087 360.000 TRUE 1.9 20 Androscoggin 24268 0.00830953 0.001178660 460.606 TRUE 2.9 50 Androscoggin 24268 0.00830953 0.001178660 460.606 TRUE 3.9 80 Androscoggin 24268 0.00830953 0.001178660 460.606 TRUE 4.9 120 Androscoggin 24268 0.00830953 0.001178660 460.606 TRUE 1.10 20 Hancock 28071 0.00238996 0.000784584 585.455 TRUE 2.10 50 Hancock 28071 0.00238996 0.000784584 585.455 TRUE 3.10 80 Hancock 28071 0.00238996 0.000784584 585.455 TRUE 4.10 120 Hancock 28071 0.00238996 0.000784584 585.455 TRUE 1.11 20 Knox 27141 0.00652269 0.001863920 684.849 TRUE 2.11 50 Knox 27141 0.00652269 0.001863920 684.849 TRUE 3.11 80 Knox 27141 0.00652269 0.001863920 684.849 TRUE 4.11 120 Knox 27141 0.00652269 0.001863920 684.849 TRUE 1.12 20 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 2.12 50 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 3.12 80 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 4.12 120 Lincoln 27839 0.00278315 0.001030800 571.515 TRUE 1.13 20 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 2.13 50 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 3.13 80 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 4.13 120 Cumberland 32549 0.00494917 0.000683236 346.061 TRUE 1.14 20 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 2.14 50 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 3.14 80 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 4.14 120 Sagadahoc 28122 0.00285524 0.000900782 544.849 TRUE 1.15 20 York 28496 0.00529228 0.000737195 332.121 TRUE 2.15 50 York 28496 0.00529228 0.000737195 332.121 TRUE 3.15 80 York 28496 0.00529228 0.000737195 332.121 TRUE 4.15 120 York 28496 0.00529228 0.000737195 332.121 TRUE Note that the union operation can generate many overlapping geometries. This is because each geometry of the layers being unioned are paired up with one another creating unique combinations of each layer’s geometries. For example, the Aroostook County polygon from s1.sf is paired with each annulus of the s2.sf layer creating four new geometries. un1 %&gt;% filter(NAME == &quot;Aroostook&quot;) Simple feature collection with 4 features and 7 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: 318980.1 ymin: 4788093 xmax: 596500.1 ymax: 5255569 Projected CRS: +proj=utm +zone=19 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0 distance NAME Income NoSchool NoSchoolSE IncomeSE med 1 20 Aroostook 21024 0.0133872 0.00140696 250.909 FALSE 2 50 Aroostook 21024 0.0133872 0.00140696 250.909 FALSE 3 80 Aroostook 21024 0.0133872 0.00140696 250.909 FALSE 4 120 Aroostook 21024 0.0133872 0.00140696 250.909 FALSE geometry 1 MULTIPOLYGON (((438980 4928... 2 MULTIPOLYGON (((438980 4958... 3 MULTIPOLYGON (((438980 4988... 4 MULTIPOLYGON (((438980 5028... The union operation creates all possible pairs of geometries between both input objects (i.e. 4 circle geometries from s2.sf times 16 county geometries from s1.sf for a total of 64 geometries). Buffering geometries To buffer point, line or polygon geometries, use sf’s st_buffer function. For example, the following code chunk generates a 10 km (10,000 m) buffer around the polyline segments. l1.sf.buf &lt;- st_buffer(l1.sf, dist = 10000) ggplot(l1.sf.buf) + geom_sf() + coord_sf(ndiscr = 1000) To create a continuous polygon geometry (i.e. to eliminate overlapping buffers), we’ll follow up with one of the dissolving techniques introduced earlier in this tutorial. l1.sf.buf.dis &lt;- l1.sf.buf %&gt;% group_by() %&gt;% summarise() ggplot(l1.sf.buf.dis) + geom_sf() If you want to preserve an attribute value (such as highway number), modify the above code as follows: l1.sf.buf.dis &lt;- l1.sf.buf %&gt;% group_by(Number) %&gt;% summarise() ggplot(l1.sf.buf.dis, aes(fill=Number) ) + geom_sf(alpha = 0.5) "],["mapping-rates-in-r.html", "E Mapping rates in R Raw Rates Standardized mortality ratios (relative risk) Dykes and Unwin’s chi-square statistic Unstable ratios", " E Mapping rates in R R spdep classInt RColorBrewer sf sp 4.5.1 1.3.13 0.4.11 1.1.3 1.0.21 2.2.0 In this exercise, we’ll make use of sf’s plot method instead of tmap to take advantage of sf’s scaled keys which will prove insightful when exploring rate mapping techniques that adopt none uniform classification schemes. The following libraries are used in the examples that follow. library(spdep) library(classInt) library(RColorBrewer) library(sf) library(sp) Next, we’ll initialize some color palettes. pal1 &lt;- brewer.pal(6,&quot;Greys&quot;) pal2 &lt;- brewer.pal(8,&quot;RdYlGn&quot;) pal3 &lt;- c(brewer.pal(9,&quot;Greys&quot;), &quot;#FF0000&quot;) The Auckland dataset from the spdep package will be used throughout this exercise. Some of the graphics that follow are R reproductions of Bailey and Gatrell’s book, Interactive Spatial Data Analysis (Bailey and Gatrell 1995). auckland &lt;- st_read(system.file(&quot;shapes/auckland.gpkg&quot;, package=&quot;spData&quot;)[1]) Reading layer `auckland&#39; from data source `C:\\Users\\mgimond\\AppData\\Local\\R\\win-library\\4.5\\spData\\shapes\\auckland.gpkg&#39; using driver `GPKG&#39; Simple feature collection with 167 features and 4 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: 7.6 ymin: -4.3 xmax: 91.5 ymax: 99.3 Projected CRS: Undefined Cartesian SRS with unknown unit The Auckland data represents total infant deaths (under the age of five) for Auckland, New Zealand, spanning the years 1977 through 1985 for different census area units. The following block of code maps these counts by area. Both equal interval and quantile classification schemes of the same data are mapped. brks1 &lt;- classIntervals(auckland$M77_85, n = 6, style = &quot;equal&quot;) brks2 &lt;- classIntervals(auckland$M77_85, n = 6, style = &quot;quantile&quot;) plot(auckland[&quot;M77_85&quot;], breaks = brks1$brks, pal = pal1, at = round(brks1$brks,2), main = &quot;Equal interval breaks&quot;, key.pos = 4, las = 1) plot(auckland[&quot;M77_85&quot;], breaks = brks2$brks, pal = pal1, at = brks2$brks, main = &quot;Quantile breaks&quot;, key.pos = 4, las = 1) These are examples of choropleth maps (choro = area and pleth = value) where some attribute (an enumeration of child deaths in this working example) is aggregated over a defined area (e.g. census area units) and displayed using two different classification schemes. Since the area units used to map death counts are not uniform in shape and area across Auckland, there is a tendency to assign more “visual weight” to polygons having larger areas than those having smaller areas. In our example, census units in the southern end of Auckland appear to have an “abnormally” large infant death count. Another perceptual interpretation of the map is one that flags those southern units as being “problematic” or of “great concern”. However, as we shall see in the following sections, this perception may not reflect reality. We therefore seek to produce perceptually tenable maps. Dykes and Unwin (Dykes and Unwin 2001) define a similar concept called map stability which seeks to produce maps that convey real effects. Raw Rates A popular approach for correcting for biased visual weights (due, for instance, to different unit area sizes) is to normalize the count data by area thus giving a count per unit area. Though this may make sense for population count data, it does not make a whole lot sense when applied to mortality counts; we are usually interested in the number of deaths per population count and not in the number of deaths per unit area. In the next chunk of code we extract population count under the age of 5 from the Auckland data set and assign this value to the variable pop. Likewise, we extract the under 5 mortality count and assign this value to the variable mor. Bear in mind that the mortality count spans a 9 year period. Since mortality rates are usually presented in rates per year, we need to multiply the population value (which is for the year 1981) by nine. This will be important in the subsequent code when we compute mortality rates. pop &lt;- auckland$Und5_81 * 9 mor &lt;- auckland$M77_85 Next, we will compute the raw rates (infant deaths per 1000 individuals per year) and map this rate by census unit area. Both quantile and equal interval classification schemes of the same data are mapped. auckland$raw.rate &lt;- mor / pop * 1000 brks1 &lt;- classIntervals(auckland$raw.rate, n = 6, style = &quot;equal&quot;) brks2 &lt;- classIntervals(auckland$raw.rate, n = 6, style = &quot;quantile&quot;) plot(auckland[&quot;raw.rate&quot;], breaks = brks1$brks, pal = pal1, at = round(brks1$brks,2), main = &quot;Equal interval breaks&quot;, key.pos = 4, las = 1) plot(auckland[&quot;raw.rate&quot;], breaks = brks2$brks, pal = pal1, at = round(brks2$brks,2), main = &quot;Quantile breaks&quot;, key.pos = 4, las = 1) Note how our perception of the distribution of infant deaths changes when looking at mapped raw rates vs. counts. A north-south trend in perceived “abnormal” infant deaths is no longer apparent in this map. Standardized mortality ratios (relative risk) Another way to re-express the data is to map the Standardized Mortality Ratios (SMR)-a very popular form of representation in the field of epidemiology. Such maps map the ratios of the number of deaths to an expected death count. There are many ways to define an expected death count, many of which can be externally specified. In the following example, the expected death count \\(E_i\\) is estimated by multiplying the under 5 population count for each area by the overall death rate for Auckland: \\[E_i = {n_i}\\times{mortality_{Auckland} } \\] where \\(n_i\\) is the population count within census unit area \\(i\\) and \\(mortality_{Auckland}\\) is the overall death rate computed from \\(mortality_{Auckland} = \\sum_{i=1}^j O_i / \\sum_{i=1}^j n_i\\) where \\(O_i\\) is the observed death count for census unit \\(i\\). This chunk of code replicates Bailey and Gatrell’s figure 8.1 with the one exception that the color scheme is reversed (Bailey and Gatrell assign lighter hues to higher numbers). auck.rate &lt;- sum(mor) / sum(pop) mor.exp &lt;- pop * auck.rate # Expected count over a nine year period auckland$rel.rate &lt;- 100 * mor / mor.exp brks &lt;- classIntervals(auckland$rel.rate, n = 6, style = &quot;fixed&quot;, fixedBreaks = c(0,47, 83, 118, 154, 190, 704)) plot(auckland[&quot;rel.rate&quot;], breaks = brks$brks, at = brks$brks, pal = pal1, key.pos = 4, las = 1) Dykes and Unwin’s chi-square statistic Dykes and Unwin (Dykes and Unwin 2001) propose a similar technique whereby the rates are standardized following: \\[\\frac{O_i - E_i}{\\sqrt{E_i}} \\] This has the effect of creating a distribution of values closer to normal (as opposed to a Poisson distribution of rates and counts encountered thus far). We can therefore apply a diverging color scheme where green hues represent less than expected rates and red hues represent greater than expected rates. auckland$chi.squ = (mor - mor.exp) / sqrt(mor.exp) brks &lt;- classIntervals(auckland$chi.squ, n = 6, style = &quot;fixed&quot;, fixedBreaks = c(-5,-3, -1, -2, 0, 1, 2, 3, 5)) plot(auckland[&quot;chi.squ&quot;], breaks = brks$brks, at = brks$brks, pal=rev(pal2), key.pos = 4, las = 1) Unstable ratios One problem with the various techniques used thus far is their sensitivity (hence instability) to small underlying population counts (i.e. unstable ratios). This next chunk of code maps the under 5 population count by census area unit. brks &lt;- classIntervals(auckland$Und5_81, n = 6, style = &quot;equal&quot;) plot(auckland[&quot;Und5_81&quot;], breaks = brks$brks, at = brks$brks, pal = pal1, key.pos = 4, las = 1) Note the variability in population count with some areas encompassing fewer than 50 infants. If there is just one death in that census unit, the death rate would be reported as \\(1/50 * 1000\\) or 20 per thousand infants–far more than then the 2.63 per thousand rate for our Auckland data set. Interestingly, the three highest raw rates in Auckland (14.2450142, 18.5185185, 10.5820106 deaths per 1000) are associated with some of the smallest underlying population counts (39, 6, 21 infants under 5). One approach to circumventing this issue is to generate a probability map of the data. The next section highlights such an example. Global Empirical Bayes (EB) rate estimate The idea behind Bayesian approach is to compare the value in some area \\(i\\) to some a priori estimate of the value and to “stabilize” the values due to unstable ratios (e.g. where area populations are small). The a priori estimate can be based on some global mean. An example of the use on a global EB infant mortality rate map is shown below. The EB map is shown side-by-side with the raw rates map for comparison. aka Global moment estimator of infant mortality per 1000 per year EB.est &lt;- EBest(auckland$M77_85, auckland$Und5_81 * 9 ) auckland$EBest &lt;- EB.est$estmm * 1000 brks1 &lt;- classIntervals(auckland$EBest, n = 10, style = &quot;quantile&quot;) brks2 &lt;- classIntervals(auckland$raw.rate, n = 10, style = &quot;quantile&quot;) plot(auckland[&quot;EBest&quot;], breaks = brks1$brks, at = round(brks1$brks, 2), pal = pal3, main=&quot;EB rates&quot;, key.pos = 4, las = 1) plot(auckland[&quot;raw.rate&quot;], breaks = brks2$brks, at = round(brks2$brks, 2), pal = pal3, main=&quot;Raw Rates&quot;, key.pos = 4, las = 1) The census units with the top 10% rates are highlighted in red. Unstable rates (i.e. those associated with smaller population counts) are assigned lower weights to reduce their “prominence” in the mapped data. Notice how the three high raw rates highlighted in the last section are reduced from 14.2450142, 18.5185185, 10.5820106 counts per thousand to 3.6610133, 2.8672132, 3.0283279 counts per thousand. The “remapping” of these values along with others can be shown on the following plot: Local Empirical Bayes (EB) rate estimate The a priori mean and variance need not be aspatial (i.e. the prior distribution being the same for the entire Auckland study area). The adjusted estimated rates can be shrunk towards a local mean instead. Such technique is referred to as local empirical Bayes rate estimates. In the following example, we define local as consisting of all first order adjacent census unit areas. nb &lt;- poly2nb(auckland) EBL.est &lt;- EBlocal(auckland$M77_85, 9*auckland$Und5_81, nb) auckland$EBLest &lt;- EBL.est$est * 1000 brks1 &lt;- classIntervals(auckland$EBLest, n = 10, style = &quot;quantile&quot;) brks2 &lt;- classIntervals(auckland$raw.rate, n = 10, style = &quot;quantile&quot;) plot(auckland[&quot;EBLest&quot;], breaks = brks1$brks, at = round(brks1$brks,2), pal = pal3, main = &quot;Local EB rates&quot;, key.pos = 4, las = 1) plot(auckland[&quot;raw.rate&quot;], breaks = brks2$brks, at = round(brks2$brks,2), pal = pal3, main = &quot;Raw Rates&quot;, key.pos = 4, las = 1) The census units with the top 10% rates are highlighted in red. References Bailey, Trevor C., and Anthony C. Gatrell. 1995. Interactive Spatial Data Analysis. England: Prentice Hall. Dykes, J. A., and D. J. Unwin. 2001. “Maps of the Census: A Rough Guide.” In Case Studies of Visualization in the Social Sciences: Technical Report 43 (43): 29–54. http://www.agocg.ac.uk/reports/visual/casestud/dykes/dykes.pdf. "],["raster-operations-in-r.html", "F Raster operations in R Sample files for this exercise Local operations and functions Focal operations and functions Zonal operations and functions Global operations and functions Computing cumulative distances", " F Raster operations in R R terra sf tmap gdistance ggplot2 rasterVis 4.5.1 1.8.60 1.0.21 4.1 1.6.4 3.5.2 0.51.7 Sample files for this exercise We’ll first load spatial objects used in this exercise from a remote website: an elevation SpatRaster object, a bathymetry SpatRaster object and a continents sf vector object library(terra) library(sf) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/elev_world.RDS&quot;)) elev &lt;- unwrap(readRDS(z)) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/bath_world.RDS&quot;)) bath &lt;- unwrap(readRDS(z)) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/continent_global.RDS&quot;)) cont &lt;- readRDS(z) Both rasters cover the entire globe. Elevation below mean sea level are encoded as 0 in the elevation raster. Likewise, bathymetry values above mean sea level are encoded as 0. Note that most of the map algebra operations and functions covered in this tutorial are implemented using the terra package. See chapter 10 for a theoretical discussion of map algebra operations. Local operations and functions Unary operations and functions (applied to single rasters) Most algebraic operations can be applied to rasters as they would with any vector element. For example, to convert all bathymetric values in bath (currently recorded as positive values) to negative values simply multiply the raster by -1. bath2 &lt;- bath * (-1) Another unary operation that can be applied to a raster is reclassification. In the following example, we will assign all bath2 values that are less than zero a 1 and all zero values will remain unchanged. A simple way to do this is to apply a conditional statement. bath3 &lt;- bath2 &lt; 0 Let’s look at the output. Note that all 0 pixels are coded as FALSE and all 1 pixels are coded as TRUE. library(tmap) tm_shape(bath3) + tm_raster(col.scale = tm_scale(values=&quot;brewer.greys&quot;), col.legend = tm_legend(position = tm_pos_out())) If a more elaborate form of reclassification is desired, you can use the classify function. In the following example, the raster object bath is reclassified to 4 unique values: 100, 500, 1000 and 11000 as follows: Original depth values Reclassified values 0 - 100 100 101 - 500 500 501 - 1000 1000 1001 - 11000 11000 The first step is to create a plain matrix where the first and second columns list the starting and ending values of the range of input values that are to be reclassified, and where the third column lists the new raster cell values. m &lt;- c(0, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000, 11000, 11000) m &lt;- matrix(m, ncol=3, byrow = T) m [,1] [,2] [,3] [1,] 0 100 100 [2,] 100 500 500 [3,] 500 1000 1000 [4,] 1000 11000 11000 bath3 &lt;- classify(bath, m, right = T) The right=T parameter indicates that the intervals should be closed to the right (i.e. the second column of the reclassification matrix is inclusive). tm_shape(bath3) + tm_raster(col.scale = tm_scale_categorical(values = &quot;seq&quot;), col.legend = tm_legend(position = tm_pos_out())) You can also assign NA (missing) values to pixels. For example, to assign NA values to cells that are equal to 100, type bath3[bath3 == 100] &lt;- NA The following chunk of code highlights all NA pixels in grey and labels them as missing. tm_shape(bath3) + tm_raster(col.scale = tm_scale_categorical(values = &quot;seq&quot;, value.na = &quot;grey&quot;), col.legend = tm_legend(position = tm_pos_out(), na.show = TRUE)) Binary operations and functions (where two rasters are used) In the following example, elev (elevation raster) is added to bath (bathymetry raster) to create a single elevation raster for the globe. Note that the bathymetric raster will need to be multiplied by -1 to differentiate above mean sea level elevation from below mean sea level depth. elevation &lt;- elev - bath tm_shape(elevation) + tm_raster(col.scale = tm_scale_intervals(values = &quot;-brewer.rd_bu&quot;, midpoint = 0), col.legend = tm_legend(position = tm_pos_out(), na.show = TRUE)) Focal operations and functions Operations or functions applied focally to rasters involve user defined neighboring cells. Focal operations can be performed using the focal function. For example, to smooth out the elevation raster by computing the mean cell values over a 11 by 11 cells window, type: f1 &lt;- focal(elevation, w = 11 , fun = mean) The w argument defines the focal window. If it’s given a single number (as is the case in the above code chunk), that number will define the width and height (in cell counts) of the focal window with each cell assigned equal weight. w can also be passed a matrix with each element in that matrix defining the weight for each cell. The following code chunk will generate the same output as the previous code chunk: f1 &lt;- focal(elevation, w = matrix(1, nrow = 11, ncol = 11), fun = mean) tm_shape(f1) + tm_raster(col.scale = tm_scale_intervals(values = &quot;-brewer.rd_bu&quot;, midpoint = 0), col.legend = tm_legend(position = tm_pos_out(), na.show = TRUE)) By default edge cells are assigned a value of NA. This is because cells outside of the input raster extent have no value, so when the average value is computed for a cell along the raster’s edge, the kernel will include the NA values outside the raster’s extent. To see an example of this, we will first smooth the raster using a 3 by 3 focal window, then we’ll zoom in on a 3 by 3 portion of the elevation raster in the above left-hand corner of its extent. # Run a 3x3 smooth on the raster f_mean &lt;- focal(elevation, w = 3 , fun = mean) Figure F.1: Upper left-hand corner of elevation raster Note the NA values in the upper row (shown in bisque color). You might have noticed the lack of edge effect issues along the western edge of the raster outputs. This is because the focal function will wrap the eastern edge of the raster to the western edge of that same raster if the input raster layer spans the entire globe (i.e from -180 ° to +180 °). To have the focal function ignore missing values, simply add the na.rm = TRUE option. # Run a 3x3 smooth on the raster f_mean_no_na &lt;- focal(elevation, w = 3 , fun = mean, na.rm = TRUE) Figure F.2: Upper left-hand corner of elevation raster. Border edge ignored. In essence, the above row of values are computed using just 6 values instead of 9 values (the corner values still make use of the across-180° values). Another option is to expand the row edge beyond its extent by replicating the edge values. This can be done by setting exapnd to true. For example: # Run a 3x3 smooth on the raster f_mean_expand &lt;- focal(elevation, w = 3, fun = mean, expand = TRUE) Figure F.3: Upper left-hand corner of elevation raster Note that if expand is set to TRUE, the na.rm argument is ignored. But, you must be careful in making use of the na.rm = TRUE if you are using a matrix to define the weights as opposed to using the fun functions. For example, the mean function can be replicated using the matrix operation as follows: f_mean &lt;- focal(elevation, w = 3, fun = mean) f_mat &lt;- focal(elevation, w = matrix(1/9, nrow = 3, ncol = 3)) Note that if fun is not defined, it will default to summing the weighted pixel values. Figure F.4: Upper left-hand corner of elevation raster Note the similar output. Now, if we set na.rm to TRUE to both functions, we get: f_mean &lt;- focal(elevation, w = 3, fun = mean, na.rm = TRUE) f_mat &lt;- focal(elevation, w = matrix(1/9, nrow = 3, ncol = 3), na.rm = TRUE) Figure F.5: Upper left-hand corner of elevation raster Note the smaller edge values from the matrix defined weights raster. This is because the matrix is assigning 1/9th the weight for each pixel regardless of the number of pixels used to compute the output pixel values. So the upper edge pixels are summing values from just 6 weighted pixels as opposed to eight. For example, the middle top pixel is computed from 1/9(-4113 -4113 -4112 -4107 -4104 -4103), which results in dividing the sum of six values by nine–hence the unbalanced weight effect. Note that we do not have that problem using the mean function. The neighbors matrix (or kernel) that defines the moving window can be customized. For example if we wanted to compute the average of all 8 neighboring cells excluding the central cell we could define the matrix as follows: m &lt;- matrix(c(1,1,1,1,0,1,1,1,1)/8,nrow = 3) f2 &lt;- focal(elevation, w=m, fun=sum) More complicated kernels can be defined. In the following example, a Sobel filter (used for edge detection in image processing) is defined then applied to the raster layer elevation. Sobel &lt;- matrix(c(-1,0,1,-2,0,2,-1,0,1) / 4, nrow=3) f3 &lt;- focal(elevation, w=Sobel, fun=sum) tm_shape(f3) + tm_raster(col.scale = tm_scale_continuous(values = &quot;brewer.greys&quot;, midpoint = 0), col.legend = tm_legend(show = FALSE)) Zonal operations and functions A common zonal operation is the aggregation of cells. In the following example, raster layer elevation is aggregated to a 5x5 raster layer. z1 &lt;- aggregate(elevation, fact=2, fun=mean, expand=TRUE) tm_shape(z1) + tm_raster(col.scale = tm_scale_intervals(values = &quot;-brewer.rd_bu&quot;, midpoint = 0, n = 6), col.legend = tm_legend(position = tm_pos_out(), na.show = TRUE)) The image may not look much different from the original, but a look at the image properties will show a difference in pixel sizes. res(elevation) [1] 0.3333333 0.3333333 res(z1) [1] 0.6666667 0.6666667 z1’s pixel dimensions are half of elevation’s dimensions. You can reverse the process by using the disaggregate function which will split a cell into the desired number of subcells while assigning each one the same parent cell value. Zonal operations can often involve two layers, one with the values to be aggregated, the other with the defined zones. In the next example, elevation’s cell values are averaged by zones defined by the cont polygon layer. The following chunk computes the mean elevation value for each unique polygon in cont, cont.elev &lt;- extract(elevation, cont, fun=mean, bind = TRUE) The output is a SpatVector. If you want to output a dataframe, set bind to FALSE. cont.elev can be converted back to an sf object as follows: cont.elev.sf &lt;- st_as_sf(cont.elev) The column of interest is automatically named band1. We can now map the average elevation by continent. tm_shape(cont.elev.sf) + tm_polygons(fill=&quot;band1&quot;, fill.scale = tm_scale_intervals(values = &quot;brewer.reds&quot;, midpoint = NA), fill.legend = tm_legend(position = tm_pos_out())) Many custom functions can be applied to extract. For example, to extract the maximum elevation value by continent, type: cont.elev &lt;- extract(elevation, cont, fun=max, bind = TRUE) As another example, we may wish to extract the number of pixels in each polygon using a customized function. cont.elev &lt;- extract(elevation, cont, fun=function(x,...){length(x)}, bind = TRUE) Global operations and functions Global operations and functions may make use of all input cells of a grid in the computation of an output cell value. An example of a global function is the Euclidean distance function, distance, which computes the shortest distance between a pixel and a source (or destination) location. To demonstrate the distance function, we’ll first create a new raster layer with two non-NA pixels. r1 &lt;- rast(ncols=100, nrows=100, xmin=0, xmax=100, ymin=0, ymax=100) r1[] &lt;- NA # Assign NoData values to all pixels r1[c(850, 5650)] &lt;- 1 # Change the pixels #850 and #5650 to 1 crs(r1) &lt;- &quot;+proj=ortho&quot; # Assign an arbitrary coordinate system (needed for mapping with tmap) tm_shape(r1) + tm_raster(col.scale = tm_scale_discrete(values = &quot;reds&quot;), col.legend = tm_legend(show = FALSE)) Next, we’ll compute a Euclidean distance raster from these two cells. The output extent will default to the input raster extent. r1.d &lt;- distance(r1) tm_shape(r1.d) + tm_raster(col.scale = tm_scale_continuous(values = &quot;brewer.greens&quot;), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Distance&quot;)) + tm_shape(r1) + tm_raster(col.scale = tm_scale_discrete(values = &quot;reds&quot;), col.legend = tm_legend(show = FALSE)) You can also compute a distance raster using sf point objects. In the following example, distances to points (25,30) and (87,80) are computed for each output cell. However, since we are working off of point objects (and not an existing raster as was the case in the previous example), we will need to create a blank raster layer which will define the extent of the Euclidean distance raster output. r2 &lt;- rast(ncols=100, nrows=100, xmin=0, xmax=100, ymin=0, ymax=100) crs(r2) &lt;- &quot;+proj=ortho&quot; # Assign an arbitrary coordinate system # Create a point layer p1 &lt;- st_as_sf(st_as_sfc(&quot;MULTIPOINT(25 30, 87 80)&quot;, crs = &quot;+proj=ortho&quot;)) Now let’s compute the Euclidean distance to these points using the distance function. r2.d &lt;- distance(r2, p1) Let’s plot the resulting output. tm_shape(r2.d) + tm_raster(col.scale = tm_scale_continuous(values = &quot;brewer.greens&quot;), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Distance&quot;)) + tm_shape(p1) + tm_dots(fill = &quot;red&quot;) Computing cumulative distances This exercise demonstrates how to use functions from the gdistance package to generate a cumulative distance raster. One objective will be to demonstrate the influence “adjacency cells” wields in the final results. Load the gdistance package. library(gdistance) First, we’ll create a 100x100 raster and assign a value of 1 to each cell. The pixel value defines the cost (other than distance) in traversing that pixel. In this example, we’ll assume that the cost is uniform across the entire extent. r &lt;- rast(nrows=100,ncols=100,xmin=0,ymin=0,xmax=100,ymax=100) r[] &lt;- rep(1, ncell(r)) If you were to include traveling costs other than distance (such as elevation) you would assign those values to each cell instead of the constant value of 1. A translation matrix allows one to define a ‘traversing’ cost going from one cell to an adjacent cell. Since we are assuming there are no ‘costs’ (other than distance) in traversing from one cell to any adjacent cell we’ll assign a value of 1, function(x){1}, to the translation between a cell and its adjacent cells (i.e. translation cost is uniform in all directions). There are four different ways in which ‘adjacency’ can be defined using the transition function. These are showcased in the next four blocks of code. In this example, adjacency is defined as a four node (vertical and horizontal) connection (i.e. a “rook” move). h4 &lt;- transition(raster(r), transitionFunction = function(x){1}, directions = 4) In this example, adjacency is defined as an eight node connection (i.e. a single cell “queen” move). h8 &lt;- transition(raster(r), transitionFunction = function(x){1}, directions = 8) In this example, adjacency is defined as a sixteen node connection (i.e. a single cell “queen” move combined with a “knight” move). h16 &lt;- transition(raster(r), transitionFunction=function(x){1},16,symm=FALSE) In this example, adjacency is defined as a four node diagonal connection (i.e. a single cell “bishop” move). hb &lt;- transition(raster(r), transitionFunction=function(x){1},&quot;bishop&quot;,symm=FALSE) The transition function treats all adjacent cells as being at an equal distance from the source cell across the entire raster. geoCorrection corrects for ‘true’ local distance. In essence, it’s adding an additional cost to traversing from one cell to an adjacent cell (the original cost being defined using the transition function). The importance of applying this correction will be shown later. Note: geoCorrection also corrects for distance distortions associated with data in a geographic coordinate system. To take advantage of this correction, make sure to define the raster layer’s coordinate system using the projection function. h4 &lt;- geoCorrection(h4, scl=FALSE) h8 &lt;- geoCorrection(h8, scl=FALSE) h16 &lt;- geoCorrection(h16, scl=FALSE) hb &lt;- geoCorrection(hb, scl=FALSE) In the “queen’s” case, the diagonal neighbors are \\(\\sqrt{2 x (CellWidth)^{2}}\\) times the cell width distance from the source cell. Next we will map the cumulative distance (accCost) from a central point (A) to all cells in the raster using the four different adjacency definitions. A &lt;- c(50,50) # Location of source cell h4.acc &lt;- accCost(h4,A) h8.acc &lt;- accCost(h8,A) h16.acc &lt;- accCost(h16,A) hb.acc &lt;- accCost(hb,A) If the geoCorrection function had not been applied in the previous steps, the cumulative distance between point location A and its neighboring adjacent cells would have been different. Note the difference in cumulative distance for the 16-direction case as shown in the next two figures. Uncorrected (i.e. geoCorrection not applied to h16): Corrected (i.e. geoCorrection applied to h16): The “bishop” case offers a unique problem: only cells in the diagonal direction are identified as being adjacent. This leaves many undefined cells (labeled as Inf). We will change the Inf cells to NA cells. hb.acc[hb.acc == Inf] &lt;- NA Now let’s compare a 7x7 subset (centered on point A) between the four different cumulative distance rasters. To highlight the differences between all four rasters, we will assign a red color to all cells that are within 20 cell units of point A. It’s obvious that the accuracy of the cumulative distance raster can be greatly influenced by how we define adjacent nodes. The number of red cells (i.e. area identified as being within a 20 units cumulative distance) ranges from 925 to 2749 cells. Working example In the following example, we will generate a raster layer with barriers (defined as NA cell values). The goal will be to identify all cells that fall within a 290 km traveling distance from the upper left-hand corner of the raster layer (the green point in the maps). Results between an 8-node and 16-node adjacency definition will be compared. # create an empty raster r &lt;- rast(nrows=300,ncols=150,xmin=0,ymin=0,xmax=150000, ymax=300000) # Define a UTM projection (this sets map units to meters) crs(r) = &quot;+proj=utm +zone=19 +datum=NAD83&quot; # Each cell is assigned a value of 1 r[] &lt;- rep(1, ncell(r)) # Generate &#39;baffles&#39; by assigning NA to cells. Cells are identified by # their index and not their coordinates. # Baffles need to be 2 cells thick to prevent the 16-node # case from &quot;jumping&quot; a one pixel thick NA cell. a &lt;- c(seq(3001,3100,1),seq(3151,3250,1)) a &lt;- c(a, a+6000, a+12000, a+18000, a+24000, a+30000, a+36000) a &lt;- c(a , a+3050) r[a] &lt;- NA # Let&#39;s check that the baffles are properly placed tm_shape(r) + tm_raster(colorNA=&quot;red&quot;) + tm_legend(legend.show=FALSE) # Next, generate a transition matrix for the 8-node case and the 16-node case h8 &lt;- transition(raster(r), transitionFunction = function(x){1}, directions = 8) h16 &lt;- transition(raster(r), transitionFunction = function(x){1}, directions = 16) # Now assign distance cost to the matrices. h8 &lt;- geoCorrection(h8) h16 &lt;- geoCorrection(h16) # Define a point source and assign a projection A &lt;- SpatialPoints(cbind(50,290000)) #crs(A) &lt;- &quot;+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs&quot; # Compute the cumulative cost raster h8.acc &lt;- accCost(h8, A) h16.acc &lt;- accCost(h16,A) # Replace Inf with NA h8.acc[h8.acc == Inf] &lt;- NA h16.acc[h16.acc == Inf] &lt;- NA Let’s plot the results. Yellow cells will identify cumulative distances within 290 km. tm_shape(h8.acc) + tm_raster(col.scale = tm_scale(values = &quot;brewer.yl_or_rd&quot;, breaks=c(0,290000,Inf)), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Distance&quot;)) + tm_shape(st_as_sf(A)) + tm_dots(fill = &quot;red&quot;, size = 0.8) tm_shape(h16.acc) + tm_raster(col.scale = tm_scale(values = &quot;brewer.yl_or_rd&quot;, breaks=c(0,290000,Inf)), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Distance&quot;)) + tm_shape(st_as_sf(A)) + tm_dots(fill = &quot;red&quot;, size = 0.8) We can compute the difference between the 8-node and 16-node cumulative distance rasters: table(h8.acc[] &lt;= 290000) FALSE TRUE 31458 10742 table(h16.acc[] &lt;= 290000) FALSE TRUE 30842 11358 The number of cells identified as being within a 290 km cumulative distance of point A for the 8-node case is 10742 whereas it’s 11358 for the 16-node case, a difference of 5.4%. "],["coordinate-systems-in-r.html", "G Coordinate Systems in R A note about the changes to the PROJ environment Sample files for this exercise Loading the sf package Checking for a coordinate system Understanding the Proj4 coordinate syntax Assigning a coordinate system Transforming coordinate systems A note about containment Creating Tissot indicatrix circles", " G Coordinate Systems in R R terra sf tmap geosphere 4.5.1 1.8.60 1.0.21 4.1 1.5.20 A note about the changes to the PROJ environment Newer versions of sf make use of the PROJ 6.0 C library or greater. Note that the version of PROJ is not to be confused with the version of the proj4 R package–the proj4 and sf packages make use of the PROJ C library that is developed independent of R. You can learn more about the PROJ development at proj.org. There has been a significant change in the PROJ library since the introduction of version 6.0. This has had serious implications in the development of the R spatial ecosystem. As such, if you are using an older version of sf or proj4 that was developed with a version of PROJ older than 6.0, some of the input/output presented in this appendix may differ from yours. Sample files for this exercise Data used in this exercise can be loaded into your current R session by running the following chunk of code. library(terra) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/elev.RDS&quot;)) elev.r &lt;- unwrap(readRDS(z)) z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/s_sf.RDS&quot;)) s.sf &lt;- readRDS(z) We’ll make use of two data layers in this exercise: a Maine counties polygon layer (s.sf) and an elevation raster layer (elev.r). The former is in an sf format and the latter is in a SpatRaster format. Loading the sf package library(sf) Note the versions of GEOS, GDAL and PROJ the package sf is linked to. Different versions of these libraries may result in different outcomes than those shown in this appendix. You can check the linked library versions as follows: sf_extSoftVersion()[1:3] GEOS GDAL proj.4 &quot;3.13.1&quot; &quot;3.11.0&quot; &quot;9.6.0&quot; Checking for a coordinate system To extract coordinate system (CS) information from an sf object use the st_crs function. st_crs(s.sf) Coordinate Reference System: User input: EPSG:26919 wkt: PROJCRS[&quot;NAD83 / UTM zone 19N&quot;, BASEGEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1]], USAGE[ SCOPE[&quot;Engineering survey, topographic mapping.&quot;], AREA[&quot;North America - between 72Â°W and 66Â°W - onshore and offshore. Canada - Labrador; New Brunswick; Nova Scotia; Nunavut; Quebec. Puerto Rico. United States (USA) - Connecticut; Maine; Massachusetts; New Hampshire; New York (Long Island); Rhode Island; Vermont.&quot;], BBOX[14.92,-72,84,-66]], ID[&quot;EPSG&quot;,26919]] With the newer version of the PROJ C library, the coordinate system is defined using the Well Known Text (WTK/WTK2) format which consists of a series of [...] tags. The WKT format will usually start with a PROJCRS[...] tag for a projected coordinate system, or a GEOGCRS[...] tag for a geographic coordinate system. The CRS output will also consist of a user defined CS definition which can be an EPSG code (as is the case in this example), or a string defining the datum and projection type. You can also extract CS information from a SpatRaster object use the st_crs function. st_crs(elev.r) Coordinate Reference System: User input: BOUNDCRS[ SOURCECRS[ PROJCRS[&quot;unknown&quot;, BASEGEOGCRS[&quot;unknown&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6269]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]], ID[&quot;EPSG&quot;,16019]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]]]], TARGETCRS[ GEOGCRS[&quot;WGS 84&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4326]]], ABRIDGEDTRANSFORMATION[&quot;Transformation from unknown to WGS84&quot;, METHOD[&quot;Geocentric translations (geog2D domain)&quot;, ID[&quot;EPSG&quot;,9603]], PARAMETER[&quot;X-axis translation&quot;,0, ID[&quot;EPSG&quot;,8605]], PARAMETER[&quot;Y-axis translation&quot;,0, ID[&quot;EPSG&quot;,8606]], PARAMETER[&quot;Z-axis translation&quot;,0, ID[&quot;EPSG&quot;,8607]]]] wkt: BOUNDCRS[ SOURCECRS[ PROJCRS[&quot;unknown&quot;, BASEGEOGCRS[&quot;unknown&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6269]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]], ID[&quot;EPSG&quot;,16019]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]]]], TARGETCRS[ GEOGCRS[&quot;WGS 84&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4326]]], ABRIDGEDTRANSFORMATION[&quot;Transformation from unknown to WGS84&quot;, METHOD[&quot;Geocentric translations (geog2D domain)&quot;, ID[&quot;EPSG&quot;,9603]], PARAMETER[&quot;X-axis translation&quot;,0, ID[&quot;EPSG&quot;,8605]], PARAMETER[&quot;Y-axis translation&quot;,0, ID[&quot;EPSG&quot;,8606]], PARAMETER[&quot;Z-axis translation&quot;,0, ID[&quot;EPSG&quot;,8607]]]] Up until recently, there has been two ways of defining a coordinate system: via the EPSG numeric code or via the PROJ4 formatted string. Both can be used with the sf and SpatRast objects. With the newer version of the PROJ C library, you can also define an sf object’s coordinate system using the Well Known Text (WTK/WTK2) format. This format has a more elaborate syntax (as can be seen in the previous outputs) and may not necessarily be the easiest way to manually define a CS. When possible, adopt an EPSG code which comes from a well established authority. However, if customizing a CS, it may be easiest to adopt a PROJ4 syntax. Understanding the Proj4 coordinate syntax The PROJ4 syntax consists of a list of parameters, each prefixed with the + character. For example, elev.r’s CS is in a UTM projection (+proj=utm) for zone 19 (+zone=19) and in an NAD 1983 datum (+datum=NAD83). A list of a few of the PROJ4 parameters used in defining a coordinate system follows. Click here for a full list of parameters. +a Semimajor radius of the ellipsoid axis +b Semiminor radius of the ellipsoid axis +datum Datum name +ellps Ellipsoid name +lat_0 Latitude of origin +lat_1 Latitude of first standard parallel +lat_2 Latitude of second standard parallel +lat_ts Latitude of true scale +lon_0 Central meridian +over Allow longitude output outside -180 to 180 range, disables wrapping +proj Projection name +south Denotes southern hemisphere UTM zone +units meters, US survey feet, etc. +x_0 False easting +y_0 False northing +zone UTM zone You can view the list of available projections +proj= here. Assigning a coordinate system A coordinate system definition can be passed to a spatial object. It can either fill a spatial object’s empty CS definition or it can overwrite its existing CS definition (the latter should only be executed if there is good reason to believe that the original definition is erroneous). Note that this step does not change an object’s underlying coordinate values (this process will be discussed in the next section). We’ll pretend that a CS definition was not assigned to s.sf and assign one manually using the st_set_crs() function. In the following example, we will define the CS using the proj4 syntax. s.sf &lt;- st_set_crs(s.sf, &quot;+proj=utm +zone=19 +ellps=GRS80 +datum=NAD83&quot;) Let’s now check the object’s CS. st_crs(s.sf) Coordinate Reference System: User input: +proj=utm +zone=19 +ellps=GRS80 +datum=NAD83 wkt: PROJCRS[&quot;unknown&quot;, BASEGEOGCRS[&quot;unknown&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6269]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]], ID[&quot;EPSG&quot;,16019]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]]] You’ll note that the User input: field now shows the proj4 string as defined in our call to the st_set_crs() function. But you’ll also note that some of the parameters in the WKT string such as the PROJCRS[...] and BASEGEOGCRS[...] tags are not defined (unknown). This is not necessarily a problem given that key datum and projection information are present in that WKT string (make sure to scroll down in the output box to see the other WKT parameters). Nonetheless, it’s not a bad idea to define the CS using EPSG code when one is available. We’ll do this next. The UTM NAD83 Zone 19N EPSG code equivalent is 26919. s.sf &lt;- st_set_crs(s.sf, 26919) Let’s now check the object’s CS. st_crs(s.sf) Coordinate Reference System: User input: EPSG:26919 wkt: PROJCRS[&quot;NAD83 / UTM zone 19N&quot;, BASEGEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1]], USAGE[ SCOPE[&quot;Engineering survey, topographic mapping.&quot;], AREA[&quot;North America - between 72°W and 66°W - onshore and offshore. Canada - Labrador; New Brunswick; Nova Scotia; Nunavut; Quebec. Puerto Rico. United States (USA) - Connecticut; Maine; Massachusetts; New Hampshire; New York (Long Island); Rhode Island; Vermont.&quot;], BBOX[14.92,-72,84,-66]], ID[&quot;EPSG&quot;,26919]] Key projection parameters remain the same. But additional information is added to the WKT header. You can use the PROJ4 string defined earlier for s.sf to define a raster’s CRS using the crs() function as follows (here too we’ll assume that the spatial object had a missing reference system or an incorrectly defined one). crs(elev.r) &lt;- &quot;+proj=utm +zone=19 +ellps=GRS80 +datum=NAD83&quot; Note that we do not need to define all of the parameters so long as we know that the default values for these undefined parameters are correct. Also note that we do not need to designate a hemisphere since the NAD83 datum applies only to North America. Let’s check the raster’s CS: st_crs(elev.r) Coordinate Reference System: User input: PROJCRS[&quot;unknown&quot;, BASEGEOGCRS[&quot;unknown&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6269]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]], ID[&quot;EPSG&quot;,16019]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]]] wkt: PROJCRS[&quot;unknown&quot;, BASEGEOGCRS[&quot;unknown&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6269]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]], ID[&quot;EPSG&quot;,16019]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]]] To define a raster’s CS using an EPSG code, use the following PROJ4 syntax: crs(elev.r) &lt;- &quot;+init=EPSG:26919&quot; st_crs(elev.r) Coordinate Reference System: User input: NAD83 / UTM zone 19N wkt: PROJCRS[&quot;NAD83 / UTM zone 19N&quot;, BASEGEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]], CONVERSION[&quot;UTM zone 19N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-69, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]], ID[&quot;EPSG&quot;,16019]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;North America - between 72°W and 66°W - onshore and offshore. Canada - Labrador; New Brunswick; Nova Scotia; Nunavut; Quebec. Puerto Rico. United States (USA) - Connecticut; Maine; Massachusetts; New Hampshire; New York (Long Island); Rhode Island; Vermont.&quot;], BBOX[14.92,-72,84,-66]]] To recreate a CS defined in a software such as ArcGIS, it is best to extract the CS’ WKID/EPSG code, then use that number to look up the PROJ4 syntax on http://spatialreference.org/ref/. For example, in ArcGIS, the WKID number can be extracted from the coordinate system properties output. Figure G.1: An ArcGIS dataframe coordinate system properties window. Note the WKID/EPSG code of 26919 (highlighted in red) associated with the NAD 1983 UTM Zone 19 N CS. That number can then be entered in the http://spatialreference.org/ref/’s search box to pull the Proj4 parameters (note that you must select Proj4 from the list of syntax options). Figure G.2: Example of a search result for EPSG 26919 at http://spatialreference.org/ref/. Note that after clicking the EPSG:269191 link, you must then select the Proj4 syntax from a list of available syntaxes to view the projection parameters Here are examples of a few common projections: Projection WKID Authority Syntax UTM NAD 83 Zone 19N 26919 EPSG +proj=utm +zone=19 +ellps=GRS80 +datum=NAD83 +units=m +no_defs USA Contiguous albers equal area 102003 ESRI +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs Alaska albers equal area 3338 EPSG +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs World Robinson 54030 ESRI +proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs Transforming coordinate systems The last section showed you how to define or modify the coordinate system definition. This section shows you how to transform the coordinate values associated with the spatial object to a different coordinate system. This process calculates new coordinate values for the points or vertices defining the spatial object. For example, to transform the s.sf vector object to a WGS 1984 geographic (long/lat) coordinate system, we’ll use the st_transform function. s.sf.gcs &lt;- st_transform(s.sf, &quot;+proj=longlat +datum=WGS84&quot;) st_crs(s.sf.gcs) Coordinate Reference System: User input: +proj=longlat +datum=WGS84 wkt: GEOGCRS[&quot;unknown&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6326]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]], CS[ellipsoidal,2], AXIS[&quot;longitude&quot;,east, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]], AXIS[&quot;latitude&quot;,north, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]]] Using the EPSG code equivalent (4326) instead of the proj4 string yields: s.sf.gcs &lt;- st_transform(s.sf, 4326) st_crs(s.sf.gcs) Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCRS[&quot;WGS 84&quot;, ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;], ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ENSEMBLEACCURACY[2.0]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], USAGE[ SCOPE[&quot;Horizontal component of 3D system.&quot;], AREA[&quot;World.&quot;], BBOX[-90,-180,90,180]], ID[&quot;EPSG&quot;,4326]] This approach may add a few more tags (These reflect changes in datum definitions in newer versions of the PROJ library) but, the coordinate values should be the same To transform a raster object, use the project() function. elev.r.gcs &lt;- project(elev.r, y=&quot;+proj=longlat +datum=WGS84&quot;) st_crs(elev.r.gcs) Coordinate Reference System: User input: GEOGCRS[&quot;unknown&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6326]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]], CS[ellipsoidal,2], AXIS[&quot;longitude&quot;,east, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]], AXIS[&quot;latitude&quot;,north, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]]] wkt: GEOGCRS[&quot;unknown&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6326]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]], CS[ellipsoidal,2], AXIS[&quot;longitude&quot;,east, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]], AXIS[&quot;latitude&quot;,north, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]]] If an EPSG code is to be used, adopt the \"+init=EPSG: ...\" syntax used earlier in this tutorial. elev.r.gcs &lt;- project(elev.r, y=&quot;+init=EPSG:4326&quot;) st_crs(elev.r.gcs) Coordinate Reference System: User input: WGS 84 wkt: GEOGCRS[&quot;WGS 84&quot;, ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;, ID[&quot;EPSG&quot;,1166]], MEMBER[&quot;World Geodetic System 1984 (G730)&quot;, ID[&quot;EPSG&quot;,1152]], MEMBER[&quot;World Geodetic System 1984 (G873)&quot;, ID[&quot;EPSG&quot;,1153]], MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;, ID[&quot;EPSG&quot;,1154]], MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;, ID[&quot;EPSG&quot;,1155]], MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;, ID[&quot;EPSG&quot;,1156]], MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;, ID[&quot;EPSG&quot;,1309]], MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;, ID[&quot;EPSG&quot;,1383]], ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,7030]], ENSEMBLEACCURACY[2.0], ID[&quot;EPSG&quot;,6326]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]], CS[ellipsoidal,2], AXIS[&quot;longitude&quot;,east, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]], AXIS[&quot;latitude&quot;,north, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433, ID[&quot;EPSG&quot;,9122]]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;World.&quot;], BBOX[-90,-180,90,180]]] A geographic coordinate system is often desired when overlapping a layer with a web based mapping service such as Google, Bing or OpenStreetMap (even though these web based services end up projecting to a projected coordinate system–most likely a Web Mercator projection). To check that s.sf.gcs was properly transformed, we’ll overlay it on top of an OpenStreetMap using the leaflet package. library(leaflet) leaflet(s.sf.gcs) %&gt;% addPolygons() %&gt;% addTiles() Next, we’ll explore other transformations using a tmap dataset of the world library(tmap) data(World) # The dataset is stored as an sf object # Let&#39;s check its current coordinate system st_crs(World) Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCRS[&quot;WGS 84&quot;, ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ENSEMBLEACCURACY[2.0]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], USAGE[ SCOPE[&quot;Horizontal component of 3D system.&quot;], AREA[&quot;World.&quot;], BBOX[-90,-180,90,180]], ID[&quot;EPSG&quot;,4326]] The following chunk transforms the world map to a custom azimuthal equidistant projection centered on latitude 0 and longitude 0. Here, we’ll use the proj4 syntax. World.ae &lt;- st_transform(World, &quot;+proj=aeqd +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) Let’s check the CRS of the newly created vector layer st_crs(World.ae) Coordinate Reference System: User input: +proj=aeqd +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs wkt: PROJCRS[&quot;unknown&quot;, BASEGEOGCRS[&quot;unknown&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ID[&quot;EPSG&quot;,6326]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8901]]], CONVERSION[&quot;unknown&quot;, METHOD[&quot;Azimuthal Equidistant&quot;, ID[&quot;EPSG&quot;,1125]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;False easting&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1, ID[&quot;EPSG&quot;,9001]]]] Here’s the mapped output: tm_shape(World.ae) + tm_fill() The following chunk transforms the world map to an Azimuthal equidistant projection centered on Maine, USA (69.8° West, 44.5° North) . World.aemaine &lt;- st_transform(World, &quot;+proj=aeqd +lat_0=44.5 +lon_0=-69.8 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) tm_shape(World.aemaine) + tm_fill() The following chunk transforms the world map to a World Robinson projection. World.robin &lt;- st_transform(World,&quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) tm_shape(World.robin) + tm_fill() The following chunk transforms the world map to a World sinusoidal projection. World.sin &lt;- st_transform(World,&quot;+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) tm_shape(World.sin) + tm_fill() The following chunk transforms the world map to a World Mercator projection. World.mercator &lt;- st_transform(World,&quot;+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) tm_shape(World.mercator) + tm_fill() Reprojecting to a new meridian center An issue that can come up when transforming spatial data is when the location of the tangent line(s) or points in the CS definition forces polygon features to be split across the 180° meridian. For example, re-centering the Mercator projection to -69° will create the following output. World.mercator2 &lt;- st_transform(World, &quot;+proj=merc +lon_0=-69 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) tm_shape(World.mercator2) + tm_borders() The polygons are split and R does not know how to piece them together. One solution is to split the polygons at the new meridian using the st_break_antimeridian function before projecting to a new re-centered coordinate system. # Define new meridian meridian2 &lt;- -69 # Split world at new meridian wld.new &lt;- st_break_antimeridian(World, lon_0 = meridian2) # Now reproject to Mercator using new meridian center wld.merc2 &lt;- st_transform(wld.new, paste(&quot;+proj=merc +lon_0=&quot;, meridian2 , &quot;+k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) ) tm_shape(wld.merc2) + tm_borders() This technique can be applied to any other projections. Here’s an example of a Robinson projection. wld.rob.sf &lt;- st_transform(wld.new, paste(&quot;+proj=robin +lon_0=&quot;, meridian2 , &quot;+k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;) ) tm_shape(wld.rob.sf) + tm_borders() A note about containment While in theory, a point completely enclosed by a bounded area should always remain bounded by that area in any projection, this is not always the case in practice. This is because the transformation applies to the vertices that define the line segments and not the lines themselves. So if a point is inside of a polygon and very close to one of its boundaries in its native projection, it may find itself on the other side of that line segment in another projection hence outside of that polygon. In the following example, a polygon layer and point layer are created in a Miller coordinate system where the points are enclosed in the polygons. # Define a few projections miller &lt;- &quot;+proj=mill +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot; lambert &lt;- &quot;+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs&quot; # Subset the World data layer and reproject to Miller wld.mil &lt;- subset(World, iso_a3 == &quot;CAN&quot; | iso_a3 == &quot;USA&quot;) |&gt; st_transform(miller) # Create polygon and point layers in the Miller projection sf1 &lt;- st_sfc( st_polygon(list(cbind(c(-13340256,-13340256,-6661069, -6661069, -13340256), c(7713751, 5326023, 5326023,7713751, 7713751 )))), crs = miller) pt1 &lt;- st_sfc( st_multipoint(rbind(c(-11688500,7633570), c(-11688500,5375780), c(-10018800,7633570), c(-10018800,5375780), c(-8348960,7633570), c(-8348960,5375780))), crs = miller) pt1 &lt;- st_cast(pt1, &quot;POINT&quot;) # Create single part points # Plot the data layers in their native projection tm_shape(wld.mil) +tm_fill(col=&quot;grey&quot;) + tm_graticules(x = c(-60,-80,-100, -120, -140), y = c(30,45, 60), labels.col = &quot;white&quot;, col=&quot;grey90&quot;) + tm_shape(sf1) + tm_polygons(&quot;red&quot;, alpha = 0.5, border.col = &quot;yellow&quot;) + tm_shape(pt1) + tm_dots(size=0.2) The points are close to the boundaries, but they are inside of the polygon nonetheless. To confirm, we can run st_contains on the dataset: st_contains(sf1, pt1) Sparse geometry binary predicate list of length 1, where the predicate was `contains&#39; 1: 1, 2, 3, 4, 5, 6 All six points are selected, as expected. Now, let’s reproject the data into a Lambert conformal projection. # Transform the data wld.lam &lt;- st_transform(wld.mil, lambert) pt1.lam &lt;- st_transform(pt1, lambert) sf1.lam &lt;- st_transform(sf1, lambert) # Plot the data in the Lambert coordinate system tm_shape(wld.lam) +tm_fill(col=&quot;grey&quot;) + tm_graticules( x = c(-60,-80,-100, -120, -140), y = c(30,45, 60), labels.col = &quot;white&quot;, col=&quot;grey90&quot;) + tm_shape(sf1.lam) + tm_polygons(&quot;red&quot;, alpha = 0.5, border.col = &quot;yellow&quot;) + tm_shape(pt1.lam) + tm_dots(size=0.2) Only three of the points are contained. We can confirm this using the st_contains function: st_contains(sf1.lam, pt1.lam) Sparse geometry binary predicate list of length 1, where the predicate was `contains&#39; 1: 1, 3, 5 To resolve this problem, one should densify the polygon by adding more vertices along the line segment. The vertices density will be dictated by the resolution needed to preserve the map’s containment properties and is best determined experimentally. We’ll use the st_segmentize function to create vertices at 1 km (1000 m) intervals. # Add vertices every 1000 meters along the polygon&#39;s line segments sf2 &lt;- st_segmentize(sf1, 1000) # Transform the newly densified polygon layer sf2.lam &lt;- st_transform(sf2, lambert) # Plot the data tm_shape(wld.lam) + tm_fill(col=&quot;grey&quot;) + tm_graticules( x = c(-60,-80,-100, -120, -140), y = c(30,45, 60), labels.col = &quot;white&quot;, col=&quot;grey90&quot;) + tm_shape(sf2.lam) + tm_polygons(&quot;red&quot;, alpha = 0.5, border.col = &quot;yellow&quot;) + tm_shape(pt1.lam) + tm_dots(size=0.2) Now all points remain contained by the polygon. We can check via: st_contains(sf2.lam, pt1.lam) Sparse geometry binary predicate list of length 1, where the predicate was `contains&#39; 1: 1, 2, 3, 4, 5, 6 Creating Tissot indicatrix circles Most projections will distort some aspect of a spatial property, especially area and shape. A nice way to visualize the distortion afforded by a projection is to create geodesic circles. First, create a point layer that will define the circle centers in a lat/long coordinate system. tissot.pt &lt;- st_sfc( st_multipoint(rbind(c(-60,30), c(-60,45), c(-60,60), c(-80,30), c(-80,45), c(-80,60), c(-100,30), c(-100,45), c(-100,60), c(-120,30), c(-120,45), c(-120,60) )), crs = &quot;+proj=longlat&quot;) tissot.pt &lt;- st_cast(tissot.pt, &quot;POINT&quot;) # Create single part points Next we’ll construct geodesic circles from these points using the geosphere package. library(geosphere) cr.pt &lt;- list() # Create an empty list # Loop through each point in tissot.pt and generate 360 vertices at 300 km # from each point in all directions at 1 degree increment. These vertices # will be used to approximate the Tissot circles for (i in 1:length(tissot.pt)){ cr.pt[[i]] &lt;- list( destPoint( as(tissot.pt[i], &quot;Spatial&quot;), b=seq(0,360,1), d=300000) ) } # Create a closed polygon from the previously generated vertices tissot.sfc &lt;- st_cast( st_sfc(st_multipolygon(cr.pt ),crs = &quot;+proj=longlat&quot;), &quot;POLYGON&quot; ) We’ll check that these are indeed geodesic circles by computing the geodesic area of each polygon. We’ll use the st_area function from sf which will revert to geodesic area calculation if a lat/long coordinate system is present. tissot.sf &lt;- st_sf( geoArea = st_area(tissot.sfc), tissot.sfc ) The true area of the circles should be \\(\\pi * r^2\\) or 2.8274334^{11} square meters in our example. Let’s compute the error in the tissot output. The values will be reported as fractions. ( (pi * 300000^2) - as.vector(tissot.sf$geoArea) ) / (pi * 300000^2) [1] -0.0008937164 0.0024530577 0.0057943110 -0.0008937164 [5] 0.0024530577 0.0057943110 -0.0008937164 0.0024530577 [9] 0.0057943110 -0.0008937164 0.0024530577 0.0057943110 In all cases, the error is less than 0.1%. The error is primarily due to the discretization of the circle parameter. Let’s now take a look at the distortions associated with a few popular coordinate systems. We’ll start by exploring the Mercator projection. # Transform geodesic circles and compute area error as a percentage tissot.merc &lt;- st_transform(tissot.sf, &quot;+proj=merc +ellps=WGS84&quot;) tissot.merc$area_err &lt;- round((st_area(tissot.merc, tissot.merc$geoArea)) / tissot.merc$geoArea * 100 , 2) # Plot the map tm_shape(World, bbox = st_bbox(tissot.merc), projection = st_crs(tissot.merc)) + tm_borders() + tm_shape(tissot.merc) + tm_polygons(col=&quot;grey&quot;, border.col = &quot;red&quot;, alpha = 0.3) + tm_graticules(x = c(-60,-80,-100, -120, -140), y = c(30,45, 60), labels.col = &quot;white&quot;, col=&quot;grey80&quot;) + tm_text(&quot;area_err&quot;, size=.8, alpha=0.8, col=&quot;blue&quot;) The mercator projection does a good job at preserving shape, but the area’s distortion increases dramatically poleward. Next, we’ll explore the Lambert azimuthal equal area projection centered at 45 degrees north and 100 degrees west. # Transform geodesic circles and compute area error as a percentage tissot.laea &lt;- st_transform(tissot.sf, &quot;+proj=laea +lat_0=45 +lon_0=-100 +ellps=WGS84&quot;) tissot.laea$area_err &lt;- round( (st_area(tissot.laea ) - tissot.laea$geoArea) / tissot.laea$geoArea * 100, 2) # Plot the map tm_shape(World, bbox = st_bbox(tissot.laea), projection = st_crs(tissot.laea)) + tm_borders() + tm_shape(tissot.laea) + tm_polygons(col=&quot;grey&quot;, border.col = &quot;red&quot;, alpha = 0.3) + tm_graticules(x=c(-60,-80,-100, -120, -140), y = c(30,45, 60), labels.col = &quot;white&quot;, col=&quot;grey80&quot;) + tm_text(&quot;area_err&quot;, size=.8, alpha=0.8, col=&quot;blue&quot;) The area error across the 48 states is near 0. But note that the shape does become slightly distorted as we move away from the center of projection. Next, we’ll explore the Robinson projection. # Transform geodesic circles and compute area error as a percentage tissot.robin &lt;- st_transform(tissot.sf, &quot;+proj=robin +ellps=WGS84&quot;) tissot.robin$area_err &lt;- round( (st_area(tissot.robin ) - tissot.robin$geoArea) / tissot.robin$geoArea * 100, 2) # Plot the map tm_shape(World, bbox = st_bbox(tissot.robin), projection = st_crs(tissot.robin)) + tm_borders() + tm_shape(tissot.robin) + tm_polygons(col=&quot;grey&quot;, border.col = &quot;red&quot;, alpha = 0.3) + tm_graticules(x=c(-60,-80,-100, -120, -140), y = c(30,45, 60), labels.col = &quot;white&quot;, col=&quot;grey80&quot;) + tm_text(&quot;area_err&quot;, size=.8, alpha=0.8, col=&quot;blue&quot;) Both shape and area are measurably distorted for the north american continent. "],["point-pattern-analysis-in-r.html", "H Point pattern analysis in R Sample files for this exercise Prepping the data Density based analysis Distance based analysis Hypothesis tests", " H Point pattern analysis in R R spatstat 4.5.1 3.4.0 For a basic theoretical treatise on point pattern analysis (PPA) the reader is encouraged to review the point pattern analysis lecture notes. This section is intended to supplement the lecture notes by implementing PPA techniques in the R programming environment. Sample files for this exercise Data used in the following exercises can be loaded into your current R session by running the following chunk of code. load(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/ppa.RData&quot;)) The data objects consist of three spatial data layers: starbucks: A ppp point layer of Starbucks stores in Massachusetts; ma: An owin polygon layer of Massachusetts boundaries; pop: An im raster layer of population density distribution. All layers are in a format supported by the spatstat (Baddeley, Rubak, and Turner 2016) package. Note that these layers are not authoritative and are to be used for instructional purposes only. Prepping the data All point pattern analysis tools used in this tutorial are available in the spatstat package. These tools are designed to work with points stored as ppp objects and not SpatialPointsDataFrame or sf objects. Note that a ppp object may or may not have attribute information (also referred to as marks). Knowing whether or not a function requires that an attribute table be present in the ppp object matters if the operation is to complete successfully. In this tutorial we will only concern ourselves with the pattern generated by the points and not their attributes. We’ll therefore remove all marks from the point object. library(spatstat) marks(starbucks) &lt;- NULL Many point pattern analyses such as the average nearest neighbor analysis should have their study boundaries explicitly defined. This can be done in spatstat by “binding” the Massachusetts boundary polygon to the Starbucks point feature object using the Window() function. Note that the function name starts with an upper case W. Window(starbucks) &lt;- ma We can plot the point layer to ensure that the boundary is properly defined for that layer. plot(starbucks, main=NULL, cols=rgb(0,0,0,.2), pch=20) We’ll make another change to the dataset. Population density values for an administrative layer are usually quite skewed. The population density for Massachusetts is no exception. The following code chunk generates a histogram from the pop raster layer. hist(pop, main=NULL, las=1) Transforming the skewed distribution in the population density covariate may help reveal relationships between point distributions and the covariate in some of the point pattern analyses covered later in this tutorial. We’ll therefore create a log-transformed version of pop. pop.lg &lt;- log(pop) hist(pop.lg, main=NULL, las=1) We’ll be making use of both expressions of the population density distribution in the following exercises. Density based analysis Quadrat density You can compute the quadrat count and intensity using spatstat’s quadratcount() and intensity() functions. The following code chunk divides the state of Massachusetts into a grid of 3 rows and 6 columns then tallies the number of points falling in each quadrat. Q &lt;- quadratcount(starbucks, nx= 6, ny=3) The object Q stores the number of points inside each quadrat. You can plot the quadrats along with the counts as follows: plot(starbucks, pch=20, cols=&quot;grey70&quot;, main=NULL) # Plot points plot(Q, add=TRUE) # Add quadrat grid You can compute the density of points within each quadrat as follows: # Compute the density for each quadrat Q.d &lt;- intensity(Q) # Plot the density plot(intensity(Q, image=TRUE), main=NULL, las=1) # Plot density raster plot(starbucks, pch=20, cex=0.6, col=rgb(0,0,0,.5), add=TRUE) # Add points The density values are reported as the number of points (stores) per square meters, per quadrat. The Length dimension unit is extracted from the coordinate system associated with the point layer. In this example, the length unit is in meters, so the density is reported as points per square meter. Such a small length unit is not practical at this scale of analysis. It’s therefore desirable to rescale the spatial objects to a larger length unit such as the kilometer. starbucks.km &lt;- rescale(starbucks, 1000, &quot;km&quot;) ma.km &lt;- rescale(ma, 1000, &quot;km&quot;) pop.km &lt;- rescale(pop, 1000, &quot;km&quot;) pop.lg.km &lt;- rescale(pop.lg, 1000, &quot;km&quot;) The second argument to the rescale function divides the current unit (meter) to get the new unit (kilometer). This gives us more sensible density values to work with. # Compute the density for each quadrat (in counts per km2) Q &lt;- quadratcount(starbucks.km, nx= 6, ny=3) Q.d &lt;- intensity(Q) # Plot the density plot(intensity(Q, image=TRUE), main=NULL, las=1) # Plot density raster plot(starbucks.km, pch=20, cex=0.6, col=rgb(0,0,0,.5), add=TRUE) # Add points Quadrat density on a tessellated surface We can use a covariate such as the population density raster to define non-uniform quadrats. We’ll first divide the population density covariate into four regions (aka tessellated surfaces) following an equal interval classification scheme. Recall that we are working with the log transformed population density values. The breaks will be defined as follows: Break Logged population density value 1 ] -Inf; 4 ] 2 ] 4 ; 6 ] 3 ] 3 ; 8 ] 4 ] 8 ; Inf ] brk &lt;- c( -Inf, 4, 6, 8 , Inf) # Define the breaks Zcut &lt;- cut(pop.lg.km, breaks=brk, labels=1:4) # Classify the raster E &lt;- tess(image=Zcut) # Create a tesselated surface The tessellated object can be mapped to view the spatial distribution of quadrats. plot(E, main=&quot;&quot;, las=1) Next, we’ll tally the quadrat counts within each tessellated area then compute their density values (number of points per quadrat area). Q &lt;- quadratcount(starbucks.km, tess = E) # Tally counts Q.d &lt;- intensity(Q) # Compute density Q.d tile 1 2 3 4 0.0000000000 0.0003706106 0.0103132964 0.0889370933 Recall that the length unit is kilometer so the above density values are number of points per square kilometer within each quadrat unit. Plot the density values across each tessellated region. plot(intensity(Q, image=TRUE), las=1, main=NULL) plot(starbucks.km, pch=20, cex=0.6, col=rgb(1,1,1,.5), add=TRUE) Let’s modify the color scheme. cl &lt;- interp.colours(c(&quot;lightyellow&quot;, &quot;orange&quot; ,&quot;red&quot;), E$n) plot( intensity(Q, image=TRUE), las=1, col=cl, main=NULL) plot(starbucks.km, pch=20, cex=0.6, col=rgb(0,0,0,.5), add=TRUE) Kernel density raster The spatstat package has a function called density which computes an isotropic kernel intensity estimate of the point pattern. Its bandwidth defines the kernel’s window extent. This next code chunk uses the default bandwidth. K1 &lt;- density(starbucks.km) # Using the default bandwidth plot(K1, main=NULL, las=1) contour(K1, add=TRUE) In this next chunk, a 50 km bandwidth (sigma = 50) is used. Note that the length unit is extracted from the point layer’s mapping units (which was rescaled to kilometers earlier in this exercise). K2 &lt;- density(starbucks.km, sigma=50) # Using a 50km bandwidth plot(K2, main=NULL, las=1) contour(K2, add=TRUE) The kernel defaults to a gaussian smoothing function. The smoothing function can be changed to a quartic, disc or epanechnikov function. For example, to change the kernel to a disc function type: K3 &lt;- density(starbucks.km, kernel = &quot;disc&quot;, sigma=50) # Using a 50km bandwidth plot(K3, main=NULL, las=1) contour(K3, add=TRUE) Kernel density adjusted for covariate In the following example, a Starbucks store point process’ intensity is estimated following the population density raster covariate. The outputs include a plot of \\(\\rho\\) vs. population density and a raster map of \\(\\rho\\) controlled for population density. # Compute rho using the ratio method rho &lt;- rhohat(starbucks.km, pop.lg.km, method=&quot;ratio&quot;) # Generate rho vs covariate plot plot(rho, las=1, main=NULL, legendargs=list(cex=0.8, xpd=TRUE, inset=c(1.01, 0) )) It’s important to note that we are not fitting a parametric model to the data. Instead, a non-parametric curve is fit to the data. Its purpose is to describe/explore the shape of the relationship between point density and covariate. Note the exponentially increasing intensity of Starbucks stores with increasing population density values when the population density is expressed as a log. The grey envelope represents the 95% confidence interval. The following code chunk generates the map of the predicted Starbucks density if population density were the sole driving process. (Note the use of the gamma parameter to “stretch” the color scheme in the map). pred &lt;- predict(rho) cl &lt;- interp.colours(c(&quot;lightyellow&quot;, &quot;orange&quot; ,&quot;red&quot;), 100) # Create color scheme plot(pred, col=cl, las=1, main=NULL, gamma = 0.25) The predicted intensity’s spatial pattern mirrors the covariate’s population distribution pattern. The predicted intensity values range from 0 to about 5 stores per square kilometer. You’ll note that this maximum value does not match the maximum value of ~3 shown in the rho vs population density plot. This is because the plot did not show the full range of population density values (the max density value shown was 10). The population raster layer has a maximum pixel value of 11.03 (this value can be extracted via max(pop.lg.km)). We can compare the output of the predicted Starbucks stores intensity function to that of the observed Starbucks stores intensity function. We’ll use the variable K1 computed earlier to represent the observed intensity function. K1_vs_pred &lt;- pairs(K1, pred, plot = FALSE) plot(K1_vs_pred$pred ~ K1_vs_pred$K1, pch=20, xlab = &quot;Observed intensity&quot;, ylab = &quot;Predicted intensity&quot;, col = rgb(0,0,0,0.1)) If the modeled intensity was comparable to the observed intensity, we would expect the points to cluster along a one-to-one diagonal. An extreme example is to compare the observed intensity with itself which offers a perfect match of intensity values. K1_vs_K1 &lt;- pairs(K1, K1, labels = c(&quot;K1a&quot;, &quot;K1b&quot;), plot = FALSE) plot(K1_vs_K1$K1a ~ K1_vs_K1$K1b, pch=20, xlab = &quot;Observed intensity&quot;, ylab = &quot;Observed intensity&quot;) So going back to our predicted vs observed intensity plot, we note a strong skew in the predicted intensity values. We also note an overestimation of intensity around higher values. summary(as.data.frame(K1_vs_pred)) K1 pred Min. :8.847e-05 Min. :0.0000000 1st Qu.:1.207e-03 1st Qu.:0.0002813 Median :3.377e-03 Median :0.0015394 Mean :8.473e-03 Mean :0.0078210 3rd Qu.:1.078e-02 3rd Qu.:0.0059043 Max. :5.693e-02 Max. :5.0861718 The predicted maximum intensity value is two orders of magnitude greater than that observed. The overestimation of intenstity values can also be observed at lower values. The following plot limits the data to observed intensities less than 0.04. A red one-to-one line is added for reference. If intensities were similar, they would aggregate around this line. plot(K1_vs_pred$pred ~ K1_vs_pred$K1, pch=20, xlab = &quot;Observed intensity&quot;, ylab = &quot;Predicted intensity&quot;, col = rgb(0,0,0,0.1), xlim = c(0, 0.04), ylim = c(0, 0.1)) abline(a=0, b = 1, col = &quot;red&quot;) Modeling intensity as a function of a covariate The relationship between the predicted Starbucks store point pattern intensity and the population density distribution can be modeled following a Poisson point process model. We’ll generate the Poisson point process model then plot the results. # Create the Poisson point process model PPM1 &lt;- ppm(starbucks.km ~ pop.lg.km) # Plot the relationship plot(effectfun(PPM1, &quot;pop.lg.km&quot;, se.fit=TRUE), main=NULL, las=1, legendargs=list(cex=0.8, xpd=TRUE, inset=c(1.01, 0) )) Note that this is not the same relationship as \\(\\rho\\) vs. population density shown in the previous section. Here, we’re fitting a well defined model to the data whose parameters can be extracted from the PPM1 object. PPM1 Nonstationary Poisson process Fitted to point pattern dataset &#39;starbucks.km&#39; Log intensity: ~pop.lg.km Fitted trend coefficients: (Intercept) pop.lg.km -13.710551 1.279928 Estimate S.E. CI95.lo CI95.hi Ztest Zval (Intercept) -13.710551 0.46745489 -14.626746 -12.794356 *** -29.33021 pop.lg.km 1.279928 0.05626785 1.169645 1.390211 *** 22.74705 Problem: Values of the covariate &#39;pop.lg.km&#39; were NA or undefined at 0.57% (4 out of 699) of the quadrature points The model takes on the form: \\[ \\lambda(i) = e^{-13.71 + 1.27(logged\\ population\\ density)} \\] Here, the base intensity is close to zero (\\(e^{-13.71}\\)) when the logged population density is zero and for every increase in one unit of the logged population density, the Starbucks point density increases by a factor of \\(e^{1.27}\\) units. Distance based analysis Next, we’ll explore three different distance based analyses: The average nearest neighbor, the \\(K\\) and \\(L\\) functions and the pair correlation function \\(g\\). Average nearest neighbor analysis Next, we’ll compute the average nearest neighbor (ANN) distances between Starbucks stores. To compute the average first nearest neighbor distance (in kilometers) set k=1: mean(nndist(starbucks.km, k=1)) [1] 3.275492 To compute the average second nearest neighbor distance set k=2: mean(nndist(starbucks.km, k=2)) [1] 5.81173 The parameter k can take on any order neighbor (up to n-1 where n is the total number of points). The average nearest neighbor function can be expended to generate an ANN vs neighbor order plot. In the following example, we’ll plot ANN as a function of neighbor order for the first 100 closest neighbors: ANN &lt;- apply(nndist(starbucks.km, k=1:100),2,FUN=mean) plot(ANN ~ eval(1:100), type=&quot;b&quot;, main=NULL, las=1) The bottom axis shows the neighbor order number and the left axis shows the average distance in kilometers. K and L functions To compute the K function, type: K &lt;- Kest(starbucks.km) plot(K, main=NULL, las=1, legendargs=list(cex=0.8, xpd=TRUE, inset=c(1.01, 0) )) The plot returns different estimates of \\(K\\) depending on the edge correction chosen. By default, the isotropic, translate and border corrections are implemented. To learn more about these edge correction methods type ?Kest at the command line. The estimated \\(K\\) functions are listed with a hat ^. The black line (\\(K_{pois}\\)) represents the theoretical \\(K\\) function under the null hypothesis that the points are completely randomly distributed (CSR/IRP). Where \\(K\\) falls under the theoretical \\(K_{pois}\\) line the points are deemed more dispersed than expected at distance \\(r\\). Where \\(K\\) falls above the theoretical \\(K_{pois}\\) line the points are deemed more clustered than expected at distance \\(r\\). To compute the L function, type: L &lt;- Lest(starbucks.km, main=NULL) plot(L, main=NULL, las=1, legendargs=list(cex=0.8, xpd=TRUE, inset=c(1.01, 0) )) To plot the L function with the Lexpected line set horizontal: plot(L, . -r ~ r, main=NULL, las=1, legendargs=list(cex=0.8, xpd=TRUE, inset=c(1.01, 0) )) Pair correlation function g To compute the pair correlation function type: g &lt;- pcf(starbucks.km) plot(g, main=NULL, las=1, legendargs=list(cex=0.8, xpd=TRUE, inset=c(1.01, 0) )) As with the Kest and Lest functions, the pcf function outputs different estimates of \\(g\\) using different edge correction methods (Ripley and Translate). The theoretical \\(g\\)-function \\(g_{Pois}\\) under a CSR process (green dashed line) is also displayed for comparison. Where the observed \\(g\\) is greater than \\(g_{Pois}\\) we can expect more clustering than expected and where the observed \\(g\\) is less than \\(g_{Pois}\\) we can expect more dispersion than expected. Hypothesis tests Test for clustering/dispersion First, we’ll run an ANN analysis for Starbucks locations assuming a uniform point density across the state (i.e. a completely spatially random process). ann.p &lt;- mean(nndist(starbucks.km, k=1)) ann.p [1] 3.275492 The observed average nearest neighbor distance is 3.28 km. Next, we will generate the distribution of expected ANN values given a homogeneous (CSR/IRP) point process using Monte Carlo methods. This is our null model. n &lt;- 599L # Number of simulations ann.r &lt;- vector(length = n) # Create an empty object to be used to store simulated ANN values for (i in 1:n){ rand.p &lt;- rpoint(n=starbucks.km$n, win=ma.km) # Generate random point locations ann.r[i] &lt;- mean(nndist(rand.p, k=1)) # Tally the ANN values } In the above loop, the function rpoint is passed two parameters: n=starbucks.km$n and win=ma.km. The first tells the function how many points to randomly generate (starbucks.km$n extracts the number of points from object starbucks.km). The second tells the function to confine the points to the extent defined by ma.km. Note that the latter parameter is not necessary if the ma boundary was already defined as the starbucks window extent. You can plot the last realization of the homogeneous point process to see what a completely random placement of Starbucks stores could look like. plot(rand.p, pch=16, main=NULL, cols=rgb(0,0,0,0.5)) Our observed distribution of Starbucks stores certainly does not look like the outcome of a completely independent random process. Next, let’s plot the histogram of expected values under the null and add a blue vertical line showing where our observed ANN value lies relative to this distribution. hist(ann.r, main=NULL, las=1, breaks=40, col=&quot;bisque&quot;, xlim=range(ann.p, ann.r)) abline(v=ann.p, col=&quot;blue&quot;) It’s obvious from the test that the observed ANN value is far smaller than the expected ANN values one could expect under the null hypothesis. A smaller observed value indicates that the stores are far more clustered than expected under the null. Next, we’ll run the same test but control for the influence due to population density distribution. Recall that the ANN analysis explores the 2nd order process underlying a point pattern thus requiring that we control for the first order process (e.g. population density distribution). This is a non-homogeneous test. Here, we pass the parameter f=pop.km to the function rpoint telling it that the population density raster pop.km should be used to define where a point should be most likely placed (high population density) and least likely placed (low population density) under this new null model. Here, we’ll use the non-transformed representation of the population density raster, pop.km. n &lt;- 599L ann.r &lt;- vector(length=n) for (i in 1:n){ rand.p &lt;- rpoint(n=starbucks.km$n, f=pop.km) ann.r[i] &lt;- mean(nndist(rand.p, k=1)) } You can plot the last realization of the non-homogeneous point process to convince yourself that the simulation correctly incorporated the covariate raster in its random point function. Window(rand.p) &lt;- ma.km # Replace raster mask with ma.km window plot(rand.p, pch=16, main=NULL, cols=rgb(0,0,0,0.5)) Note the cluster of points near the highly populated areas. This pattern is different from the one generated from a completely random process. Next, let’s plot the histogram and add a blue line showing where our observed ANN value lies. hist(ann.r, main=NULL, las=1, breaks=40, col=&quot;bisque&quot;, xlim=range(ann.p, ann.r)) abline(v=ann.p, col=&quot;blue&quot;) Even though the distribution of ANN values we would expect when controlled for the population density nudges closer to our observed ANN value, we still cannot say that the clustering of Starbucks stores can be explained by a completely random process when controlled for population density. Computing a pseudo p-value from the simulation A (pseudo) p-value can be extracted from a Monte Carlo simulation. We’ll work off of the last simulation. First, we need to find the number of simulated ANN values greater than our observed ANN value. N.greater &lt;- sum(ann.r &gt; ann.p) To compute the p-value, find the end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value. See lecture notes for more information. p &lt;- min(N.greater + 1, n + 1 - N.greater) / (n +1) p [1] 0.001666667 In our working example, you’ll note that or simulated ANN value was nowhere near the range of ANN values computed under the null yet we don’t have a p-value of zero. This is by design since the strength of our estimated p will be proportional to the number of simulations–this reflects the chance that given an infinite number of simulations at least one realization of a point pattern could produce an ANN value more extreme than ours. Test for a poisson point process model with a covariate effect The ANN analysis addresses the 2nd order effect of a point process. Here, we’ll address the 1st order process using the poisson point process model. We’ll first fit a model that assumes that the point process’ intensity is a function of the logged population density (this will be our alternate hypothesis). PPM1 &lt;- ppm(starbucks.km ~ pop.lg.km) PPM1 Nonstationary Poisson process Fitted to point pattern dataset &#39;starbucks.km&#39; Log intensity: ~pop.lg.km Fitted trend coefficients: (Intercept) pop.lg.km -13.710551 1.279928 Estimate S.E. CI95.lo CI95.hi Ztest Zval (Intercept) -13.710551 0.46745489 -14.626746 -12.794356 *** -29.33021 pop.lg.km 1.279928 0.05626785 1.169645 1.390211 *** 22.74705 Problem: Values of the covariate &#39;pop.lg.km&#39; were NA or undefined at 0.57% (4 out of 699) of the quadrature points Next, we’ll fit the model that assumes that the process’ intensity is not a function of population density (the null hypothesis). PPM0 &lt;- ppm(starbucks.km ~ 1) PPM0 Stationary Poisson process Fitted to point pattern dataset &#39;starbucks.km&#39; Intensity: 0.008268627 Estimate S.E. CI95.lo CI95.hi Ztest Zval log(lambda) -4.795287 0.07647191 -4.945169 -4.645405 *** -62.70651 In our working example, the null model (homogeneous intensity) takes on the form: \\[ \\lambda(i) = e^{-4.795} \\] \\(\\lambda(i)\\) under the null is nothing more than the observed density of Starbucks stores within the State of Massachusetts, or: starbucks.km$n / area(ma.km) [1] 0.008268627 The alternate model takes on the form: \\[ \\lambda(i) = e^{-13.71 + 1.27\\ (logged\\ population\\ density)} \\] The models are then compared using the likelihood ratio test which produces the following output: anova(PPM0, PPM1, test=&quot;LRT&quot;) Npar Df Deviance Pr(&gt;Chi) 5 NA NA NA 6 1 537.218 0 The value under the heading PR(&gt;Chi) is the p-value which gives us the probability that we would be wrong in rejecting the null. Here p~0 suggests that there is close to a 0% chance that we would be wrong in rejecting the base model in favor of the alternate model–put another way, the alternate model (that the logged population density can help explain the distribution of Starbucks stores) is a significant improvement over the null. Note that if you were to compare two competing non-homogeneous models such as population density and income distributions, you would need to compare the model with one of the covariates with an augmented version of that model using the other covariate. In other words, you would need to compare PPM1 &lt;- ppm(starbucks.km ~ pop.lg.km) with something like PPM2 &lt;- ppm(starbucks.km ~ pop.lg.km + income.km). References Baddeley, Adrian, Ege Rubak, and Rolf Turner. 2016. Spatial Point Patterns, Methodology and Applications with r. Florida: CRC Press. "],["spatial-autocorrelation-in-r.html", "I Spatial autocorrelation in R Sample files for this exercise Introduction Define neighboring polygons Generating a Moran’s I scatter plot Computing the Moran’s I coefficient Assessing statistical significance Moran’s I as a function of a distance band Local Moran’s I", " I Spatial autocorrelation in R R sf tmap spatstat gstat terra sp 4.5.1 1.0.21 4.1 3.4.0 2.1.4 1.8.60 2.2.0 For a basic theoretical treatise on spatial autocorrelation the reader is encouraged to review the lecture notes. This section is intended to supplement the lecture notes by implementing spatial autocorrelation techniques in the R programming environment. Sample files for this exercise Data used in the following exercises can be loaded into your current R session by running the following chunk of code. z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/ma2.rds&quot;)) ma &lt;- readRDS(z) The ma object consists of an sf vector layer representing census data aggregated at the county subdivision level for 2021 (src. Census Bureau 5-year ACS). Introduction The spatial object ma has nine attributes with the first being a FIPS identifier. The one of interest for this exercise is house_inc (median household income for 2021, in units of dollars). The following table shows just the first few records of the 343 polygon layer (FIPS and geometry columns are not shown). A map of the income distribution using an equal interval classification scheme is generated using the tmap package. library(tmap) tm_shape(ma) + tm_polygons(style=&quot;equal&quot;, border.col = &quot;grey80&quot;, lwd = 0.5, col = &quot;house_inc&quot;, palette=&quot;Greens&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) Define neighboring polygons We must first define what is meant by “neighboring” polygons. This can refer to contiguous polygons, polygons within a certain distance band, or it could be non-spatial in nature and defined by social, political or cultural “neighbors”. In this example, we’ll adopt a contiguous neighbor definition where we’ll accept any contiguous polygon that shares at least on vertex (this is the “queen” case and is defined by setting the parameter queen=TRUE in the poly2nb() function). If we required that at least one edge be shared between polygons then we would set queen=FALSE. library(spdep) nb &lt;- poly2nb(ma, queen=TRUE) Other neighborhood functions that can be implemented in spdep include: dnearneigh distance based neighbor (allows for annulus neighbors) For point geometry knearneigh + knn2nb k nearest neighbor For point geometry For each polygon in our polygon object, nb lists all polygons deemed contiguous. For example, to see the neighbors for the first polygon in the object, type: nb[[1]] [1] 3 34 149 150 Polygon 1 has 4 neighbors. The numbers represent the polygon IDs as stored in the spatial object ma. Polygon 1 is associated with the following name attribute: ma$name[1] [1] &quot;Hanover town, Plymouth County, Massachusetts&quot; Its four neighboring polygons are associated with the counties: ma$name[c(2,3,4,5)] [1] &quot;Marshfield town, Plymouth County, Massachusetts&quot; [2] &quot;Norwell town, Plymouth County, Massachusetts&quot; [3] &quot;Wareham town, Plymouth County, Massachusetts&quot; [4] &quot;Framingham city, Middlesex County, Massachusetts&quot; Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be multiplied by the weight \\(1/ (\\#\\ of\\ neighbors)\\) (style=\"W\"–note the uppercase \"W\") such that the sum of the weights equal 1. If a binary weight is desired (i.e. one where each neighboring polygon is a assigned a weight of 1, regardless of the number of neighbors), set style=\"B\". lw &lt;- nb2listw(nb, style=&quot;W&quot;, zero.policy=TRUE) The zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset. Setting zero.policy to FALSE will return an error if at least one polygon has no neighbor. To see the weight of the first polygon’s four neighbors type: lw$weights[1] [[1]] [1] 0.25 0.25 0.25 0.25 For polygon 1, each neighbor is assigned a quarter of the total weight. This means that when R computes the neighboring income values, each neighbor’s income will be multiplied by 0.25 before being summed thus giving us the arithmetic mean of polygon 1’s neighbors. Generating a Moran’s I scatter plot If you wish to view the relationship between each polygon’s value (house_inc in this working example) and its spatially lagged values, you need to first extract the lagged values from the lw object. ma$lag &lt;- lag.listw(lw, ma$house_inc) We can plot lagged income vs. income and fit a linear regression model to the data. # Create a regression model M &lt;- lm(lag ~ house_inc, ma) # Plot the data plot( lag ~ house_inc, ma, pch=21, asp=1, las=1, col = &quot;grey40&quot;, bg=&quot;grey80&quot;) abline(M, col=&quot;blue&quot;) # Add the regression line from model M abline(v = mean(ma$house_inc), lty=3, col = &quot;grey80&quot;) abline(h = mean(ma$house_inc), lty=3, col = &quot;grey80&quot;) The slope of the regression model is the Moran’s I coefficient. The next step will show you how to compute this statistic without needing to compute the lagged values and fitting a regression model. Computing the Moran’s I coefficient The Moran’s I global statistic can be computed uisng the moran function. Note that you need to specify the attribute value (house_inc in this example) and not just the geometric elements. moran(ma$house_inc, listw = lw, n = length(nb), S0 = Szero(lw)) $I [1] 0.5199357 $K [1] 4.602809 listw is passed the weights list. n is the total number of features having at least one neighbor. This values can be extracted via length(nb) . S0 is the sum of all weights which, in our example should sum to the number of observations with non-zero neighbors. Here too, we make use of another function, Szero(lw) (note the uppercase S) to extract that number. Assessing statistical significance To assess if the Moran’s I statistic (i.e. the slope in the scatter plot) is significantly different from zero, we can randomly permute the income values across all polygons (i.e. we are not imposing any spatial autocorrelation structure), then we compute a Moran’s I coefficient for each permuted set of values. This gives us the distribution of Moran’s I values we could expect to get under the null hypothesis that the income values are randomly distributed across all census units. We then compare the observed Moran’s I value to this distribution. In this example, we’ll permute the data 999 times by setting nsim = 999. MC&lt;- moran.mc(ma$house_inc, lw, nsim = 999) # View results (including pseudo p-value) MC Monte-Carlo simulation of Moran I data: ma$house_inc weights: lw number of simulations + 1: 1000 statistic = 0.51994, observed rank = 1000, p-value = 0.001 alternative hypothesis: greater # Plot the distribution (note that this is a density plot instead of a histogram) plot(MC, main=&quot;&quot;, las=1) The simulation suggests that our observed Moran’s I value is not consistent with a Moran’s I value one would expect to get if the income values were not spatially autocorrelated. The pseudo p-value can be extracted from the MC model as: MC$p.value [1] 0.001 In our simulation, none of the Moran’s statistics computed from the permuted data was more extreme than our observed Moran’s I value. The p-value is therefore capped by the number of simulations. in other words, we cannot compute a p-value less than \\(1/(1 + N)\\) (where N is the total number of simulations) or 1/(1 + 999) = 0.001. Note that by default, the moran.mc function will compute the number of simulations greater than the observed statistic. So, if the observed statistic is on the left side of the distribution, the pseudo p-value will be greater than 0.5 (note that this is a one-sided test). You can change the alternative hypothesis to one where we may expect a more dispersed pattern by setting the parameter alternative to \"less\". Moran’s I as a function of a distance band In this section, we will explore spatial autocorrelation as a function of distance bands. Instead of defining neighbors as contiguous polygons, we will define neighbors based on distances to polygon centers. We therefore need to extract the center of each polygon. xy &lt;- st_centroid(ma) The object xy stores all 343 pairs of coordinate values (i.e. the same number of points as polygons). Next, we will define the search radius to include all neighboring polygon centers that are at least 70 km away but less than 80 km away. s.dist &lt;- dnearneigh(xy, 70000, 80000) The dnearneigh function takes three parameters: the coordinate values xy, the radius for the inner radius of the annulus band, and the radius for the outer annulus band. In our example, the inner annulus radius is 70km which implies that all polygon centers up to 70km are not included in the lagged value calculation. Polygon 1 has 26 polygons as neighbors when adopting the annulus definition. s.dist[[1]] |&gt; length() [1] 26 You can map the neighbors for polygon 1 as follows. (Polygon 1 is shown with a red fill color). annulus1 &lt;- ma[s.dist[[1]], &quot;house_inc&quot;] # Extract polygon 1&#39;s neighbors tm_shape(ma) + tm_borders(col = &quot;grey80&quot;) + tm_shape(ma[1,]) + tm_fill(col=&quot;red&quot;) + tm_shape(annulus1) + tm_polygons(style=&quot;equal&quot;, border.col = &quot;grey80&quot;, lwd = 0.5, col = &quot;house_inc&quot;, palette=&quot;Greens&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) Now that we defined the distance based neighbors for all polygons, we need to compute their weights. We will adopt the same weighing scheme used in the contiguous definition of neighbors in the earlier example. lw &lt;- nb2listw(s.dist, style=&quot;W&quot;, zero.policy=T) The polygon 1’s neighbors are each assigned a weight of 0.0384615. lw$weights[[1]] [1] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 [7] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 [13] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 [19] 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 0.03846154 [25] 0.03846154 0.03846154 Run the MC simulation. MC &lt;- moran.mc(ma$house_inc, lw, nsim=599, zero.policy=T) Plot the results. plot(MC, main=&quot;&quot;, las=1) Here, we are observing a different statistical outcome. The observed Moran’s I coefficient is on the left side of the distribution suggesting that income values tend to be more different than similar when separated by a 50 km distance. Display p-value and other summary statistics. MC Monte-Carlo simulation of Moran I data: ma$house_inc weights: lw number of simulations + 1: 600 statistic = -0.027383, observed rank = 31, p-value = 0.9483 alternative hypothesis: greater Since we went with the default alternative parameter of \"greater\" in the moran.mc function, the output is giving us the fraction of simulated values greater than our observed value–about 94.8% in our example. Given that our observed statistic is on the left side of the distribution, it’s best to report the p-value as (1 - 0.948) or 0.052 with the alternative hypothesis that the values are more dissimilar than expected. Local Moran’s I To compute local indicators of spatial autocorrelation (Local Moran’s I), we can make use of the localmoran function from the spdep package. However, this function adopts an analytical approach to computing the p-value. It’s best to adopt a Monte Carlo approach. This can be performed using the localmoran_perm function. In this example, we will run 9999 permutations for each polygon. We’ll first revert to our earlier definition of neighbor (i.e. one of contiguity). lw &lt;- nb2listw(nb, style=&quot;W&quot;, zero.policy=TRUE) Next, we’ll run the local Moran’s I analysis using the localmoran_perm function. MCi &lt;- localmoran_perm(ma$house_inc, lw, nsim = 9999) MCi.df &lt;- as.data.frame(MCi) The object MCi stores several parameters including the local Moran’s statistic for each polygon, \\(I_i\\), and its associated pseudo p-value. MCi.df is a dataframe version of the MCiobject. Next, we’ll add the pseudo p-values to the ma spatial object. ma$p &lt;- MCi.df$`Pr(folded) Sim` Note that the localmoran_perm function generates two different p-values: MCi.df$`Pr(z != E(Ii)) Sim` and MCi.df$`Pr(folded) Sim`. The former is for a two sided test (alternative = \"two.sided\") and the latter is a “folded” p-value for a one-sided test. We can generate a map of the pseudo p-values as follows: pal1 &lt;- c(&quot;#DE2D26&quot;,&quot;#FC9272&quot;, &quot;#FEE0D2&quot;, &quot;grey90&quot;) library(tmap) tm_shape(ma) + tm_polygons(style=&quot;fixed&quot;, breaks = c(0, 0.001, 0.01, 0.05, 0.5), col = &quot;p&quot;, palette=pal1, border.col = &quot;grey80&quot;, lwd = 0.5) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) Next, we’ll map the significant clusters. First, we will use the hotspot function to identify “significant” clusters using a p-value cutoff of 0.05. Since the output is a factor, a manipulation of its levels is made to facilitate the assignment of colors when generating the map. ma$Ii &lt;- hotspot(MCi, Prname=&quot;Pr(folded) Sim&quot;, cutoff = 0.05, p.adjust = &quot;none&quot;) # Replace NA with &quot;&gt;0.05&quot;. This requires that the Ii factor be re-leveled ma$Ii &lt;- factor(ma$Ii, levels=c(&quot;High-High&quot;,&quot;Low-Low&quot;, &quot;Low-High&quot;, &quot;High-Low&quot;, &quot;&gt;0.05&quot;)) ma$Ii[is.na(ma$Ii)] &lt;- &quot;&gt;0.05&quot; Next, we define the color palette, then we generate the map. pal2 &lt;- c( &quot;#FF0000&quot;, &quot;#0000FF&quot;, &quot;#a7adf9&quot;, &quot;#f4ada8&quot;,&quot;#ededed&quot;) tm_shape(ma) + tm_polygons(style=&quot;cat&quot;, border.col = &quot;grey80&quot;, lwd = 0.5, col = &quot;Ii&quot;, palette=pal2) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) Correcting for multiple comparisons The p.adjust parameter in the hotspot function can be used to correct for multiple tests. One popular correction is the False Discovery Rate (FDR) that can be implemented by setting p.adjust to \"fdr\". ma$Ii &lt;- hotspot(MCi, Prname=&quot;Pr(folded) Sim&quot;, cutoff = 0.05, p.adjust = &quot;fdr&quot;) # Replace NA with &quot;&gt;0.05&quot;. This requires that the Ii factor be re-leveled ma$Ii &lt;- factor(ma$Ii, levels=c(&quot;High-High&quot;,&quot;Low-Low&quot;, &quot;Low-High&quot;, &quot;High-Low&quot;, &quot;&gt;0.05 corrected&quot;)) ma$Ii[is.na(ma$Ii)] &lt;- &quot;&gt;0.05 corrected&quot; We now have a more stringent implementation of the p-value. tm_shape(ma) + tm_polygons(style=&quot;cat&quot;, border.col = &quot;grey80&quot;, lwd = 0.5, col = &quot;Ii&quot;, palette=pal2) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) "],["interpolation-in-r.html", "J Interpolation in R Thiessen polygons IDW 1st order polynomial fit 2nd order polynomial Kriging", " J Interpolation in R R grid tmap spdep tmap 4.5.1 4.5.1 4.1 1.3.13 4.1 First, let’s load the data from the website. The data are vector layers stored as sf objects. library(sf) library(tmap) # Load precipitation data z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/precip.rds&quot;)) p.sp &lt;- readRDS(z) p &lt;- st_as_sf(p.sp) # Create sf version of point layer # Load Texas boundary map z &lt;- gzcon(url(&quot;https://github.com/mgimond/Spatial/raw/main/Data/texas.rds&quot;)) w.sp &lt;- readRDS(z) w &lt;- st_as_sf(w.sp) # Generate map tm_shape(w) + tm_polygons() + tm_shape(p) + tm_dots(fill=&quot;Precip_in&quot;, size=0.5, fill.scale = tm_scale_intervals(n=5, values = &quot;brewer.rd_bu&quot;, style = &quot;pretty&quot;), fill.legend = tm_legend(title = &quot;Precipitation (in)&quot;)) + tm_text(text = &quot;Precip_in&quot;, options = opt_tm_text(just=&quot;left&quot;), xmod=.5, size = 0.7) The p point layer defines the sampled precipitation values. These points will be used to predict values at unsampled locations. The w polygon layer defines the boundary of Texas. This will be the extent for which we will interpolate precipitation data. Thiessen polygons The Thiessen polygons (or proximity interpolation) can be created using spatstat’s dirichlet function. Note that this function will require that the input point layer be converted to a spatstat ppp object–hence the use of the inline as.ppp(p) syntax in the following code chunk. library(spatstat) # Used for the dirichlet tessellation function # Create a tessellated surface th &lt;- dirichlet(as.ppp(p, W = as.owin(w))) |&gt; st_as_sfc() |&gt; st_as_sf() # The dirichlet function does not carry over projection information # requiring that this information be added manually st_crs(th) &lt;- st_crs(p) # The tessellated surface does not store attribute information # from the point data layer. We&#39;ll join the point attributes to the polygons th2 &lt;- st_join(th, p, fn=mean) # Finally, we&#39;ll clip the tessellated surface to the Texas boundaries th.clp &lt;- st_intersection(th2, w) # Map the data tm_shape(th.clp) + tm_polygons(fill=&quot;Precip_in&quot;, fill.scale = tm_scale_intervals(n=5, values = &quot;brewer.rd_bu&quot;, style = &quot;pretty&quot;), fill.legend = tm_legend(title = &quot;Predicted precipitation \\n(in inches)&quot;)) IDW Unlike the Thiessen method shown in the previous section, the IDW interpolation will output a raster. This requires that we first create an empty raster grid, then interpolate the precipitation values to each unsampled grid cell. An IDW power value of 2 (idp=2.0) will be used in this example. Many packages share the same function names. This can be a problem when these packages are loaded in a same R session. For example, the idw function is available in both spatstat.explore and gstat. Here, we make use of gstat’s idw function. This requires that we either detach the spatstat.explore package (this package was automatically installed when we installed spatstat) or that we explicitly identify the package by typing gstat::idw. Here, we opted for the former approach. detach(&quot;package:spatstat.explore&quot;, unload = TRUE, force=TRUE) library(gstat) library(terra) library(sp) # Create an empty grid where n is the total number of cells grd &lt;- as.data.frame(spsample(w.sp, &quot;regular&quot;, n=50000)) names(grd) &lt;- c(&quot;X&quot;, &quot;Y&quot;) coordinates(grd) &lt;- c(&quot;X&quot;, &quot;Y&quot;) gridded(grd) &lt;- TRUE # Create SpatialPixel object fullgrid(grd) &lt;- TRUE # Create SpatialGrid object # Add p.sp&#39;s projection information to the empty grid crs(grd) &lt;- crs(p.sp) # Interpolate the grid cells using a power value of 2 (idp=2.0) p.idw &lt;- idw(Precip_in ~ 1, p.sp, newdata=grd, idp = 2.0) # Convert to raster object then clip to Texas r &lt;- rast(p.idw) r.m &lt;- mask(r, w) # Plot tm_shape(r.m[&quot;var1.pred&quot;]) + tm_raster(col.scale = tm_scale_intervals(values = &quot;brewer.rd_bu&quot;, n = 8), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Predicted precipitation \\n(in inches)&quot; )) + tm_shape(p) + tm_dots(size=0.5) Fine-tuning the interpolation The choice of power function can be subjective. To fine-tune the choice of the power parameter, you can perform a leave-one-out validation routine to measure the error in the interpolated values. # Leave-one-out validation routine IDW.out &lt;- vector(length = length(p.sp)) for (i in 1:length(p.sp)) { IDW.out[i] &lt;- idw(Precip_in ~ 1, p.sp[-i,], p.sp[i,], idp=2.0)$var1.pred } # Plot the differences OP &lt;- par(pty=&quot;s&quot;, mar=c(4,3,0,0)) plot(IDW.out ~ p.sp$Precip_in, asp=1, xlab=&quot;Observed&quot;, ylab=&quot;Predicted&quot;, pch=16, col=rgb(0,0,0,0.5)) abline(lm(IDW.out ~ p.sp$Precip_in), col=&quot;red&quot;, lw=2,lty=2) abline(0,1) par(OP) The RMSE can be computed from IDW.out as follows: # Compute RMSE sqrt( sum((IDW.out - p.sp$Precip_in)^2) / length(p.sp)) [1] 6.989294 Cross-validation In addition to generating an interpolated surface, you can create a 95% confidence interval map of the interpolation model. Here we’ll create a 95% CI map from an IDW interpolation that uses a power parameter of 2 (idp=2.0). # Create the interpolated surface (using gstat&#39;s idw function) img &lt;- idw(Precip_in~1, p.sp, newdata=grd, idp=2.0) n &lt;- length(p.sp) Zi &lt;- matrix(nrow = length(img$var1.pred), ncol = n) # Remove a point then interpolate (do this n times for each point) st &lt;- rast() for (i in 1:n){ Z1 &lt;- gstat::idw(Precip_in~1, p.sp[-i,], newdata=grd, idp=2.0) st &lt;- c(st,rast(Z1)) # Calculated pseudo-value Z at j Zi[,i] &lt;- n * img$var1.pred - (n-1) * Z1$var1.pred } # Jackknife estimator of parameter Z at location j Zj &lt;- as.matrix(apply(Zi, 1, sum, na.rm=T) / n ) # Compute (Zi* - Zj)^2 c1 &lt;- apply(Zi,2,&#39;-&#39;,Zj) # Compute the difference c1 &lt;- apply(c1^2, 1, sum, na.rm=T ) # Sum the square of the difference # Compute the confidence interval CI &lt;- sqrt( 1/(n*(n-1)) * c1) # Create (CI / interpolated value) raster img.sig &lt;- img img.sig$v &lt;- CI /img$var1.pred # Clip the confidence raster to Texas r &lt;- rast(img.sig, layer=&quot;v&quot;) r.m &lt;- mask(r, w) # Plot the map tm_shape(r.m[&quot;var1.pred&quot;]) + tm_raster(col.scale = tm_scale_intervals(values = &quot;yl_or_rd&quot;, n = 6), col.legend = tm_legend(position = tm_pos_out(), na.show = TRUE, title=&quot;95% confidence interval \\n(in inches)&quot;)) + tm_shape(p) + tm_dots(size=0.4) 1st order polynomial fit To fit a first order polynomial model of the form \\(precip = intercept + aX + bY\\) to the data, # Define the 1st order polynomial equation f.1 &lt;- as.formula(Precip_in ~ X + Y) # Add X and Y to p.sp p.sp$X &lt;- coordinates(p.sp)[,1] p.sp$Y &lt;- coordinates(p.sp)[,2] # Run the regression model lm.1 &lt;- lm( f.1, data=p.sp) # Use the regression model output to interpolate the surface dat.1st &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.1, newdata=grd))) # Clip the interpolated raster to Texas r &lt;- rast(dat.1st) r.m &lt;- mask(r, w) # Plot the map tm_shape(r.m) + tm_raster(col.scale = tm_scale_intervals(values = &quot;brewer.rd_bu&quot;, n = 8), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Predicted precipitation \\n(in inches)&quot; )) + tm_shape(p) + tm_dots(size=0.5) 2nd order polynomial To fit a second order polynomial model of the form \\(precip = intercept + aX + bY + dX^2 + eY^2 +fXY\\) to the data, # Define the 2nd order polynomial equation f.2 &lt;- as.formula(Precip_in ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y)) # Add X and Y to p.sp p.sp$X &lt;- coordinates(p.sp)[,1] p.sp$Y &lt;- coordinates(p.sp)[,2] # Run the regression model lm.2 &lt;- lm( f.2, data=p.sp) # Use the regression model output to interpolate the surface dat.2nd &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.2, newdata=grd))) # Clip the interpolated raster to Texas r &lt;- rast(dat.2nd) r.m &lt;- mask(r, w) # Plot the map tm_shape(r.m) + tm_raster(col.scale = tm_scale_intervals(values = &quot;brewer.rd_bu&quot;, n = 8), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Predicted precipitation \\n(in inches)&quot; )) + tm_shape(p) + tm_dots(size=0.5) Kriging Fit the variogram model First, we need to create a variogram model. Note that the variogram model is computed on the de-trended data. This is implemented in the following chunk of code by passing the 1st order trend model (defined in an earlier code chunk as formula object f.1) to the variogram function. # Define the 1st order polynomial equation f.1 &lt;- as.formula(Precip_in ~ X + Y) # Compute the sample variogram; note that the f.1 trend model is one of the # parameters passed to variogram(). This tells the function to create the # variogram on the de-trended data. var.smpl &lt;- variogram(f.1, p.sp, cloud = FALSE, cutoff=1000000, width=89900) # Compute the variogram model by passing the nugget, sill and range values # to fit.variogram() via the vgm() function. dat.fit &lt;- fit.variogram(var.smpl, fit.ranges = FALSE, fit.sills = FALSE, vgm(psill=14, model=&quot;Sph&quot;, range=590000, nugget=0)) # The following plot allows us to assess the fit plot(var.smpl, dat.fit, xlim=c(0,1000000)) Generate Kriged surface Next, use the variogram model dat.fit to generate a kriged interpolated surface. The krige function allows us to include the trend model thus saving us from having to de-trend the data, krige the residuals, then combine the two rasters. Instead, all we need to do is pass krige the trend formula f.1. # Define the trend model f.1 &lt;- as.formula(Precip_in ~ X + Y) # Perform the krige interpolation (note the use of the variogram model # created in the earlier step) dat.krg &lt;- krige( f.1, p.sp, grd, dat.fit) # Convert kriged surface to a raster object for clipping r &lt;- rast(dat.krg) r.m &lt;- mask(r, w) # Plot the map tm_shape(r.m[&quot;var1.pred&quot;]) + tm_raster(col.scale = tm_scale_intervals(values = &quot;brewer.rd_bu&quot;, n = 8), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Predicted precipitation \\n(in inches)&quot; )) + tm_shape(p) + tm_dots(size=0.5) Generate the variance and confidence interval maps The dat.krg object stores not just the interpolated values, but the variance values as well. These are also passed to the raster object for mapping as follows: tm_shape(r.m[&quot;var1.var&quot;]) + tm_raster(col.scale = tm_scale_intervals(values = &quot;brewer.reds&quot;, n = 8), col.legend = tm_legend(position = tm_pos_out(), title = &quot;Predicted precipitation \\n(in inches)&quot; )) + tm_shape(p) + tm_dots(size=0.5) A more readily interpretable map is the 95% confidence interval map which can be generated from the variance object as follows (the map values should be interpreted as the number of inches above and below the estimated rainfall amount). r &lt;- rast(dat.krg) r.m &lt;- mask(sqrt(r[&quot;var1.var&quot;])* 1.96, w) tm_shape(r.m) + tm_raster(col.scale = tm_scale_intervals(values = &quot;reds&quot;, n = 6), col.legend = tm_legend(position = tm_pos_out(), na.show = TRUE, title=&quot;95% confidence interval \\n(in inches)&quot;)) + tm_shape(p) + tm_dots(size=0.5) "]]
